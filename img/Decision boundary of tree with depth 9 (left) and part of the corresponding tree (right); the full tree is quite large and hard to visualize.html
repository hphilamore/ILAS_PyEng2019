<!DOCTYPE html>
<!-- saved from url=(0109)https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#supervised-learning -->
<html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/introduction-to-machine/9781449369880/ch02.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="6079485" data-user-uuid="f0b2a002-7968-4745-a334-a519b5a26e85" data-username="hemmaphilamore1" data-account-type="Trial" data-activated-trial-date="12/06/2019" data-archive="9781449369880" data-publishers="O&#39;Reilly Media, Inc." data-htmlfile-name="ch02.html" data-epub-title="Introduction to Machine Learning with Python" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781449369880"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><script type="text/javascript" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/510f1a6865"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/cool-2.1.15.min.js"></script><script id="twitter-wjs" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/widgets.js"></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/nr-1153.min.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/ec.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/2508.js"></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/443792972845831" async=""></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/1732687426968531" async=""></script><script async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/fbevents.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f.txt"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/analytics.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/bat.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f.txt"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/insight.min.js"></script><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f.txt"></script><script async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/gtm.js"></script><script type="text/javascript">(window.NREUM||(NREUM={})).loader_config={licenseKey:"510f1a6865",applicationID:"172641827"};window.NREUM||(NREUM={}),__nr_require=function(n,e,t){function r(t){if(!e[t]){var i=e[t]={exports:{}};n[t][0].call(i.exports,function(e){var i=n[t][1][e];return r(i||e)},i,i.exports)}return e[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var i=0;i<t.length;i++)r(t[i]);return r}({1:[function(n,e,t){function r(){}function i(n,e,t){return function(){return o(n,[u.now()].concat(f(arguments)),e?null:this,t),e?void 0:this}}var o=n("handle"),a=n(4),f=n(5),c=n("ee").get("tracer"),u=n("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(n,e){s[e]=i(d+e,!0,"api")}),s.addPageAction=i(d+"addPageAction",!0),s.setCurrentRouteName=i(d+"routeName",!0),e.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(n,e){var t={},r=this,i="function"==typeof e;return o(l+"tracer",[u.now(),n,t],r),function(){if(c.emit((i?"":"no-")+"fn-start",[u.now(),r,i],t),i)try{return e.apply(this,arguments)}catch(n){throw c.emit("fn-err",[arguments,this,n],t),n}finally{c.emit("fn-end",[u.now()],t)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(n,e){m[e]=i(l+e)}),newrelic.noticeError=function(n,e){"string"==typeof n&&(n=new Error(n)),o("err",[n,u.now(),!1,e])}},{}],2:[function(n,e,t){function r(n,e){var t=n.getEntries();t.forEach(function(n){"first-paint"===n.name?a("timing",["fp",Math.floor(n.startTime)]):"first-contentful-paint"===n.name&&a("timing",["fcp",Math.floor(n.startTime)])})}function i(n){if(n instanceof c&&!s){var e,t=Math.round(n.timeStamp);e=t>1e12?Date.now()-t:f.now()-t,s=!0,a("timing",["fi",t,{type:n.type,fid:e}])}}if(!("init"in NREUM&&"page_view_timing"in NREUM.init&&"enabled"in NREUM.init.page_view_timing&&NREUM.init.page_view_timing.enabled===!1)){var o,a=n("handle"),f=n("loader"),c=NREUM.o.EV;if("PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){o=new PerformanceObserver(r);try{o.observe({entryTypes:["paint"]})}catch(u){}}if("addEventListener"in document){var s=!1,p=["click","keydown","mousedown","pointerdown","touchstart"];p.forEach(function(n){document.addEventListener(n,i,!1)})}}},{}],3:[function(n,e,t){function r(n,e){if(!i)return!1;if(n!==i)return!1;if(!e)return!0;if(!o)return!1;for(var t=o.split("."),r=e.split("."),a=0;a<r.length;a++)if(r[a]!==t[a])return!1;return!0}var i=null,o=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var f=navigator.userAgent,c=f.match(a);c&&f.indexOf("Chrome")===-1&&f.indexOf("Chromium")===-1&&(i="Safari",o=c[1])}e.exports={agent:i,version:o,match:r}},{}],4:[function(n,e,t){function r(n,e){var t=[],r="",o=0;for(r in n)i.call(n,r)&&(t[o]=e(r,n[r]),o+=1);return t}var i=Object.prototype.hasOwnProperty;e.exports=r},{}],5:[function(n,e,t){function r(n,e,t){e||(e=0),"undefined"==typeof t&&(t=n?n.length:0);for(var r=-1,i=t-e||0,o=Array(i<0?0:i);++r<i;)o[r]=n[e+r];return o}e.exports=r},{}],6:[function(n,e,t){e.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(n,e,t){function r(){}function i(n){function e(n){return n&&n instanceof r?n:n?c(n,f,o):o()}function t(t,r,i,o){if(!d.aborted||o){n&&n(t,r,i);for(var a=e(i),f=v(t),c=f.length,u=0;u<c;u++)f[u].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(n,e){h[n]=v(n).concat(e)}function m(n,e){var t=h[n];if(t)for(var r=0;r<t.length;r++)t[r]===e&&t.splice(r,1)}function v(n){return h[n]||[]}function g(n){return p[n]=p[n]||i(t)}function w(n,e){u(n,function(n,t){e=e||"feature",y[t]=e,e in s||(s[e]=[])})}var h={},y={},b={on:l,addEventListener:l,removeEventListener:m,emit:t,get:g,listeners:v,context:e,buffer:w,abort:a,aborted:!1};return b}function o(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var f="nr@context",c=n("gos"),u=n(4),s={},p={},d=e.exports=i();d.backlog=s},{}],gos:[function(n,e,t){function r(n,e,t){if(i.call(n,e))return n[e];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(n,e,{value:r,writable:!0,enumerable:!1}),r}catch(o){}return n[e]=r,r}var i=Object.prototype.hasOwnProperty;e.exports=r},{}],handle:[function(n,e,t){function r(n,e,t,r){i.buffer([n],r),i.emit(n,e,t)}var i=n("ee").get("handle");e.exports=r,r.ee=i},{}],id:[function(n,e,t){function r(n){var e=typeof n;return!n||"object"!==e&&"function"!==e?-1:n===window?0:a(n,o,function(){return i++})}var i=1,o="nr@id",a=n("gos");e.exports=r},{}],loader:[function(n,e,t){function r(){if(!x++){var n=E.info=NREUM.info,e=l.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(n&&n.licenseKey&&n.applicationID&&e))return s.abort();u(y,function(e,t){n[e]||(n[e]=t)}),c("mark",["onload",a()+E.offset],null,"api");var t=l.createElement("script");t.src="https://"+n.agent,e.parentNode.insertBefore(t,e)}}function i(){"complete"===l.readyState&&o()}function o(){c("mark",["domContent",a()+E.offset],null,"api")}function a(){return O.exists&&performance.now?Math.round(performance.now()):(f=Math.max((new Date).getTime(),f))-E.offset}var f=(new Date).getTime(),c=n("handle"),u=n(4),s=n("ee"),p=n(3),d=window,l=d.document,m="addEventListener",v="attachEvent",g=d.XMLHttpRequest,w=g&&g.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:g,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var h=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1153.min.js"},b=g&&w&&w[m]&&!/CriOS/.test(navigator.userAgent),E=e.exports={offset:f,now:a,origin:h,features:{},xhrWrappable:b,userAgent:p};n(1),n(2),l[m]?(l[m]("DOMContentLoaded",o,!1),d[m]("load",r,!1)):(l[v]("onreadystatechange",i),d[v]("onload",r)),c("mark",["firstbyte",f],null,"api");var x=0,O=n(6)},{}],"wrap-function":[function(n,e,t){function r(n){return!(n&&n instanceof Function&&n.apply&&!n[a])}var i=n("ee"),o=n(5),a="nr@original",f=Object.prototype.hasOwnProperty,c=!1;e.exports=function(n,e){function t(n,e,t,i){function nrWrapper(){var r,a,f,c;try{a=this,r=o(arguments),f="function"==typeof t?t(r,a):t||{}}catch(u){d([u,"",[r,a,i],f])}s(e+"start",[r,a,i],f);try{return c=n.apply(a,r)}catch(p){throw s(e+"err",[r,a,p],f),p}finally{s(e+"end",[r,a,c],f)}}return r(n)?n:(e||(e=""),nrWrapper[a]=n,p(n,nrWrapper),nrWrapper)}function u(n,e,i,o){i||(i="");var a,f,c,u="-"===i.charAt(0);for(c=0;c<e.length;c++)f=e[c],a=n[f],r(a)||(n[f]=t(a,u?f+i:i,o,f))}function s(t,r,i){if(!c||e){var o=c;c=!0;try{n.emit(t,r,i,e)}catch(a){d([a,t,r,i])}c=o}}function p(n,e){if(Object.defineProperty&&Object.keys)try{var t=Object.keys(n);return t.forEach(function(t){Object.defineProperty(e,t,{get:function(){return n[t]},set:function(e){return n[t]=e,e}})}),e}catch(r){d([r])}for(var i in n)f.call(n,i)&&(e[i]=n[i]);return e}function d(e){try{n.emit("internal-error",e)}catch(t){}}return n||(n=i),t.inPlace=u,t.flag=a,t}},{}]},{},["loader"]);</script><link rel="apple-touch-icon" href="https://learning.oreilly.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://learning.oreilly.com/favicon.ico" type="image/x-icon"><link href="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/css" rel="stylesheet" type="text/css"><title>2. Supervised Learning - Introduction to Machine Learning with Python</title><link rel="stylesheet" href="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/output.68851547a55f.css" type="text/css"><link rel="stylesheet" type="text/css" href="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:2em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content p[data-type="footnote"]{font-size:90% !important;line-height:1.2em !important;margin-left:2.5em !important;text-indent:-2.3em !important}#sbo-rt-content p[data-type="footnote"] sup{display:inline-block !important;position:static !important;width:2em !important;text-align:right !important;font-size:100% !important;padding-right:.5em !important}#sbo-rt-content p[data-type="footnote"] a[href$="-marker"]{font-family:sans-serif !important;font-size:90% !important;color:#8e0012 !important}#sbo-rt-content p[data-type="footnote"] a[data-type="xref"]{margin:0 !important;padding:0 !important;text-indent:0 !important}#sbo-rt-content a[data-type="noteref"]{font-family:sans-serif !important;color:#8e0012;margin-left:0;padding-left:0}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content table.border tbody>tr:last-child>td{border-bottom:transparent}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content .width-10{width:10vw !important}#sbo-rt-content .width-20{width:20vw !important}#sbo-rt-content .width-30{width:30vw !important}#sbo-rt-content .width-40{width:40vw !important}#sbo-rt-content .width-50{width:50vw !important}#sbo-rt-content .width-60{width:60vw !important}#sbo-rt-content .width-70{width:70vw !important}#sbo-rt-content .width-80{width:80vw !important}#sbo-rt-content .width-90{width:90vw !important}#sbo-rt-content .width-full,#sbo-rt-content .width-100{width:100vw !important}
    </style><script> // <![CDATA[
    var g = {
      position_cache: {
        
          "chapter": "/api/v1/book/9781449369880/chapter/ch02.html",
          "book_id": "9781449369880",
          "chapter_uri": "ch02.html",
          "position": 0,
          "user_uuid": "f0b2a002-7968-4745-a334-a519b5a26e85",
          "next_chapter_uri": "/library/view/introduction-to-machine/9781449369880/ch03.html"
        
      },
      title: "Introduction to Machine Learning with Python",
      author_list: "Sarah Guido, Andreas C. Müller",
      format: "book",
      source: "application/epub+zip",
      is_system_book: true,
      is_public: false,
      loaded_from_server: true,
      allow_scripts: false,
      has_mathml: true
    };
    // ]]></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/modernizr.8e35451ddb64.js"></script><script>
    
      

      
        
          window.PUBLIC_ANNOTATIONS = true;
        
      

      window.MOBILE_PUBLIC_ANNOTATIONS = false;

    

    
      window.PRIVACY_CONTROL_OVERRIDE = false;
    

      window.PRIVACY_CONTROL_SWITCH = true;

      window.PUBLISHER_PAGES = true;

      window.SBO = {
        "constants": {
          "SITB_ENDPOINT": "/api/v2/sitb/",
          "SEARCH_SELECT_ENDPOINT": "https://learning.oreilly.com/api/v2/search/select/",
          "ENABLE_ONLINE_TRAINING": true
        }
      };
  </script><link rel="canonical" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html"><meta name="description" content=" Chapter 2. Supervised Learning As we mentioned earlier, supervised machine learning is one of the most commonly used and successful types of machine learning. In this chapter, we will describe ... "><meta property="og:title" content="2. Supervised Learning"><meta itemprop="isPartOf" content="/library/view/introduction-to-machine/9781449369880/"><meta itemprop="name" content="2. Supervised Learning"><meta property="og:url" itemprop="url" content="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://learning.oreilly.com/library/cover/9781449369880/"><meta property="og:description" itemprop="description" content=" Chapter 2. Supervised Learning As we mentioned earlier, supervised machine learning is one of the most commonly used and successful types of machine learning. In this chapter, we will describe ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O&#39;Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449369415"><meta property="og:book:author" itemprop="author" content="Sarah Guido"><meta property="og:book:author" itemprop="author" content="Andreas C. Müller"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }"></style><noscript><meta http-equiv="refresh" content="0; url=/library/no-js/" /></noscript><script>
    var dataLayer = window.dataLayer || [];

    
      window.medalliaVsgUserIdentifier = 'f0b2a002-7968-4745-a334-a519b5a26e85';
      dataLayer.push({userIdentifier: 'f0b2a002-7968-4745-a334-a519b5a26e85'});
      dataLayer.push({loggedIn: 'yes'});

      
        window.medalliaVsgAccountIdentifier = '66639016-5950-4bd9-9754-6697effed455';
        

        window.medalliaVsgIsIndividual = true;
        
          
          dataLayer.push({learningAccountType: 'free trial'});
          
        

        
      
    

    (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-5P4V6Z');
    (function () {
      var VERSION = 'V1.1';
      var AUTHOR = 'Awwad';
      if (!window.GtmHelper)
        window.GtmHelper = function () {
          var instance = this;
          var loc = document.location;
          this.version = VERSION;
          this.author = AUTHOR;
          this.readCookie = function (name) {
            var nameEQ = name + "=";
            var ca = document.cookie.split(';');
            for (var i = 0; i < ca.length; i++) {
              var c = ca[i];
              while (c.charAt(0) == ' ') c = c.substring(1, c.length);
              if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length, c.length);
            }
            return null;
          };
          this.createCookie = function (name, value, days, cookieDomain) {
            var domain = "";
            var expires = "";

            if (days) {
              var date = new Date();
              date.setTime(date.getTime() + Math.ceil(days * 24 * 60 * 60 * 1000));
              var expires = " expires=" + date.toGMTString() + ";";
            }

            if (typeof (cookieDomain) != 'undefined')
              domain = " domain=" + cookieDomain + "; ";

            document.cookie = name + "=" + value + ";" + expires + domain + "path=/";
          };

          this.isDuplicated = function (currentTransactionId) {
            // the previous transaction id:
            var previousTransIdValue = this.readCookie("previousTransId");

            if (currentTransactionId === previousTransIdValue) {
              return true; // Duplication
            } else {
              return false;
            }
          };
        }
    })()
  </script><script defer="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/vendor.65ffd92d2889.js"></script><script defer="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/reader.94483f00dede.js"></script><script async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/MathJax.js"></script><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f(1).txt"></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f(2).txt"></script><script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/f(3).txt"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style></head>


<body class="reading sidenav  scalefonts subscribe-panel library nav-collapsed"><div id="MathJax_Message" style="display: none;"></div>

    
  <noscript> 
    <iframe src="//www.googletagmanager.com/ns.html?id=GTM-5P4V6Z"
            height="0" width="0"
            style="display:none;visibility:hidden">
    </iframe>
  </noscript>



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        

  


<a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li><a href="https://learning.oreilly.com/home/" class="l0 nav-icn"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.738 14H9.254v-3.676a.617.617 0 0 0-.621-.613H7.39a.617.617 0 0 0-.62.613V14H4.284a.617.617 0 0 1-.622-.613V10.22c0-.327.132-.64.367-.87l3.547-3.493a.627.627 0 0 1 .875 0l3.54 3.499c.234.229.366.54.367.864v3.167a.617.617 0 0 1-.62.613zM7.57 2.181a.625.625 0 0 1 .882 0l5.77 5.692-.93.92-5.28-5.209-5.28 5.208-.932-.919 5.77-5.692z"></path></svg><span>Home</span></a></li><li class="search"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M8,8 C6.34321755,8 5.00013,6.65691245 5.00013,5.00013 C5.00013,3.34334755 6.34321755,2.00026001 8,2.00026001 C9.65678245,2.00026001 10.99987,3.34334755 10.99987,5.00013 C10.99987,6.65691245 9.65678245,8 8,8 Z M2.33024571,11.3523547 L2.33774538,11.3523547 C3.7622187,9.70968996 5.82947484,8.76608166 8.00374984,8.76608166 C10.1780248,8.76608166 12.245281,9.70968996 13.6697543,11.3523547 C13.8892083,11.6177474 14.0062813,11.9530021 13.99974,12.2973138 L13.99974,13.99974 L2.00026001,13.99974 L2.00026001,12.2973138 C1.99371867,11.9530021 2.11079172,11.6177474 2.33024571,11.3523547 Z" id="path-1"></path></svg><span>Your O'Reilly</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/profile/" class="l2 nav-icn"><span>Profile</span></a></li><li><a href="https://learning.oreilly.com/history/" class="l2 nav-icn"><span>History</span></a></li><li><a href="https://learning.oreilly.com/playlists/" class="l2 nav-icn"><span>Playlists</span></a></li><li><a href="https://learning.oreilly.com/u/f0b2a002-7968-4745-a334-a519b5a26e85/" class="l2 nav-icn"><span>Highlights</span></a></li></ul></li><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.564 2.263l2.172 2.174c.17.168.264.397.264.636V11a.6.6 0 0 1-.6.6h-.6V6.2h-6V2.6a.6.6 0 0 1 .6-.6h3.527c.239 0 .468.095.637.263zM2.6 14a.6.6 0 0 1-.6-.6V6.8a.6.6 0 0 1 .6-.6h1.903a1.2 1.2 0 0 1 .849.352L6.2 7.4H11a.6.6 0 0 1 .6.6v5.4a.6.6 0 0 1-.6.6H2.6zM11 5h1.8L11 3.2V5z"></path></svg><span>Featured</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/resource-centers/" class="l2 nav-icn"><span>Resource Centers</span></a></li><li><a href="https://learning.oreilly.com/search/?query=&amp;extended_publisher_data=true&amp;highlight=true&amp;include_assessments=false&amp;include_case_studies=true&amp;include_courses=true&amp;include_orioles=true&amp;include_playlists=true&amp;include_collections=true&amp;include_notebooks=true&amp;is_academic_institution_account=false&amp;source=user&amp;formats=collection&amp;sort=date_added&amp;facet_json=true&amp;page=0&amp;collection_type=expert" class="l2 nav-icn"><span>Expert Playlists</span></a></li></ul></li><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.6467109,4.35328907 L14.7964612,7.51003884 C15.0678463,7.78304342 15.0678463,8.22395603 14.7964612,8.49696061 L11.6467109,11.6467109 L10.6597892,10.6597892 L13.3055794,8 L10.6597892,5.34021084 L11.6467109,4.35328907 Z M4.35328907,11.6467109 L1.20353875,8.48996116 C0.932153749,8.21695658 0.932153749,7.77604397 1.20353875,7.50303939 L4.35328907,4.35328907 L5.34021084,5.34021084 L2.69442057,8 L5.34021084,10.6597892 L4.35328907,11.6467109 Z M5.84417089,11.4997226 L8.67194674,4.50027742 L10.1838269,4.50027742 L7.35605105,11.4997226 L5.84417089,11.4997226 Z" id="Mask"></path></svg><span>Practice</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/scenarios/" class="l2 nav-icn"><span>Katacoda Scenarios</span></a></li><li><a href="https://learning.oreilly.com/interactive/#notebooks" class="l2 nav-icn"><span>Jupyter Notebooks</span></a></li></ul></li><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><!--?xml version="1.0" encoding="UTF-8"?--><svg width="16px" height="16px" viewBox="0 0 16 16" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M10.0440191,12.1582649 C10.177209,12.3351976 10.2476998,12.552011 10.2434767,12.7742985 L10.2434767,13.9482829 L1.96869684,13.9482829 L1.96869684,12.7742985 C1.96418597,12.536863 2.04491894,12.3056731 2.19625329,12.1226596 L2.20142503,12.1226596 C3.18373453,10.9898864 4.60930376,10.339179 6.10867266,10.339179 C7.30793519,10.339179 8.45998426,10.7554703 9.37576632,11.5017585 L14.3434936,11.5017585 L14.3434936,3.65650645 L1.65650645,3.65650645 L1.65650645,11.5017585 L2.06682298,11.5017585 L1.82063306,12.1582649 L1.32825322,12.1582649 C1.14771395,12.1582649 1,12.010551 1,11.8300117 L1,3.32825322 C1,3.14771395 1.14771395,3 1.32825322,3 L14.6717468,3 C14.852286,3 15,3.14771395 15,3.32825322 L15,11.8300117 C15,12.010551 14.852286,12.1582649 14.6717468,12.1582649 L10.0440191,12.1582649 Z M12.5483683,7.16419284 C12.6159362,7.20929881 12.6563112,7.2853432 12.6558247,7.36658194 C12.6553382,7.44782068 12.6140555,7.5233761 12.5459522,7.56766966 L10.4270949,8.91581376 C10.3903366,8.94010203 10.3431312,8.9419685 10.3045699,8.9206583 C10.2660086,8.89934811 10.2424712,8.85838696 10.2434767,8.81434055 L10.2434767,5.91510593 C10.2420711,5.86982855 10.2666257,5.82771857 10.3067229,5.80664182 C10.3468202,5.78556507 10.3954292,5.7892172 10.431927,5.81604875 L12.5483683,7.16419284 Z M6.10608679,9.81089299 C4.9635781,9.81089299 4.03739181,8.8847067 4.03739181,7.74219802 C4.03739181,6.59968933 4.9635781,5.67350304 6.10608679,5.67350304 C7.24859547,5.67350304 8.17478176,6.59968933 8.17478176,7.74219802 C8.17478176,8.8847067 7.24859547,9.81089299 6.10608679,9.81089299 Z"></path></svg><span>Sandboxes</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/scenarios/kubernetes-sandbox/9781492062820/" class="l2 nav-icn"><span>Kubernetes</span></a></li><li><a href="https://learning.oreilly.com/scenarios/python-sandbox/9781492062844/" class="l2 nav-icn"><span>Python</span></a></li><li><a href="https://learning.oreilly.com/scenarios/tensorflow-sandbox/9781492062851/" class="l2 nav-icn"><span>TensorFlow</span></a></li><li><a href="https://learning.oreilly.com/scenarios/ubuntu-sandbox/9781492062837/" class="l2 nav-icn"><span>Ubuntu</span></a></li></ul></li><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z"></path></g></svg><span>Explore</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/topics/" class="l2 nav-icn"><span>All Topics</span></a></li><li><a href="https://learning.oreilly.com/search/?query=&amp;extended_publisher_data=true&amp;highlight=true&amp;include_assessments=false&amp;include_case_studies=true&amp;include_courses=true&amp;include_orioles=true&amp;include_playlists=true&amp;include_collections=true&amp;include_notebooks=true&amp;is_academic_institution_account=false&amp;source=user&amp;formats=book&amp;formats=case%20study&amp;formats=learning%20path&amp;formats=live%20online%20training&amp;formats=notebook&amp;formats=oriole&amp;formats=video&amp;sort=popularity&amp;facet_json=true&amp;page=0&amp;collection_type=expert" class="l2 nav-icn"><span>Most Popular Titles</span></a></li><li><a href="https://learning.oreilly.com/r/" class="l2 nav-icn"><span>Recommendations</span></a></li><li><a href="https://learning.oreilly.com/search/?query=&amp;extended_publisher_data=true&amp;highlight=true&amp;include_assessments=false&amp;include_case_studies=true&amp;include_courses=true&amp;include_orioles=true&amp;include_playlists=true&amp;include_collections=true&amp;include_notebooks=true&amp;is_academic_institution_account=false&amp;source=user&amp;formats=book&amp;sort=publication_date&amp;facet_json=true&amp;page=0" class="l2 nav-icn"><span>Early Release</span></a></li><li><a href="https://learning.oreilly.com/playlists/discover/" class="l2 nav-icn"><span>Playlists</span></a></li></ul></li><li class="flyout-parent"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="l1 nav-icn "><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M13.4 9.8a.6.6 0 0 1 .6.6v1.2h-1.2l-.6 2.4H3.8l-.6-2.4H2v-1.2a.6.6 0 0 1 .6-.6h10.8zM8 5.6a1.8 1.8 0 1 1-.001-3.599A1.8 1.8 0 0 1 8 5.6zm3.6 2.35v.95H4.4v-.95c0-.167.066-.33.19-.443A5.08 5.08 0 0 1 8 6.2a5.08 5.08 0 0 1 3.41 1.307c.124.113.19.276.19.444z"></path></svg><span>Attend</span></a><ul class="flyout"><li><a href="https://learning.oreilly.com/live-training/" class="l2 nav-icn"><span>Live Training</span></a></li><li><a href="https://www.oreilly.com/conferences/?discount=learn" class="l2 nav-icn" target="&quot;_blank&quot;"><span>Conferences</span></a></li></ul></li><li><a href="https://get.oreilly.com/email-signup.html" class="l1 nav-icn " target="&quot;_blank&quot;"><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M11.564 2.263l2.172 2.174c.17.168.264.397.264.636V11a.6.6 0 0 1-.6.6h-.6V6.2h-6V2.6a.6.6 0 0 1 .6-.6h3.527c.239 0 .468.095.637.263zM2.6 14a.6.6 0 0 1-.6-.6V6.8a.6.6 0 0 1 .6-.6h1.903a1.2 1.2 0 0 1 .849.352L6.2 7.4H11a.6.6 0 0 1 .6.6v5.4a.6.6 0 0 1-.6.6H2.6zM11 5h1.8L11 3.2V5z"></path></svg><span>Newsletters</span></a></li><li><a href="https://learning.oreilly.com/u/preferences/" class="l1 nav-icn "><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://learning.oreilly.com/public/support/" class="l1 nav-icn "><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M7.363 6.656a2.692 2.692 0 0 1-2.681-2.703c0-1.493 1.2-2.703 2.681-2.703a2.692 2.692 0 0 1 2.682 2.703c0 1.493-1.2 2.703-2.682 2.703zm4.023 2.027c-1.852 0-3.352 1.513-3.352 3.379H2v-1.534c-.006-.31.099-.612.295-.852a6.666 6.666 0 0 1 9.09-.993zm-.543.676h1.12v.304c.003.284.16.543.408.676a.766.766 0 0 0 .77 0l.303-.176.556.966-.302.176a.772.772 0 0 0-.362.676v.08a.772.772 0 0 0 .362.677l.302.21-.556.965-.302-.175a.766.766 0 0 0-.771 0 .778.778 0 0 0-.409.675v.352h-1.106v-.372a.778.778 0 0 0-.409-.676.766.766 0 0 0-.77 0l-.303.176-.556-.912.302-.176a.772.772 0 0 0 .362-.676v-.04-.04a.772.772 0 0 0-.362-.676l-.302-.176.556-.966.289.155a.766.766 0 0 0 .77 0 .778.778 0 0 0 .41-.676V9.36zm1.562 2.703c0-.271-.108-.531-.3-.722a1.001 1.001 0 0 0-.72-.292 1.01 1.01 0 0 0-.992 1.023 1.01 1.01 0 0 0 1.01 1.004 1.01 1.01 0 0 0 1.002-1.013z"></path></svg><span>Support</span></a></li><li><a href="https://learning.oreilly.com/accounts/logout/" class="l1 nav-icn "><svg width="16" height="16" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M2.613 12.63A.607.607 0 0 1 2 12.03V3.602C2 3.269 2.274 3 2.613 3h5.515v1.204H3.226v7.223h4.902v1.203H2.613zM5.677 9.02V6.611h4.903V4.926a.301.301 0 0 1 .19-.274.31.31 0 0 1 .33.063l2.722 2.673a.594.594 0 0 1 0 .849L11.1 10.909a.31.31 0 0 1-.331.063.301.301 0 0 1-.19-.274V9.02H5.677z"></path></svg><span>Sign Out</span></a></li></ul></div></li></ul></nav></header>



      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Introduction to Machine Learning with Python
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781449369880/chapter/ch02.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html&amp;text=Introduction%20to%20Machine%20Learning%20with%20Python&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%202.%20Supervised%20Learning&amp;body=https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html%0D%0Afrom%20Introduction%20to%20Machine%20Learning%20with%20Python%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    
    <section role="document">
        
        




  <script defer="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/djangoMessagesPage.86bcb4095734.js"></script>


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch01.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">1. Introduction</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">3. Unsupervised Learning and Preprocessing</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Supervised Learning"><div class="chapter" id="supervised-learning">
<h1><span class="label">Chapter 2. </span>Supervised Learning</h1>


<p><a data-type="indexterm" data-primary="machine learning" data-secondary="supervised learning" id="MLsuper2"></a><a data-type="indexterm" data-primary="supervised learning" id="super2"></a><a data-type="indexterm" data-primary="models" data-see="also algorithms" id="idm45613687178392"></a>
As we mentioned earlier, supervised machine learning is one
of the most commonly used and successful types of machine learning. In
this chapter, we will describe supervised learning in more detail and
explain several popular supervised learning algorithms. We already saw
an application of supervised machine learning in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch01.html#introduction">Chapter&nbsp;1</a>:
classifying iris flowers into several species using physical
measurements of the <span class="keep-together">flowers</span>.</p>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="goals for" id="idm45613687074152"></a>
Remember that supervised learning is used whenever we want to predict a
certain outcome from a given input, and we have examples of input/output
pairs. We build a machine learning model from these input/output pairs,
which comprise our training set. Our goal is to make accurate
predictions for new, never-before-seen data. Supervised learning often
requires human effort to build the training set, but afterward automates
and often speeds up an otherwise laborious or infeasible task.</p>






<section data-type="sect1" data-pdf-bookmark="2.1 Classification and Regression"><div class="sect1" id="classification-and-regression">
<h1>2.1 Classification and Regression</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-see="also classification problems; regression problems" id="idm45613687070808"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="goals for" id="idm45613687069864"></a>
There are two major types of supervised machine learning problems,
called <em>classification</em> and <em>regression</em>.</p>

<p><a data-type="indexterm" data-primary="class labels" id="idm45613687067640"></a><a data-type="indexterm" data-primary="binary classification" id="idm45613687066744"></a><a data-type="indexterm" data-primary="multiclass classification" data-secondary="vs. binary classification" data-secondary-sortas="binary classification" id="idm45613687066072"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="binary vs. multiclass" id="idm45613687064872"></a>
In classification, the goal is to predict a <em>class label</em>, which is a
choice from a predefined list of possibilities. In
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch01.html#introduction">Chapter&nbsp;1</a> we used the example of
classifying irises into one of three possible species. Classification is
sometimes separated into <em>binary classification</em>, which is the special
case of distinguishing between exactly two classes, and <em>multiclass
classification</em>, which is classification between more than two classes.
You can think of binary classification as trying to answer a yes/no
question. Classifying emails as either spam or not spam is an example of
a binary classification problem. In this binary classification task, the
yes/no question being asked would be “Is this email spam?”</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="indexterm" data-primary="positive class" id="idm45613687060568"></a><a data-type="indexterm" data-primary="negative class" id="idm45613687059864"></a>
In binary classification we often speak of one class being the
<em>positive</em> class and the other class being the <em>negative</em> class. Here,
positive doesn’t represent having benefit or value, but rather what the
object of the study is. So, when looking for spam, “positive” could mean
the spam class. Which of the two classes is called positive is often a
subjective matter, and specific to the domain.</p>
</div>

<p><a data-type="indexterm" data-primary="iris classification application" data-secondary="multiclass problem" id="idm45613687057544"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="examples of" id="idm45613687056504"></a>
The iris example, on the other hand, is an example of a multiclass
classification problem. Another example is predicting what language a
website is in from the text on the website. The classes here would be a
pre-defined list of possible languages.</p>

<p><a data-type="indexterm" data-primary="regression problems" data-secondary="goals for" id="idm45613687054904"></a><a data-type="indexterm" data-primary="floating-point numbers" id="idm45613687053736"></a><a data-type="indexterm" data-primary="real numbers" id="idm45613687053064"></a>
For regression tasks, the goal is to predict a continuous number, or a
<em>floating-point number</em> in programming terms (or <em>real number</em> in
mathematical terms).
<a data-type="indexterm" data-primary="regression problems" data-secondary="examples of" id="idm45613687051464"></a>
Predicting a person’s annual income from their
education, their age, and where they live is an example of a regression
task. When predicting income, the predicted value is an <em>amount</em>, and
can be any number in a given range. Another example of a regression task
is predicting the yield of a corn farm given attributes such as previous
yields, weather, and number of employees working on the farm. The yield
again can be an arbitrary number.</p>

<p><a data-type="indexterm" data-primary="classification problems" data-secondary="vs. regression problems" data-secondary-sortas="regression problems" id="idm45613687049352"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="vs. classification problems" data-secondary-sortas="classification problems" id="idm45613687048104"></a>
An easy way to distinguish between classification and regression tasks
is to ask whether there is some kind of continuity in the output. If
there is continuity between possible outcomes, then the problem is a
regression problem. Think about predicting annual income. There is a
clear continuity in the output. Whether a person makes $40,000 or
$40,001 a year does not make a tangible difference, even though these
are different amounts of money; if our algorithm predicts $39,999 or
$40,001 when it should have predicted $40,000, we don’t mind that much.</p>

<p>By contrast, for the task of recognizing the language of a website
(which is a classification problem), there is no matter of degree. A
website is in one language, or it is in another. There is no continuity
between languages, and there is no language that is <em>between</em> English
and French.<sup><a data-type="noteref" id="idm45613687045144-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613687045144" class="totri-footnote">1</a></sup></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="2.2 Generalization, Overfitting, and Underfitting"><div class="sect1" id="generalization-overfitting-and-underfitting">
<h1>2.2 Generalization, Overfitting, and Underfitting</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="generalization" id="idm45613687042472"></a><a data-type="indexterm" data-primary="generalization" data-secondary="building models for" id="idm45613687041496"></a><a data-type="indexterm" data-primary="models" data-secondary="capable of generalization" id="idm45613687040552"></a>
In supervised learning, we want to build a model on the training data
and then be able to make accurate predictions on new, unseen data that
has the same characteristics as the training set that we used. If a
model is able to make accurate predictions on unseen data, we say it is
able to <em>generalize</em> from the training set to the test set. We want to
build a model that is able to generalize as accurately as possible.</p>

<p>Usually we build a model in such a way that it can make accurate
predictions on the training set. If the training and test sets have
enough in common, we expect the model to also be accurate on the test
set. However, there are some cases where this can go wrong. For example,
if we allow ourselves to build very complex models, we can always be as
accurate as we like on the training set.</p>

<p><a data-type="indexterm" data-primary="generalization" data-secondary="examples of" id="idm45613687037592"></a>
Let’s take a look at a made-up example to illustrate this point. Say a
novice data scientist wants to predict whether a customer will buy a
boat, given records of previous boat buyers and customers who we know
are not interested in buying a boat.<sup><a data-type="noteref" id="idm45613687036216-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613687036216" class="totri-footnote">2</a></sup> The goal is to send out promotional emails to people who are
likely to actually make a purchase, but not bother those customers who
won’t be interested.</p>

<p>Suppose we have the customer records shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#table0201">Table&nbsp;2-1</a>.</p>
<table id="table0201">
<caption><span class="label">Table 2-1. </span>Example data about customers</caption>
<thead>
<tr>
<th>Age</th>
<th>Number of <br>cars owned</th>
<th>Owns house</th>
<th>Number of children</th>
<th>Marital status</th>
<th>Owns a dog</th>
<th>Bought a boat</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>66</p></td>
<td><p>1</p></td>
<td><p>yes</p></td>
<td><p>2</p></td>
<td><p>widowed</p></td>
<td><p>no</p></td>
<td><p>yes</p></td>
</tr>
<tr>
<td><p>52</p></td>
<td><p>2</p></td>
<td><p>yes</p></td>
<td><p>3</p></td>
<td><p>married</p></td>
<td><p>no</p></td>
<td><p>yes</p></td>
</tr>
<tr>
<td><p>22</p></td>
<td><p>0</p></td>
<td><p>no</p></td>
<td><p>0</p></td>
<td><p>married</p></td>
<td><p>yes</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>25</p></td>
<td><p>1</p></td>
<td><p>no</p></td>
<td><p>1</p></td>
<td><p>single</p></td>
<td><p>no</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>44</p></td>
<td><p>0</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>divorced</p></td>
<td><p>yes</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>39</p></td>
<td><p>1</p></td>
<td><p>yes</p></td>
<td><p>2</p></td>
<td><p>married</p></td>
<td><p>yes</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>26</p></td>
<td><p>1</p></td>
<td><p>no</p></td>
<td><p>2</p></td>
<td><p>single</p></td>
<td><p>no</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>40</p></td>
<td><p>3</p></td>
<td><p>yes</p></td>
<td><p>1</p></td>
<td><p>married</p></td>
<td><p>yes</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>53</p></td>
<td><p>2</p></td>
<td><p>yes</p></td>
<td><p>2</p></td>
<td><p>divorced</p></td>
<td><p>no</p></td>
<td><p>yes</p></td>
</tr>
<tr>
<td><p>64</p></td>
<td><p>2</p></td>
<td><p>yes</p></td>
<td><p>3</p></td>
<td><p>divorced</p></td>
<td><p>no</p></td>
<td><p>no</p></td>
</tr>
<tr>
<td><p>58</p></td>
<td><p>2</p></td>
<td><p>yes</p></td>
<td><p>2</p></td>
<td><p>married</p></td>
<td><p>yes</p></td>
<td><p>yes</p></td>
</tr>
<tr>
<td><p>33</p></td>
<td><p>1</p></td>
<td><p>no</p></td>
<td><p>1</p></td>
<td><p>single</p></td>
<td><p>no</p></td>
<td><p>no</p></td>
</tr>
</tbody>
</table>

<p>After looking at the data for a while, our novice data scientist comes
up with the following rule: “If the customer is older than 45, and has
less than 3 children or is not divorced, then they want to buy a boat.”
When asked how well this rule of his does, our data scientist answers,
“It’s 100 percent accurate!” And indeed, on the data that is in the
table, the rule is perfectly accurate. There are many possible rules we
could come up with that would explain perfectly if someone in this
dataset wants to buy a boat. No age appears twice in the data, so we
could say people who are 66, 52, 53, or 58 years old want to buy a boat,
while all others don’t. While we can make up many rules that work well
on this data, remember that we are not interested in making predictions
for this dataset; we already know the answers for these customers. We
want to know if <em>new customers</em> are likely to buy a boat. We therefore
want to find a rule that will work well for new customers, and achieving
100 percent accuracy on the training set does not help us there. We
might not expect that the rule our data scientist came up with will work
very well on new customers. It seems too complex, and it is supported by
very little data. For example, the “or is not divorced” part of the rule
hinges on a single customer.</p>

<p><a data-type="indexterm" data-primary="algorithms" data-secondary="evaluating" id="idm45613686943000"></a><a data-type="indexterm" data-primary="models" data-secondary="overfitting vs. underfitting" id="idm45613686941800"></a><a data-type="indexterm" data-primary="overfitting" id="idm45613686940888"></a><a data-type="indexterm" data-primary="underfitting" id="idm45613686940216"></a><a data-type="indexterm" data-primary="supervised learning" data-secondary="overfitting vs. underfitting" id="idm45613686939544"></a>
The only measure of whether an algorithm will perform well on new data
is the evaluation on the test set. However, intuitively<sup><a data-type="noteref" id="idm45613686938360-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613686938360" class="totri-footnote">3</a></sup> we expect simple models to
generalize better to new data. If the rule was “People older than 50
want to buy a boat,” and this would explain the behavior of all the
customers, we would trust it more than the rule involving children and
marital status in addition to age. Therefore, we always want to find the
simplest model. Building a model that is too complex for the amount of
information we have, as our novice data scientist did, is called
<em>overfitting</em>. Overfitting occurs when you fit a model too closely to
the particularities of the training set and obtain a model that works
well on the training set but is not able to generalize to new data. On
the other hand, if your model is too simple—say, “Everybody who owns a
house buys a boat”—then you might not be able to capture all the aspects
of and variability in the data, and your model will do badly even on the
training set. Choosing too simple a model is called <em>underfitting</em>.</p>

<p>The more complex we allow our model to be, the better we will be able to
predict on the training data. However, if our model becomes too complex,
we start focusing too much on each individual data point in our training
set, and the model will not generalize well to new data.</p>

<p>There is a sweet spot in between that will yield the best generalization
performance. This is the model we want to find.</p>

<p>The trade-off between overfitting and underfitting is illustrated in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>.</p>

<figure><div id="model_complexity" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0201.png" alt="model_complexity" width="1057" height="744" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0201.png">
<h6><span class="label">Figure 2-1. </span>Trade-off of model complexity against training and test accuracy</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="2.2.1 Relation of Model Complexity to Dataset Size"><div class="sect2" id="relation-of-model-complexity-to-dataset-size">
<h2>2.2.1 Relation of Model Complexity to Dataset Size</h2>

<p><a data-type="indexterm" data-primary="models" data-secondary="complexity vs. dataset size" id="idm45613686929720"></a><a data-type="indexterm" data-primary="supervised learning" data-secondary="model complexity vs. dataset size" id="idm45613686928728"></a><a data-type="indexterm" data-primary="data representation" data-secondary="model complexity vs. dataset size" id="idm45613686927768"></a>
It’s important to note that model complexity is intimately tied to the
variation of inputs contained in your training dataset: the larger
variety of data points your dataset contains, the more complex a model
you can use without overfitting. Usually, collecting more data points
will yield more variety, so larger datasets allow building more complex
models. However, simply duplicating the same data points or collecting
very similar data will not help.</p>

<p>Going back to the boat selling example, if we saw 10,000 more rows of
customer data, and all of them complied with the rule “If the customer
is older than 45, and has less than 3 children or is not divorced, then
they want to buy a boat,” we would be much more likely to believe this
to be a good rule than when it was developed using only the 12 rows in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#table0201">Table&nbsp;2-1</a>.</p>

<p>Having more data and building appropriately more complex models can
often work wonders for supervised learning tasks. In this book, we will
focus on working with datasets of fixed sizes. In the real world, you
often have the ability to decide how much data to collect, which might
be more beneficial than tweaking and tuning your model. Never
underestimate the power of more data.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="2.3 Supervised Machine Learning Algorithms"><div class="sect1" id="supervised-machine-learning-algorithms">
<h1>2.3 Supervised Machine Learning Algorithms</h1>

<p>We will now review the most popular machine learning algorithms and
explain how they learn from data and how they make predictions. We will
also discuss how the concept of model complexity plays out for each of
these models, and provide an overview of how each algorithm builds a
model. We will examine the strengths and weaknesses of each algorithm,
and what kind of data they can best be applied to. We will also explain
the meaning of the most important parameters and
options.<sup><a data-type="noteref" id="idm45613686921400-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613686921400" class="totri-footnote">4</a></sup> Many algorithms have a classification and a regression
variant, and we will describe both.</p>

<p>It is not necessary to read through the descriptions of each algorithm
in detail, but understanding the models will give you a better feeling
for the different ways machine learning algorithms can work. This
chapter can also be used as a reference guide, and you can come back to
it when you are unsure about the workings of any of the algorithms.</p>








<section data-type="sect2" data-pdf-bookmark="2.3.1 Some Sample Datasets"><div class="sect2" id="idm45613686918680">
<h2>2.3.1 Some Sample Datasets</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="sample datasets" id="SLsampled"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="sample datasets" id="Asampled2"></a>
We will use several datasets to illustrate the different algorithms.
Some of the datasets will be small and synthetic (meaning made-up),
designed to highlight particular aspects of the algorithms. Other
datasets will be large, real-world examples.</p>

<p><a data-type="indexterm" data-primary="synthetic datasets" id="idm45613686912968"></a><a data-type="indexterm" data-primary="test data/test sets" data-secondary="forge dataset" id="idm45613686912264"></a><a data-type="indexterm" data-primary="forge dataset" id="idm45613686911320"></a>
An example of a synthetic two-class classification dataset is the
<code>forge</code> dataset, which has two features. The following code creates a
scatter plot (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#x_shape">Figure&nbsp;2-2</a>) visualizing all of the data points in this dataset. The
plot has the first feature on the x-axis and the second feature on the
y-axis. As is always the case in scatter plots, each data point is
represented as one dot. The color and shape of the dot indicates its
class:</p>

<p><code><strong>In[1]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># generate dataset</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_forge</code><code class="p">()</code>
<code class="c1"># plot dataset</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Class 0"</code><code class="p">,</code> <code class="s2">"Class 1"</code><code class="p">],</code> <code class="n">loc</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"First feature"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Second feature"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"X.shape:"</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[1]:</strong></code></p>

<pre data-type="programlisting">X.shape: (26, 2)</pre>

<figure><div id="x_shape" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0202.png" alt="malp 0202" width="1595" height="1132" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0202.png">
<h6><span class="label">Figure 2-2. </span>Scatter plot of the forge dataset</h6>
</div></figure>

<p>As you can see from <code>X.shape</code>, this dataset consists of 26 data points,
with 2 features.</p>

<p><a data-type="indexterm" data-primary="regression problems" data-secondary="wave dataset illustration" id="idm45613686846552"></a><a data-type="indexterm" data-primary="wave dataset" id="idm45613686845416"></a><a data-type="indexterm" data-primary="test data/test sets" data-secondary="wave dataset" id="idm45613686844744"></a>
To illustrate regression algorithms, we will use the synthetic <code>wave</code>
dataset. The <code>wave</code> dataset has a single input feature and a continuous
target variable (or <em>response</em>) that we want to model. The plot created
here (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_feature">Figure&nbsp;2-3</a>) shows the single
feature on the x-axis and the regression target (the output) on the
y-axis:</p>

<p><code><strong>In[2]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_wave</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">40</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s1">'o'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Target"</code><code class="p">)</code></pre>

<figure><div id="single_feature" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0203.png" alt="malp 0203" width="1565" height="1101" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0203.png">
<h6><span class="label">Figure 2-3. </span>Plot of the wave dataset, with the x-axis showing the feature and the y-axis showing the regression target</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="low-dimensional datasets" id="idm45613686757160"></a><a data-type="indexterm" data-primary="high-dimensional datasets" id="idm45613686756488"></a>
We are using these very simple, low-dimensional datasets because we can
easily visualize them—a printed page has two dimensions, so data with
more than two features is hard to show. Any intuition derived from
datasets with few features (also called <em>low-dimensional</em> datasets)
might not hold in datasets with many features (<em>high-dimensional</em>
datasets). As long as you keep that in mind, inspecting algorithms on
low-dimensional datasets can be very instructive.</p>

<p><a data-type="indexterm" data-primary="Wisconsin Breast Cancer dataset" id="idm45613686754216"></a><a data-type="indexterm" data-primary="test data/test sets" data-secondary="Wisconsin Breast Cancer dataset" id="idm45613686753544"></a><a data-type="indexterm" data-primary="cancer dataset" id="idm45613686752632"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="cancer dataset" id="idm45613686751960"></a>
We will complement these small synthetic datasets with two real-world
datasets that are included in <code>scikit-learn</code>. One is the Wisconsin
Breast Cancer dataset (<code>cancer</code>, for short), which records clinical
measurements of breast cancer tumors. Each tumor is labeled as “benign”
(for harmless tumors) or “malignant” (for cancerous tumors), and the
task is to learn to predict whether a tumor is malignant based on the
measurements of the tissue.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_breast_cancer" id="idm45613686749320"></a>
The data can be loaded using the <code>load_breast_cancer</code> function from
<code>scikit-learn</code>:</p>

<p><code><strong>In[3]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_breast_cancer</code>
<code class="n">cancer</code> <code class="o">=</code> <code class="n">load_breast_cancer</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"cancer.keys():</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code></pre>

<p><code><strong>Out[3]:</strong></code></p>

<pre data-type="programlisting">cancer.keys():
 dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="indexterm" data-primary="Bunch objects" id="idm45613686735592"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="Bunch objects" id="idm45613686734696"></a>
 Datasets that are included in <code>scikit-learn</code> are usually stored
as <code>Bunch</code> objects, which contain some information about the dataset as
well as the actual data. All you need to know about <code>Bunch</code> objects is
that they behave like dictionaries, with the added benefit that you can
access values using a dot (as in <code>bunch.key</code> instead of <code>bunch['key']</code>).</p>
</div>

<p>The dataset consists of 569 data points, with 30 features each:</p>

<p><code><strong>In[4]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Shape of cancer data:"</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[4]:</strong></code></p>

<pre data-type="programlisting">Shape of cancer data: (569, 30)</pre>

<p>Of these 569 data points, 212 are labeled as malignant and 357 as
benign:</p>

<p><code><strong>In[5]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Sample counts per class:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code>
      <code class="p">{</code><code class="n">n</code><code class="p">:</code> <code class="n">v</code> <code class="k">for</code> <code class="n">n</code><code class="p">,</code> <code class="n">v</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">cancer</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">bincount</code><code class="p">(</code><code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">))})</code></pre>

<p><code><strong>Out[5]:</strong></code></p>

<pre data-type="programlisting">Sample counts per class:
 {'malignant': 212, 'benign': 357}</pre>

<p><a data-type="indexterm" data-primary="feature_names attribute" id="idm45613686649224"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="feature_names attribute" id="idm45613686648552"></a>
To get a description of the semantic meaning of each feature, we can
have a look at the <code>feature_names</code> attribute:</p>

<p><code><strong>In[6]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Feature names:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">)</code></pre>

<p><code><strong>Out[6]:</strong></code></p>

<pre data-type="programlisting">Feature names:
['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean smoothness' 'mean compactness' 'mean concavity'
 'mean concave points' 'mean symmetry' 'mean fractal dimension'
 'radius error' 'texture error' 'perimeter error' 'area error'
 'smoothness error' 'compactness error' 'concavity error'
 'concave points error' 'symmetry error' 'fractal dimension error'
 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
 'worst smoothness' 'worst compactness' 'worst concavity'
 'worst concave points' 'worst symmetry' 'worst fractal dimension']</pre>

<p>You can find out more about the data by reading <code>cancer.DESCR</code> if you
are interested.</p>

<p><a data-type="indexterm" data-primary="Boston Housing dataset" id="idm45613686638184"></a><a data-type="indexterm" data-primary="test data/test sets" data-secondary="Boston Housing dataset" id="idm45613686637480"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="Boston Housing dataset" id="idm45613686644312"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_boston" id="idm45613686643368"></a>
We will also be using a real-world regression dataset, the Boston
Housing dataset. The task associated with this dataset is to predict the
median value of homes in several Boston neighborhoods in the 1970s,
using information such as crime rate, proximity to the Charles River,
highway accessibility, and so on. The dataset contains 506 data points,
described by 13 features:</p>

<p><code><strong>In[7]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_boston</code>
<code class="n">boston</code> <code class="o">=</code> <code class="n">load_boston</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Data shape:"</code><code class="p">,</code> <code class="n">boston</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[7]:</strong></code></p>

<pre data-type="programlisting">Data shape: (506, 13)</pre>

<p><a data-type="indexterm" data-primary="interactions" id="idm45613690810856"></a><a data-type="indexterm" data-primary="feature extraction/feature engineering" data-secondary="defined" id="idm45613690809928"></a>
Again, you can get more information about the dataset by reading the
<code>DESCR</code> attribute of <code>boston</code>. For our purposes here, we will actually
expand this dataset by not only considering these 13 measurements as
input features, but also looking at all products (also called
<em>interactions</em>) between features. In other words, we will not only
consider crime rate and highway accessibility as features, but also the
product of crime rate and highway accessibility. Including derived
feature like these is called <em>feature engineering</em>, which we will
discuss in more detail in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch04.html#representing-data-and-engineering-features">Chapter&nbsp;4</a>. This derived dataset can be loaded using the
<code>load_extended_boston</code> function:</p>

<p><code><strong>In[8]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">load_extended_boston</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"X.shape:"</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[8]:</strong></code></p>

<pre data-type="programlisting">X.shape: (506, 104)</pre>

<p>The resulting 104 features are the 13 original features together with
the 91 possible combinations of two features within those 13 (with
replacement).<sup><a data-type="noteref" id="idm45613690758712-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613690758712" class="totri-footnote">5</a></sup></p>

<p>We will use these datasets to explain and illustrate the properties of
the different machine learning algorithms. But for now, let’s get to the
algorithms themselves. First, we will revisit the <em>k</em>-nearest neighbors (<em>k</em>-NN)
algorithm that we saw in the previous chapter.
<a data-type="indexterm" data-primary="" data-startref="SLsampled" id="idm45613690756520"></a><a data-type="indexterm" data-primary="" data-startref="Asampled2" id="idm45613690755544"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.2 k-Nearest Neighbors"><div class="sect2" id="k-nearest-neighbor">
<h2>2.3.2 k-Nearest Neighbors</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="k-nearest neighbors" id="SLalkn2"></a><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="predictions with" id="idm45613690752504"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="k-nearest neighbors" id="Aknn2"></a>
The <em>k</em>-NN algorithm is arguably the simplest machine
learning algorithm. Building the model consists only of storing the
training dataset. To make a prediction for a new data point, the
algorithm finds the closest data points in the training dataset—its
“nearest neighbors.”</p>










<section data-type="sect3" data-pdf-bookmark="k-Neighbors classification"><div class="sect3" id="k-neighbors-classification">
<h3>k-Neighbors classification</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="classification" id="kNNclass2"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="k-nearest neighbors" id="idm45613690745832"></a>
In its simplest version, the <em>k</em>-NN algorithm only considers exactly one
nearest neighbor, which is the closest training data point to the point
we want to make a prediction for. The prediction is then simply the
known output for this training point. <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#forge_one_neighbor">Figure&nbsp;2-4</a> illustrates this for the case of classification on
the <code>forge</code> dataset:</p>

<p><code><strong>In[9]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_knn_classification</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<figure><div id="forge_one_neighbor" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0204.png" alt="malp 0204" width="1533" height="1107" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0204.png">
<h6><span class="label">Figure 2-4. </span>Predictions made by the one-nearest-neighbor model on the forge dataset</h6>
</div></figure>

<p>Here, we added three new data points, shown as stars. For each of them,
we marked the closest point in the training set. The prediction of the
one-nearest-neighbor algorithm is the label of that point (shown by the
color of the cross).</p>

<p><a data-type="indexterm" data-primary="voting" id="idm45613690713288"></a>
Instead of considering only the closest neighbor,
we can also consider an arbitrary number, <em>k</em>, of neighbors.
This is where the name of the <em>k</em>-nearest neighbors algorithm comes
from. When considering more than one neighbor, we use <em>voting</em> to assign
a label. This means that for each test point, we count how many neighbors
belong to class 0 and how many neighbors belong to class 1. We then
assign the class that is more frequent: in other words, the majority
class among the <em>k</em>-nearest neighbors. The following example (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#three_closest_neighbors">Figure&nbsp;2-5</a>) uses the three
closest neighbors:</p>

<p><code><strong>In[10]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_knn_classification</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<figure><div id="three_closest_neighbors" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0205.png" alt="malp 0205" width="1533" height="1048" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0205.png">
<h6><span class="label">Figure 2-5. </span>Predictions made by the three-nearest-neighbors model on the forge dataset</h6>
</div></figure>

<p>Again, the prediction is shown as the color of the
cross. You can see that the prediction for the new data point at the top
left is not the same as the prediction when we used only one neighbor.</p>

<p>While this illustration is for a binary classification problem, this
method can be applied to datasets with any number of classes. For more
classes, we count how many neighbors belong to each class and again
predict the most common class.</p>

<p>Now let’s look at how we can apply the <em>k</em>-nearest neighbors
algorithm using <code>scikit-learn</code>. First, we split our data into a training
and a test set so we can evaluate generalization performance, as
discussed in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch01.html#introduction">Chapter&nbsp;1</a>:</p>

<p><code><strong>In[11]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_forge</code><code class="p">()</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="KNeighborsClassifier" id="SLCFkn2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="KNeighborsClassifier" id="Ckn2"></a>
Next, we import and instantiate the class. This is when we can set
parameters, like the number of neighbors to use. Here, we set it to 3:</p>

<p><code><strong>In[12]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<p>Now, we fit the classifier using the training set. For
<code>KNeighborsClassifier</code> this means storing the dataset, so we can compute
neighbors during prediction:</p>

<p><code><strong>In[13]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="predict method" id="idm45613690585704"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="predict method" id="idm45613690585096"></a>
To make predictions on the test data, we call the <code>predict</code> method. For
each data point in the test set, this computes its nearest neighbors in
the training set and finds the most common class among these:</p>

<p><code><strong>In[14]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Test set predictions:"</code><code class="p">,</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code></pre>

<p><code><strong>Out[14]:</strong></code></p>

<pre data-type="programlisting">Test set predictions: [1 0 1 0 1 0 0]</pre>

<p><a data-type="indexterm" data-primary="score method" id="idm45613690527256"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="score method" id="idm45613690526664"></a>
To evaluate how well our model generalizes, we can call the <code>score</code>
method with the test data together with the test labels:</p>

<p><code><strong>In[15]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Test set accuracy: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[15]:</strong></code></p>

<pre data-type="programlisting">Test set accuracy: 0.86</pre>

<p>We see that our model is about 86% accurate, meaning the model predicted
the class correctly for 86% of the samples in the test dataset.
<a data-type="indexterm" data-primary="" data-startref="kNNclass2" id="idm45613690491688"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Analyzing KNeighborsClassifier"><div class="sect3" id="analyzing-kneighborsclassifier">
<h3>Analyzing KNeighborsClassifier</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="analyzing KNeighborsClassifier" id="idm45613690489272"></a><a data-type="indexterm" data-primary="decision boundaries" id="idm45613690488360"></a>
For two-dimensional datasets, we can also illustrate the prediction for
all possible test points in the xy-plane. We color the plane according to
the class that would be assigned to a point in this region. This lets us
view the <em>decision boundary</em>, which is the divide between where the
algorithm assigns class 0 versus where it assigns class 1. The following
code produces the visualizations of the decision boundaries for one,
three, and nine neighbors shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_neighbor">Figure&nbsp;2-6</a>:</p>

<p><code><strong>In[16]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>

<code class="k">for</code> <code class="n">n_neighbors</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">9</code><code class="p">],</code> <code class="n">axes</code><code class="p">):</code>
    <code class="c1"># the fit method returns the object self, so we can instantiate</code>
    <code class="c1"># and fit in one line</code>
    <code class="n">clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">n_neighbors</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">4</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"{} neighbor(s)"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">n_neighbors</code><code class="p">))</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"feature 0"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"feature 1"</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<figure><div id="single_neighbor" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0206.png" alt="malp 0206" width="1565" height="562" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0206.png">
<h6><span class="label">Figure 2-6. </span>Decision boundaries created by the nearest neighbors model for different values of n_neighbors</h6>
</div></figure>

<p>As you can see on the left in the figure, using a single neighbor results in a
decision boundary that follows the training data closely. Considering
more and more neighbors leads to a smoother decision boundary. A
smoother boundary corresponds to a simpler model. In other words, using
few neighbors corresponds to high model complexity (as shown on the
right side of <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>), and using many neighbors
corresponds to low model complexity (as shown on the left side of <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>). If you consider the extreme case where the number of
neighbors is the number of all data points in the training set, each
test point would have exactly the same neighbors (all training points)
and all predictions would be the same: the class that is most frequent
in the training set.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_breast_cancer" id="idm45613690312168"></a>
Let’s investigate whether we can confirm the connection between model
complexity and generalization that we discussed earlier. We will do this
on the real-world Breast Cancer dataset. We begin by splitting the
dataset into a training and a test set. Then we evaluate training and
test set performance with different numbers of neighbors. The results
are shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#n_neighbors">Figure&nbsp;2-7</a>:</p>

<p><code><strong>In[17]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_breast_cancer</code>

<code class="n">cancer</code> <code class="o">=</code> <code class="n">load_breast_cancer</code><code class="p">()</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">66</code><code class="p">)</code>

<code class="n">training_accuracy</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">test_accuracy</code> <code class="o">=</code> <code class="p">[]</code>
<code class="c1"># try n_neighbors from 1 to 10</code>
<code class="n">neighbors_settings</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">11</code><code class="p">)</code>

<code class="k">for</code> <code class="n">n_neighbors</code> <code class="ow">in</code> <code class="n">neighbors_settings</code><code class="p">:</code>
    <code class="c1"># build the model</code>
    <code class="n">clf</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">n_neighbors</code><code class="p">)</code>
    <code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="c1"># record training set accuracy</code>
    <code class="n">training_accuracy</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">))</code>
    <code class="c1"># record generalization accuracy</code>
    <code class="n">test_accuracy</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">))</code>

<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">neighbors_settings</code><code class="p">,</code> <code class="n">training_accuracy</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"training accuracy"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">neighbors_settings</code><code class="p">,</code> <code class="n">test_accuracy</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"test accuracy"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Accuracy"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"n_neighbors"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code></pre>

<p>The plot shows the training and test set accuracy on the y-axis against
the setting of <code>n_neighbors</code> on the x-axis. While real-world plots
are rarely very smooth, we can still recognize some of the
characteristics of overfitting and underfitting (note that because considering
fewer neighbors corresponds to a more complex model, the plot is
horizontally flipped relative to the illustration in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>). Considering a single nearest neighbor, the prediction
on the training set is perfect. But when more neighbors are considered, the model
becomes simpler and the training accuracy drops. The test set
accuracy for using a single neighbor is lower than when using more
neighbors, indicating that using the single nearest neighbor leads to a
model that is too complex. On the other hand, when considering 10
neighbors, the model is too simple and performance is even worse. The
best performance is somewhere in the middle, using around six neighbors.
Still, it is good to keep the scale of the plot in mind. The worst
performance is around 88% accuracy, which might still be acceptable.</p>

<figure><div id="n_neighbors" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0207.png" alt="malp 0207" width="1565" height="1073" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0207.png">
<h6><span class="label">Figure 2-7. </span>Comparison of training and test accuracy as a function of n_neighbors</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="k-neighbors regression"><div class="sect3" id="k-neighbors-regression">
<h3>k-neighbors regression</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="regression" id="idm45613690086184"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="k-nearest neighbors" id="idm45613690085352"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="k-nearest neighbors" id="idm45613690084408"></a>
There is also a regression variant of the <em>k</em>-nearest neighbors algorithm.
Again, let’s start by using the single nearest neighbor, this time using
the <code>wave</code> dataset. We’ve added three test data points as green stars on
the x-axis. The prediction using a single neighbor is just the target
value of the nearest neighbor. These are shown as blue stars in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#knn_regression">Figure&nbsp;2-8</a>:</p>

<p><code><strong>In[18]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_knn_regression</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<figure><div id="knn_regression" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0208.png" alt="malp 0208" width="1565" height="1032" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0208.png">
<h6><span class="label">Figure 2-8. </span>Predictions made by one-nearest-neighbor regression on the wave dataset</h6>
</div></figure>

<p>Again, we can use more than the single closest neighbor for regression.
When using multiple nearest neighbors, the prediction is the average, or
mean, of the relevant neighbors
(<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#multiple_nearest_neighbors">Figure&nbsp;2-9</a>):</p>

<p><code><strong>In[19]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_knn_regression</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<figure><div id="multiple_nearest_neighbors" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0209.png" alt="malp 0209" width="1565" height="1032" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0209.png">
<h6><span class="label">Figure 2-9. </span>Predictions made by three-nearest-neighbors regression on the wave dataset</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="KNeighborsRegressor" id="SLCFknreg2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="KNeighborsRegressor" id="Cknreg2"></a>
The <em>k</em>-nearest neighbors algorithm for regression is implemented in the
<code>KNeighborsRegressor</code> class in <code>scikit-learn</code>. It’s used similarly to
<code>KNeighborsClassifier</code>:</p>

<p><code><strong>In[20]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsRegressor</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_wave</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">40</code><code class="p">)</code>

<code class="c1"># split the wave dataset into a training and a test set</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># instantiate the model and set the number of neighbors to consider to 3</code>
<code class="n">reg</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="c1"># fit the model using the training data and training targets</code>
<code class="n">reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>Now we can make predictions on the test set:</p>

<p><code><strong>In[21]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Test set predictions:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code></pre>

<p class="pagebreak-before"><code><strong>Out[21]:</strong></code></p>

<pre data-type="programlisting">Test set predictions:
[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]</pre>

<p><a data-type="indexterm" data-primary="score method" id="idm45613689900568"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="score method" id="idm45613689899640"></a><a data-type="indexterm" data-primary="" data-startref="Ckn2" id="idm45613689898696"></a><a data-type="indexterm" data-primary="" data-startref="SLCFkn2" id="idm45613689897752"></a>
We can also evaluate the model using the <code>score</code> method, which for
regressors returns the <em>R</em><sup>2</sup> score. The <em>R</em><sup>2</sup>
score, also known as the coefficient of determination, is a measure of
goodness of a prediction for a regression model, and yields a score
that’s usually between 0 and 1. A value of 1 corresponds to a perfect
prediction, and a value of 0 corresponds to a constant model that just
predicts the mean of the training set responses, <code>y_train</code>. The
formulation of <em>R</em><sup>2</sup> used here can even be negative, which
can indicate anticorrelated predictions.</p>

<p><code><strong>In[22]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Test set R^2: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[22]:</strong></code></p>

<pre data-type="programlisting">Test set R^2: 0.83</pre>

<p>Here, the score is 0.83, which indicates a relatively good model fit.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Analyzing KNeighborsRegressor"><div class="sect3" id="analyzing-k-nearest-neighbors-regression">
<h3>Analyzing KNeighborsRegressor</h3>

<p><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="analyzing KNeighborsRegressor" id="idm45613689881832"></a>
For our one-dimensional dataset, we can see what the predictions look
like for all possible feature values
(<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_nearest_neighbor_regression">Figure&nbsp;2-10</a>).
To do this, we create a test dataset consisting of many points on the
x-axis, which corresponds to the single feature:</p>

<p><code><strong>In[23]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="c1"># create 1,000 data points, evenly spaced between -3 and 3</code>
<code class="n">line</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="k">for</code> <code class="n">n_neighbors</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">9</code><code class="p">],</code> <code class="n">axes</code><code class="p">):</code>
    <code class="c1"># make predictions using 1, 3, or 9 neighbors</code>
    <code class="n">reg</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="n">n_neighbors</code><code class="p">)</code>
    <code class="n">reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="n">reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">line</code><code class="p">))</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="s1">'^'</code><code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code> <code class="n">markersize</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code> <code class="n">markersize</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code>

    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code>
        <code class="s2">"{} neighbor(s)</code><code class="se">\n</code><code class="s2"> train score: {:.2f} test score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
            <code class="n">n_neighbors</code><code class="p">,</code> <code class="n">reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">),</code>
            <code class="n">reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Feature"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Target"</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Model predictions"</code><code class="p">,</code> <code class="s2">"Training data/target"</code><code class="p">,</code>
                <code class="s2">"Test data/target"</code><code class="p">],</code> <code class="n">loc</code><code class="o">=</code><code class="s2">"best"</code><code class="p">)</code></pre>

<figure><div id="single_nearest_neighbor_regression" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0210.png" alt="malp 0210" width="1565" height="512" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0210.png">
<h6><span class="label">Figure 2-10. </span>Comparing predictions made by nearest neighbors regression for different values of <code>n_neighbors</code></h6>
</div></figure>

<p>As we can see from the plot, using only a single neighbor, each point in
the training set has an obvious influence on the predictions, and the
predicted values go through all of the data points. This leads to a very
unsteady prediction. Considering more neighbors leads to smoother
predictions, but these do not fit the training data as well.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters">
<h3>Strengths, weaknesses, and parameters</h3>

<p>In principle, there are two important parameters to the <code>KNeighbors</code>
classifier: the number of neighbors and how you measure distance between
data points. In practice, using a small number of neighbors like three
or five often works well, but you should certainly adjust this
parameter. Choosing the right distance measure is somewhat beyond the
scope of this book. By default, Euclidean distance is used, which works
well in many settings.</p>

<p>One of the strengths of <em>k</em>-NN is that the model is very easy to
understand, and often gives reasonable performance without a lot of
adjustments. Using this algorithm is a good baseline method to try
before considering more advanced techniques. Building the nearest
neighbors model is usually very fast, but when your training set is very
large (either in number of features or in number of samples) prediction
can be slow. When using the <em>k</em>-NN algorithm, it’s important to preprocess
your data (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html#unsupervised-learning-and-preprocessing">Chapter&nbsp;3</a>). This approach often
does not perform well on datasets with many features (hundreds or more), and it does particularly badly with datasets where most features are 0
most of the time (so-called <em>sparse datasets</em>).</p>

<p>So, while the <em>k</em>-nearest neighbors algorithm is easy to understand, it is
not often used in practice, due to prediction being slow and its
inability to handle many features. The method we discuss next has
neither of these drawbacks.
<a data-type="indexterm" data-primary="" data-startref="SLalkn2" id="idm45613689574008"></a><a data-type="indexterm" data-primary="" data-startref="Aknn2" id="idm45613689573032"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.3 Linear Models"><div class="sect2" id="linear-models">
<h2>2.3.3 Linear Models</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="linear models" id="SLalglin2"></a><a data-type="indexterm" data-primary="linear models" data-secondary="predictions with" id="idm45613689568760"></a>
Linear models are a class of models that are widely used in practice
and have been studied extensively in the last few decades, with roots
going back over a hundred years. Linear models make a
prediction using a <em>linear function</em> of the input features, which
we will explain shortly.</p>










<section data-type="sect3" data-pdf-bookmark="Linear models for regression"><div class="sect3" id="linear-models-for-regression">
<h3>Linear models for regression</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="regression" id="idm45613689565144"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="linear models" id="idm45613689564168"></a>
For regression, the general prediction formula for a linear model looks
as follows:</p>
<ul class="simplelist">
  <li><em>ŷ</em> = <em>w</em>[0] * <em>x</em>[0] + <em>w</em>[1] * <em>x</em>[1] + ... + <em>w</em>[<em>p</em>] * <em>x</em>[<em>p</em>] + <em>b</em> </li>
</ul>

<p>Here, <em>x</em>[0] to <em>x</em>[<em>p</em>] denotes the features (in
this example, the number of features is <span class="keep-together"><em>p</em>+1</span>) of a single data
point, <em>w</em> and <em>b</em> are parameters of the model
that are learned, and <em>ŷ</em> is the prediction the model
makes. For a dataset with a single feature, this is:</p>
<ul class="simplelist">
  <li><em>ŷ</em> = <em>w</em>[0] * <em>x</em>[0] + <em>b</em></li>
</ul>

<p>which you might remember from high school mathematics as the equation
for a line. Here, <em>w</em>[0] is the slope and <em>b</em>
is the y-axis offset. For more features, <em>w</em> contains the slopes along
each feature axis. Alternatively, you can think of the predicted
response as being a weighted sum of the input features, with weights
(which can be negative) given by the entries of <em>w</em>.</p>

<p>Trying to learn the parameters <em>w</em>[0] and <em>b</em> on
our one-dimensional <code>wave</code> dataset might lead to the following line (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#one-d_wave">Figure&nbsp;2-11</a>):</p>

<p><code><strong>In[24]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_linear_regression_wave</code><code class="p">()</code></pre>

<p><code><strong>Out[24]:</strong></code></p>

<pre data-type="programlisting">w[0]: 0.393906  b: -0.031804</pre>

<figure><div id="one-d_wave" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0211.png" alt="malp 0211" width="1565" height="1564" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0211.png">
<h6><span class="label">Figure 2-11. </span>Predictions of a linear model on the wave dataset</h6>
</div></figure>

<p>We added a coordinate cross into the plot to make it easier to
understand the line. Looking at <code>w[0]</code> we see that the slope should be
around 0.4, which we can confirm visually in the plot. The intercept is
where the prediction line should cross the y-axis: this is slightly
below zero, which you can also confirm in the image.</p>

<p>Linear models for regression can be characterized as regression models
for which the prediction is a line for a single feature, a plane when
using two features, or a hyperplane in higher dimensions (that is, when
using more features).</p>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="vs. k-nearest neighbors" data-secondary-sortas="k-nearest neighbors" id="idm45613689534968"></a><a data-type="indexterm" data-primary="k-nearest neighbors (k-NN)" data-secondary="vs. linear models" data-secondary-sortas="linear models" id="idm45613689533528"></a>
If you compare the predictions made by the straight line with those made
by the <code>KNeighborsRegressor</code> in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_nearest_neighbor_regression">Figure&nbsp;2-10</a>, using
a straight line to make predictions seems very restrictive. It looks
like all the fine details of the data are lost. In a sense, this is true.
It is a strong (and somewhat unrealistic) assumption that our target
<em>y</em> is a linear combination of the features. But looking at
one-dimensional data gives a somewhat skewed perspective. For datasets
with many features, linear models can be very powerful. In particular,
if you have more features than training data points, any target
<em>y</em> can be perfectly modeled (on the training set) as a
linear function.<sup><a data-type="noteref" id="idm45613689524920-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613689524920" class="totri-footnote">6</a></sup>
<a data-type="indexterm" data-primary="ordinary least squares (OLS)" id="idm45613689524168"></a></p>

<p>There are many different linear models for regression. The difference
between these models lies in how the model parameters <em>w</em> and
<em>b</em> are learned from the training data, and how model
complexity can be controlled. We will now take a look at the most popular
linear models for regression.<a data-type="indexterm" data-primary="" data-startref="Cknreg2" id="idm45613689522008"></a><a data-type="indexterm" data-primary="" data-startref="SLCFknreg2" id="idm45613689521032"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Linear regression (aka ordinary least squares)"><div class="sect3" id="linear-regression-aka-ordinary-least-squares">
<h3>Linear regression (aka ordinary least squares)</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="ordinary least squares" id="idm45613689517912"></a><a data-type="indexterm" data-primary="linear regression" id="idm45613689516712"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="linear regression (OLS)" id="idm45613689499480"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearRegression" id="SLCFlinreg2"></a><a data-type="indexterm" data-primary="regression" data-secondary="LinearRegression" id="Rlinreg2"></a>
Linear regression, or <em>ordinary least squares</em> (OLS), is the simplest and
most classic linear method for regression. Linear regression finds the
parameters <em>w</em> and <em>b</em> that minimize the <em>mean
squared error</em> between predictions and the true regression targets,
<em>y</em>, on the training set. The mean squared error is the sum
of the squared differences between the predictions and the true values, divided by the number of samples.
Linear regression has no parameters, which is a benefit, but it also has
no way to control model complexity.</p>

<p>Here is the code that produces the model you can see in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#one-d_wave">Figure&nbsp;2-11</a>:</p>

<p><code><strong>In[25]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_wave</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">60</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">lr</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="coef_ attribute" id="idm45613689488648"></a><a data-type="indexterm" data-primary="intercept_ attribute" id="idm45613689385112"></a><a data-type="indexterm" data-primary="weights" id="idm45613689384504"></a>
The “slope” parameters (<em>w</em>), also called weights or <em>coefficients</em>, are
stored in the <code>coef_</code> attribute, while the offset or <em>intercept</em> (<em>b</em>) is
stored in the <code>intercept_</code> attribute:</p>

<p><code><strong>In[26]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"lr.coef_:"</code><code class="p">,</code> <code class="n">lr</code><code class="o">.</code><code class="n">coef_</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"lr.intercept_:"</code><code class="p">,</code> <code class="n">lr</code><code class="o">.</code><code class="n">intercept_</code><code class="p">)</code></pre>

<p><code><strong>Out[26]:</strong></code></p>

<pre data-type="programlisting">lr.coef_: [0.394]
lr.intercept_: -0.031804343026759746</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You might notice the strange-looking trailing underscore at the
end of <code>coef_</code> and <code>intercept_</code>. <code>scikit-learn</code> always stores anything
that is derived from the training data in attributes that end with a
trailing underscore. That is to separate them from parameters that are
set by the user.</p>
</div>

<p>The <code>intercept_</code> attribute is always a single float number, while the
<code>coef_</code> attribute is a NumPy array with one entry per input feature. As
we only have a single input feature in the <code>wave</code> dataset, <code>lr.coef_</code>
only has a single entry.</p>

<p>Let’s look at the training set and test set performance:</p>

<p><code><strong>In[27]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[27]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.67
Test set score: 0.66</pre>

<p>An <em>R</em><sup>2</sup> of around 0.66 is not very good, but we can see
that the scores on the training and test sets are very close together. This
means we are likely underfitting, not overfitting. For this
one-dimensional dataset, there is little danger of overfitting, as the
model is very simple (or restricted). However, with higher-dimensional
datasets (meaning datasets with a large number of features), linear
models become more powerful, and there is a higher chance of
overfitting. Let’s take a look at how <code>LinearRegression</code> performs on a
more complex dataset, like the Boston Housing dataset. Remember that
this dataset has 506 samples and 104 derived features. First, we load
the dataset and split it into a training and a test set. Then we build
the linear regression model as before:</p>

<p><code><strong>In[28]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">load_extended_boston</code><code class="p">()</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">lr</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>When comparing training set and test set scores, we find that we predict
very accurately on the training set, but the <em>R</em><sup>2</sup> on the
test set is much worse:</p>

<p><code><strong>In[29]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p class="pagebreak-before"><code><strong>Out[29]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.95
Test set score: 0.61</pre>

<p>This discrepancy between performance on the training set and the test
set is a clear sign of overfitting, and therefore we should try to find
a model that allows us to control complexity. One of the most commonly
used alternatives to standard linear regression is <em>ridge regression</em>,
which we will look into next.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Ridge regression"><div class="sect3" id="ridge-regression">
<h3>Ridge regression</h3>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="Ridge" id="SCLCFridge2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="Ridge" id="Asregridge2"></a><a data-type="indexterm" data-primary="linear models" data-secondary="ridge regression" id="idm45613689206904"></a><a data-type="indexterm" data-primary="ridge regression" id="idm45613689205960"></a><a data-type="indexterm" data-primary="regularization" data-secondary="L2 regularization" id="idm45613689205288"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="ridge regression" id="idm45613689156888"></a><a data-type="indexterm" data-primary="L2 regularization" id="idm45613689155944"></a>
Ridge regression is also a linear model for regression, so the formula
it uses to make predictions is the same one used for ordinary least
squares. In ridge regression, though, the coefficients (<em>w</em>) are chosen
not only so that they predict well on the training data, but also to fit
an additional constraint. We also want the magnitude of coefficients to
be as small as possible; in other words, all entries of <em>w</em> should be
close to zero. Intuitively, this means each feature should have as
little effect on the outcome as possible (which translates to having a
small slope), while still predicting well. This constraint is an example
of what is called <em>regularization</em>. Regularization means explicitly
restricting a model to avoid overfitting. The particular kind used by
ridge regression is known as L2 regularization.<sup><a data-type="noteref" id="idm45613689153080-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613689153080" class="totri-footnote">7</a></sup></p>

<p>Ridge regression is implemented in <code>linear_model.Ridge</code>. Let’s see how
well it does on the extended Boston Housing dataset:</p>

<p><code><strong>In[30]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Ridge</code>

<code class="n">ridge</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[30]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.89
Test set score: 0.75</pre>

<p>As you can see, the training set score of <code>Ridge</code> is <em>lower</em> than for
<code>LinearRegression</code>, while the test set score is <em>higher</em>. This is
consistent with our expectation. With linear regression, we were
overfitting our data. <code>Ridge</code> is a more restricted model, so we are less
likely to overfit. A less complex model means worse performance on the
training set, but better generalization. As we are only interested in
generalization performance, we should choose the <code>Ridge</code> model over the
<code>LinearRegression</code> model.</p>

<p><a data-type="indexterm" data-primary="alpha parameter in linear models" id="idm45613689077192"></a>
The <code>Ridge</code> model makes a trade-off between the simplicity of the model
(near-zero coefficients) and its performance on the training set. How
much importance the model places on simplicity versus training set
performance can be specified by the user, using the <code>alpha</code> parameter.
In the previous example, we used the default parameter <code>alpha=1.0</code>.
There is no reason why this will give us the best trade-off, though. The
optimum setting of <code>alpha</code> depends on the particular dataset we are
using. Increasing <code>alpha</code> forces coefficients to move more toward zero,
which decreases training set performance but might help generalization.
For example:</p>

<p><code><strong>In[31]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ridge10</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge10</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge10</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[31]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.79
Test set score: 0.64</pre>

<p>Decreasing <code>alpha</code> allows the coefficients to be less restricted,
meaning we move right in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>. For very small values of <code>alpha</code>, coefficients are barely
restricted at all, and we end up with a model that resembles
<code>LinearRegression</code>:</p>

<p><code><strong>In[32]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ridge01</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge01</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">ridge01</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[32]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.93
Test set score: 0.77</pre>

<p>Here, <code>alpha=0.1</code> seems to be working well. We could try decreasing
<code>alpha</code> even more to improve generalization. For now, notice how the
parameter <code>alpha</code> corresponds to the model complexity as shown in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>. We will discuss methods
to properly select parameters in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch05.html#model-evaluation-and-improvement">Chapter&nbsp;5</a>.</p>

<p><a data-type="indexterm" data-primary="coef_ attribute" id="idm45613688921608"></a>
We can also get a more qualitative insight into how the <code>alpha</code>
parameter changes the model by inspecting the <code>coef_</code> attribute of
models with different values of <code>alpha</code>. A higher <code>alpha</code> means a more
restricted model, so we expect the entries of <code>coef_</code> to have smaller
magnitude for a high value of <code>alpha</code> than for a low value of <code>alpha</code>.
This is confirmed in the plot in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#regularization">Figure&nbsp;2-12</a>:</p>

<p class="pagebreak-before"><code><strong>In[33]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ridge</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Ridge alpha=1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ridge10</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'^'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Ridge alpha=10"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ridge01</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Ridge alpha=0.1"</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'o'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"LinearRegression"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Coefficient index"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Coefficient magnitude"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">hlines</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">lr</code><code class="o">.</code><code class="n">coef_</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">25</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code></pre>

<figure><div id="regularization" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0212.png" alt="malp 0212" width="1565" height="1102" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0212.png">
<h6><span class="label">Figure 2-12. </span>Comparing coefficient magnitudes for ridge regression with different values of alpha and linear regression</h6>
</div></figure>

<p>Here, the x-axis enumerates the entries of <code>coef_</code>: <code>x=0</code> shows the
coefficient associated with the first feature, <code>x=1</code> the coefficient
associated with the second feature, and so on up to <code>x=100</code>. The y-axis
shows the numeric values of the corresponding values of the coefficients.
The main takeaway here is that for <code>alpha=10</code>, the coefficients are
mostly between around –3 and 3. The coefficients for the <code>Ridge</code> model
with <code>alpha=1</code>, are somewhat larger. The dots corresponding to
<code>alpha=0.1</code> have larger magnitude still, and many of the dots
corresponding to linear regression without any regularization (which
would be <code>alpha=0</code>) are so large they are outside of the chart.</p>

<p>Another way to understand the influence of regularization is to fix a
value of <code>alpha</code> but vary the amount of training data available. For
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#ridge_n_samples">Figure&nbsp;2-13</a>, we subsampled the Boston
Housing dataset and evaluated <code>LinearRegression</code> and <code>Ridge(alpha=1)</code> on
subsets of increasing size (plots that show model performance as a
function of dataset size are called <em>learning curves</em>):</p>

<p><code><strong>In[34]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_ridge_n_samples</code><code class="p">()</code></pre>

<figure><div id="ridge_n_samples" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0213.png" alt="malp 0213" width="1565" height="1247" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0213.png">
<h6><span class="label">Figure 2-13. </span>Learning curves for ridge regression and linear regression on the Boston Housing dataset</h6>
</div></figure>

<p>As one would expect, the training score is higher than the test score
for all dataset sizes, for both ridge and linear regression. Because
ridge is regularized, the training score of ridge is lower than the
training score for linear regression across the board. However, the test
score for ridge is better, particularly for small subsets of the data.
For less than 400 data points, linear regression is not able to learn
anything. As more and more data becomes available to the model, both
models improve, and linear regression catches up with ridge in the end.
The lesson here is that with enough training data, regularization
becomes less important, and given enough data, ridge and linear
regression will have the same performance (the fact that this happens
here when using the full dataset is just by chance). Another interesting
aspect of <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#ridge_n_samples">Figure&nbsp;2-13</a> is the decrease
in training performance for linear regression. If more data is added, it
becomes harder for a model to overfit, or memorize the data.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Lasso"><div class="sect3" id="lasso">
<h3>Lasso</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="Lasso" id="idm45613688754968"></a><a data-type="indexterm" data-primary="Lasso model" id="idm45613688753768"></a><a data-type="indexterm" data-primary="regression problems" data-secondary="Lasso" id="idm45613688753096"></a><a data-type="indexterm" data-primary="L1 regularization" id="idm45613688752152"></a><a data-type="indexterm" data-primary="regularization" data-secondary="L1 regularization" id="idm45613688751480"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="Lasso" id="SLCFlasso2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="Lasso" id="Asreglasso2"></a>
An alternative to <code>Ridge</code> for regularizing linear regression is <code>Lasso</code>.
As with ridge regression, the lasso also restricts coefficients to be
close to zero, but in a slightly different way, called L1
regularization.<sup><a data-type="noteref" id="idm45613688738392-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613688738392" class="totri-footnote">8</a></sup> The consequence of L1 regularization is that when
using the lasso, some coefficients are <em>exactly zero</em>. This means some
features are entirely ignored by the model. This can be seen as a form
of automatic feature selection. Having some coefficients be exactly zero
often makes a model easier to interpret, and can reveal the most
important features of your model.</p>

<p>Let’s apply the lasso to the extended Boston Housing dataset:</p>

<p><code><strong>In[35]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Lasso</code>

<code class="n">lasso</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of features used:"</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">lasso</code><code class="o">.</code><code class="n">coef_</code> <code class="o">!=</code> <code class="mi">0</code><code class="p">))</code></pre>

<p><code><strong>Out[35]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.29
Test set score: 0.21
Number of features used: 4</pre>

<p>As you can see, <code>Lasso</code> does quite badly, both on the training and the
test set. This indicates that we are underfitting, and we find that it
used only 4 of the 104 features. Similarly to <code>Ridge</code>, the Lasso also has
a regularization parameter, <code>alpha</code>, that controls how strongly
coefficients are pushed toward zero. In the previous example, we used
the default of <code>alpha=1.0</code>. To reduce underfitting, let’s try decreasing
<code>alpha</code>. When we do this, we also need to increase the default setting
of <code>max_iter</code> (the maximum number of iterations to run):</p>

<p class="pagebreak-before"><code><strong>In[36]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># we increase the default setting of "max_iter",</code>
<code class="c1"># otherwise the model would warn us that we should increase max_iter.</code>
<code class="n">lasso001</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">100000</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of features used:"</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">lasso001</code><code class="o">.</code><code class="n">coef_</code> <code class="o">!=</code> <code class="mi">0</code><code class="p">))</code></pre>

<p><code><strong>Out[36]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.90
Test set score: 0.77
Number of features used: 33</pre>

<p>A lower <code>alpha</code> allowed us to fit a more complex model, which worked
better on the training and test data. The performance is slightly better
than using <code>Ridge</code>, and we are using only 33 of the 104 features. This
makes this model potentially easier to understand.</p>

<p>If we set <code>alpha</code> too low, however, we again remove the effect of
regularization and end up overfitting, with a result similar to
<code>LinearRegression</code>:</p>

<p><code><strong>In[37]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">lasso00001</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.0001</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">100000</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso00001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">lasso00001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of features used:"</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">lasso00001</code><code class="o">.</code><code class="n">coef_</code> <code class="o">!=</code> <code class="mi">0</code><code class="p">))</code></pre>

<p><code><strong>Out[37]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.95
Test set score: 0.64
Number of features used: 94</pre>

<p>Again, we can plot the coefficients of the different models, similarly
to <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#regularization">Figure&nbsp;2-12</a>.  The result is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#lasso_alpha_sample">Figure&nbsp;2-14</a>:</p>

<p><code><strong>In[38]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">lasso</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'s'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Lasso alpha=1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">lasso001</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'^'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Lasso alpha=0.01"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">lasso00001</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Lasso alpha=0.0001"</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ridge01</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="s1">'o'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Ridge alpha=0.1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">ncol</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mf">1.05</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">25</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Coefficient index"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Coefficient magnitude"</code><code class="p">)</code></pre>

<figure><div id="lasso_alpha_sample" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0213a.png" alt="malp 0213a" width="1565" height="1242" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0213a.png">
<h6><span class="label">Figure 2-14. </span>Comparing coefficient magnitudes for lasso regression with different values of alpha and ridge regression</h6>
</div></figure>

<p>For <code>alpha=1</code>, we not only see that most of the coefficients are zero (which we already
knew), but that the remaining coefficients are also small in magnitude.
Decreasing <code>alpha</code> to <code>0.01</code>, we obtain the solution shown as an upward pointing triangle,
which causes most features to be exactly zero. Using <code>alpha=0.0001</code>, we get
a model that is quite unregularized, with most coefficients nonzero and
of large magnitude. For comparison, the best <code>Ridge</code> solution is shown as circles. The <code>Ridge</code> model with <code>alpha=0.1</code> has similar predictive performance
as the lasso model with <code>alpha=0.01</code>, but using <code>Ridge</code>, all coefficients
are nonzero.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="ElasticNet class" id="idm45613688302120"></a>
In practice, ridge regression is usually the first choice between these
two models. However, if you have a large amount of features and expect
only a few of them to be important, <code>Lasso</code> might be a better choice.
Similarly, if you would like to have a model that is easy to interpret,
<code>Lasso</code> will provide a model that is easier to understand, as it will
select only a subset of the input features. <code>scikit-learn</code> also provides
the <code>ElasticNet</code> class, which combines the penalties of <code>Lasso</code> and
<code>Ridge</code>. In practice, this combination works best, though at the price
of having two parameters to adjust: one for the L1 regularization, and
one for the L2 regularization.
<a data-type="indexterm" data-primary="" data-startref="Asregridge2" id="idm45613688297624"></a><a data-type="indexterm" data-primary="" data-startref="SLCFridge2" id="idm45613688296648"></a><a data-type="indexterm" data-primary="" data-startref="Asreglasso2" id="idm45613688295704"></a><a data-type="indexterm" data-primary="" data-startref="SLCFlasso2" id="idm45613688294760"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Linear models for classification"><div class="sect3" id="linear-models-for-classification">
<h3>Linear models for classification</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="classification" id="idm45613688293112"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="linear models" id="idm45613688290792"></a><a data-type="indexterm" data-primary="binary classification" id="idm45613688289848"></a>
Linear models are also extensively used for classification. Let’s look
at binary classification first. In this case, a prediction is made using
the following formula:</p>
<ul class="simplelist">
  <li><em>ŷ</em> = <em>w</em>[0] * <em>x</em>[0] + <em>w</em>[1] * <em>x</em>[1] + ... + <em>w</em>[<em>p</em>] * <em>x</em>[<em>p</em>] + <em>b</em>  &gt; 0</li>
</ul>

<p>The formula looks very similar to the one for linear regression, but
instead of just returning the weighted sum of the features, we threshold
the predicted value at zero. If the function is smaller than zero, we
predict the class –1; if it is larger than zero, we predict the class
+1. This prediction rule is common to all linear models for
classification. Again, there are many different ways to find the
coefficients (<em>w</em>) and the intercept (<em>b</em>).</p>

<p><a data-type="indexterm" data-primary="decision boundaries" id="idm45613688281272"></a><a data-type="indexterm" data-primary="linear functions" id="idm45613688280568"></a>
For linear models for regression, the output, <em>ŷ</em>, is a linear function
of the features: a line, plane, or hyperplane (in higher dimensions).
For linear models for classification, the <em>decision boundary</em> is a
linear function of the input. In other words, a (binary) linear
classifier is a classifier that separates two classes using a line, a
plane, or a hyperplane. We will see examples of that in this section.</p>

<p>There are many algorithms for learning linear models. These algorithms
all differ in the following two ways:</p>

<ul>
<li>
<p>The way in which they measure how well a particular combination of
coefficients and intercept fits the training data</p>
</li>
<li>
<p>If and what kind of regularization they use</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="loss functions" id="idm45613688275336"></a>
Different algorithms choose different ways to measure what “fitting the
training set well” means. For technical mathematical reasons, it is
not possible to adjust <em>w</em> and <em>b</em> to minimize the number of
misclassifications the algorithms produce, as one might hope. For our
purposes, and many applications, the different choices for item 1 in the
preceding list (called <em>loss functions</em>) are of little significance.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearSVC" id="SLCFlinearsvc2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LinearSVC" id="Clinearsvc2"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="SVC" id="idm45613688269688"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="SVC" id="idm45613688268728"></a><a data-type="indexterm" data-primary="linear models" data-secondary="logistic regression" id="idm45613688267784"></a><a data-type="indexterm" data-primary="linear models" data-secondary="linear SVMs" id="idm45613688266840"></a><a data-type="indexterm" data-primary="linear support vector machines (SVMs)" id="idm45613688265896"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="logistic regression" id="idm45613688265208"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LogisticRegression" id="SLCFlogreg2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LogisticRegression" id="Clogreg2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="linear SVMs" id="idm45613688261528"></a>
The two most common linear classification algorithms are <em>logistic
regression</em>, implemented in <code>linear_model.LogisticRegression</code>, and
<em>linear support vector machines</em> (linear SVMs), implemented in
<code>svm.LinearSVC</code> (SVC stands for support vector classifier). Despite its
name, <code>LogisticRegression</code> is a classification algorithm and not a
regression algorithm, and it should not be confused with
<code>LinearRegression</code>.
<a data-type="indexterm" data-primary="" data-startref="Rlinreg2" id="idm45613688257496"></a><a data-type="indexterm" data-primary="" data-startref="SLCFlinreg2" id="idm45613688256488"></a></p>

<p>We can apply the <code>LogisticRegression</code> and <code>LinearSVC</code> models to the
<code>forge</code> dataset, and visualize the decision boundary as found by the
linear models (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#logisticregression_and_linearsvc">Figure&nbsp;2-15</a>):</p>

<p class="pagebreak-before"><code><strong>In[39]:</strong></code></p>
<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">make_forge</code><code class="p">()</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>

<code class="k">for</code> <code class="n">model</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="n">LinearSVC</code><code class="p">(),</code> <code class="n">LogisticRegression</code><code class="p">()],</code> <code class="n">axes</code><code class="p">):</code>
    <code class="n">clf</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">eps</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code>
                                    <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">7</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="n">clf</code><code class="o">.</code><code class="nv-Magic">__class__</code><code class="o">.</code><code class="nv-Magic">__name__</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code>
</pre>

<figure><div id="logisticregression_and_linearsvc" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0214.png" alt="malp 0214" width="1565" height="570" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0214.png">
<h6><span class="label">Figure 2-15. </span>Decision boundaries of a linear SVM and logistic regression on the forge dataset with the default parameters</h6>
</div></figure>

<p>In this figure, we have the first feature of the <code>forge</code> dataset on the
x-axis and the second feature on the y-axis, as before. We display the
decision boundaries found by <code>LinearSVC</code> and <code>LogisticRegression</code>
respectively as straight lines, separating the area classified as class
1 on the top from the area classified as class 0 on the bottom. In other
words, any new data point that lies above the black line will be
classified as class 1 by the respective classifier, while any point that
lies below the black line will be classified as class 0.</p>

<p>The two models come up with similar decision boundaries. Note that both
misclassify two of the points. By default, both models apply an L2
regularization, in the same way that <code>Ridge</code> does for regression.</p>

<p>For <code>LogisticRegression</code> and <code>LinearSVC</code> the trade-off parameter that
determines the strength of the regularization is called <code>C</code>, and higher
values of <code>C</code> correspond to <em>less</em> regularization. In other words, when
you use a high value for the parameter <code>C</code>, <code>LogisticRegression</code> and
<code>LinearSVC</code> try to fit the training set as best as possible, while with
low values of the parameter <code>C</code>, the models put more emphasis on finding
a coefficient vector (<em>w</em>) that is close to zero.</p>

<p>There is another interesting aspect of how the parameter <code>C</code> acts. Using
low values of <code>C</code> will cause the algorithms to try to adjust to the
“majority” of data points, while using a higher value of <code>C</code> stresses
the importance that each individual data point be classified correctly.
Here is an illustration using <code>LinearSVC</code> (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linearsvc">Figure&nbsp;2-16</a>):</p>

<p><code><strong>In[40]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_linear_svc_regularization</code><code class="p">()</code></pre>

<figure><div id="linearsvc" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0215.png" alt="malp 0215" width="2831" height="989" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0215.png">
<h6><span class="label">Figure 2-16. </span>Decision boundaries of a linear SVM on the forge dataset for different <span class="keep-together">values</span> of C</h6>
</div></figure>

<p>On the lefthand side, we have a very small <code>C</code> corresponding to a lot of
regularization. Most of the points in class 0 are at the bottom, and
most of the points in class 1 are at the top. The strongly regularized
model chooses a relatively horizontal line, misclassifying two points.
In the center plot, <code>C</code> is slightly higher, and the model focuses more
on the two misclassified samples, tilting the decision boundary.
Finally, on the righthand side, the very high value of <code>C</code> in the model
tilts the decision boundary a lot, now correctly classifying all points
in class 0. One of the points in class 1 is still misclassified, as it
is not possible to correctly classify all points in this dataset using a
straight line. The model illustrated on the righthand side tries hard to
correctly classify all points, but might not capture the overall layout
of the classes well. In other words, this model is likely overfitting.</p>

<p>Similarly to the case of regression, linear models for classification
might seem very restrictive in low-dimensional spaces, only allowing for
decision boundaries that are straight lines or planes. Again, in high
dimensions, linear models for classification become very powerful, and
guarding against overfitting becomes increasingly important when
considering more features.
<a data-type="indexterm" data-primary="" data-startref="Clinearsvc2" id="idm45613688069096"></a><a data-type="indexterm" data-primary="" data-startref="SLCFlinearsvc2" id="idm45613688068120"></a></p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_breast_cancer" id="idm45613688066920"></a>
Let’s analyze <code>LogisticRegression</code> in more detail on the Breast Cancer
dataset:</p>

<p><code><strong>In[41]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_breast_cancer</code>
<code class="n">cancer</code> <code class="o">=</code> <code class="n">load_breast_cancer</code><code class="p">()</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">logreg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[41]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.953
Test set score: 0.958</pre>

<p>The default value of <code>C=1</code> provides quite good performance, with 95%
accuracy on both the training and the test set. But as training and test
set performance are very close, it is likely that we are underfitting.
Let’s try to increase <code>C</code> to fit a more flexible model:</p>

<p><code><strong>In[42]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">logreg100</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg100</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg100</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[42]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.972
Test set score: 0.965</pre>

<p>Using <code>C=100</code> results in higher training set accuracy, and also a
slightly increased test set accuracy, confirming our intuition that a
more complex model should perform better.</p>

<p>We can also investigate what happens if we use an even more regularized
model than the default of <code>C=1</code>, by setting <code>C=0.01</code>:</p>

<p><code><strong>In[43]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">logreg001</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Training set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Test set score: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">logreg001</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[43]:</strong></code></p>

<pre data-type="programlisting">Training set score: 0.934
Test set score: 0.930</pre>

<p>As expected, when moving more to the left along the scale shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#model_complexity">Figure&nbsp;2-1</a>
from an already underfit model, both training and test set accuracy
decrease relative to the default parameters.</p>

<p>Finally, let’s look at the coefficients learned by the models with the
three different settings of the regularization parameter <code>C</code> (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#l2_regularization">Figure&nbsp;2-17</a>):</p>

<p><code><strong>In[44]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">logreg</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="s1">'o'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"C=1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">logreg100</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="s1">'^'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"C=100"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">logreg001</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"C=0.001"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code> <code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">,</code> <code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">hlines</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Coefficient magnitude"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code></pre>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p><a data-type="indexterm" data-primary="L2 regularization" id="idm45613684925928"></a><a data-type="indexterm" data-primary="regularization" data-secondary="L2 regularization" id="idm45613684800904"></a>
As <code>LogisticRegression</code> applies an L2 regularization by default, the
result looks similar to  that produced by <code>Ridge</code> in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#regularization">Figure&nbsp;2-12</a>. Stronger
regularization pushes coefficients more and more toward zero, though
coefficients never become exactly zero. Inspecting the plot more
closely, we can also see an interesting effect in the third coefficient,
for “mean perimeter.” For <code>C=100</code> and <code>C=1</code>, the coefficient is negative,
while for <code>C=0.001</code>, the coefficient is positive, with a magnitude that is
even larger than for <code>C=1</code>. Interpreting a model like this, one might
think the coefficient tells us which class a feature is associated with.
For example, one might think that a high “texture error” feature is
related to a sample being “malignant.” However, the change of sign in
the coefficient for “mean perimeter” means that depending on which model
we look at, a high “mean perimeter” could be taken as being either
indicative of “benign” or indicative of “malignant.” This illustrates
that interpretations of coefficients of linear models should always be
taken with a grain of salt.</p>
</div>

<figure><div id="l2_regularization" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0216.png" alt="malp 0216" width="1563" height="1499" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0216.png">
<h6><span class="label">Figure 2-17. </span>Coefficients learned by logistic regression on the Breast Cancer dataset for different values of C</h6>
</div></figure>

<p class="pagebreak-before">If we desire a more interpretable model, using L1 regularization might
help, as it limits the model to using only a few features. Here is the
coefficient plot and classification accuracies for L1 regularization (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#l1_regularization">Figure&nbsp;2-18</a>):</p>

<p><code><strong>In[45]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">C</code><code class="p">,</code> <code class="n">marker</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="mf">0.001</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">100</code><code class="p">],</code> <code class="p">[</code><code class="s1">'o'</code><code class="p">,</code> <code class="s1">'^'</code><code class="p">,</code> <code class="s1">'v'</code><code class="p">]):</code>
    <code class="n">lr_l1</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="n">C</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="s2">"l1"</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Training accuracy of l1 logreg with C={:.3f}: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
          <code class="n">C</code><code class="p">,</code> <code class="n">lr_l1</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Test accuracy of l1 logreg with C={:.3f}: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
          <code class="n">C</code><code class="p">,</code> <code class="n">lr_l1</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">lr_l1</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">marker</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"C={:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">C</code><code class="p">))</code>

<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]),</code> <code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">,</code> <code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">hlines</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Coefficient magnitude"</code><code class="p">)</code>

<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code></pre>

<p><code><strong>Out[45]:</strong></code></p>

<pre data-type="programlisting">Training accuracy of l1 logreg with C=0.001: 0.91
Test accuracy of l1 logreg with C=0.001: 0.92
Training accuracy of l1 logreg with C=1.000: 0.96
Test accuracy of l1 logreg with C=1.000: 0.96
Training accuracy of l1 logreg with C=100.000: 0.99
Test accuracy of l1 logreg with C=100.000: 0.98</pre>

<p>As you can see, there are many parallels between linear models for
binary classification and linear models for regression. As in
regression, the main difference between the models is the <code>penalty</code> parameter, which
influences the regularization and whether the model will use all
available features or select only a subset.
<a data-type="indexterm" data-primary="" data-startref="Clogreg2" id="idm45613684571608"></a><a data-type="indexterm" data-primary="" data-startref="SLCFlogreg2" id="idm45613684570632"></a></p>

<figure><div id="l1_regularization" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0217.png" alt="malp 0217" width="1563" height="1499" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0217.png">
<h6><span class="label">Figure 2-18. </span>Coefficients learned by logistic regression with L1 penalty on the Breast Cancer dataset for different values of C</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Linear models for multiclass classification"><div class="sect3" id="linear-models-for-multiclass-classification">
<h3>Linear models for multiclass classification</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="multiclass classification" id="idm45613684566424"></a><a data-type="indexterm" data-primary="multiclass classification" data-secondary="linear models for" id="idm45613684565480"></a><a data-type="indexterm" data-primary="one-vs.-rest approach" id="idm45613684564568"></a>
Many linear classification models are for binary classification only,
and don’t extend naturally to the multiclass case (with the exception
of logistic regression). A common technique to extend a binary
classification algorithm to a multiclass classification algorithm is
the <em>one-vs.-rest</em> approach. In the one-vs.-rest approach, a binary model
is learned for each class that tries to separate that class from all
of the other classes, resulting in as many binary models as there are
classes. To make a prediction, all binary classifiers are run on a test
point. The classifier that has the highest score on its single class
“wins,” and this class label is returned as the prediction.</p>

<p>Having one binary classifier per class results in having one vector of
coefficients (<em>w</em>) and one intercept (<em>b</em>) for each
class. The class for which the result of the classification confidence formula given here is highest is the assigned class label:</p>
<ul class="simplelist">
  <li><em>w</em>[0] * <em>x</em>[0] + <em>w</em>[1] * <em>x</em>[1] + ... + <em>w</em>[<em>p</em>] * <em>x</em>[<em>p</em>] + <em>b</em></li>
</ul>

<p>The mathematics behind multiclass logistic regression differ somewhat from the one-vs.-rest approach, but they also result in one
coefficient vector and intercept per class, and the same method of
making a prediction is applied.</p>

<p>Let’s apply the one-vs.-rest method to a simple three-class
classification dataset. We use a two-dimensional dataset, where each
class is given by data sampled from a Gaussian distribution (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#one_vs_rest">Figure&nbsp;2-19</a>):</p>

<p><code><strong>In[46]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Class 0"</code><code class="p">,</code> <code class="s2">"Class 1"</code><code class="p">,</code> <code class="s2">"Class 2"</code><code class="p">])</code></pre>

<figure><div id="one_vs_rest" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0218.png" alt="malp 0218" width="1565" height="1054" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0218.png">
<h6><span class="label">Figure 2-19. </span>Two-dimensional toy dataset containing three classes</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearSVC" id="idm45613684509560"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LinearSVC" id="idm45613684508616"></a>
Now, we train a <code>LinearSVC</code> classifier on the dataset:</p>

<p><code><strong>In[47]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">linear_svm</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Coefficient shape: "</code><code class="p">,</code> <code class="n">linear_svm</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Intercept shape: "</code><code class="p">,</code> <code class="n">linear_svm</code><code class="o">.</code><code class="n">intercept_</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[47]:</strong></code></p>

<pre data-type="programlisting">Coefficient shape:  (3, 2)
Intercept shape:  (3,)</pre>

<p>We see that the shape of the <code>coef_</code> is <code>(3, 2)</code>, meaning that each row
of <code>coef_</code> contains the coefficient vector for one of the three classes
and each column holds the coefficient value for a specific feature
(there are two in this dataset). The <code>intercept_</code> is now a
one-dimensional array, storing the intercepts for each class.</p>

<p>Let’s visualize the lines given by the three binary classifiers (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#three_binary_classifiers">Figure&nbsp;2-20</a>):</p>

<p><code><strong>In[48]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">line</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">15</code><code class="p">,</code> <code class="mi">15</code><code class="p">)</code>
<code class="k">for</code> <code class="n">coef</code><code class="p">,</code> <code class="n">intercept</code><code class="p">,</code> <code class="n">color</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">linear_svm</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="n">linear_svm</code><code class="o">.</code><code class="n">intercept_</code><code class="p">,</code>
                                  <code class="n">mglearn</code><code class="o">.</code><code class="n">cm3</code><code class="o">.</code><code class="n">colors</code><code class="p">):</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="o">-</code><code class="p">(</code><code class="n">line</code> <code class="o">*</code> <code class="n">coef</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">intercept</code><code class="p">)</code> <code class="o">/</code> <code class="n">coef</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">color</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">10</code><code class="p">,</code> <code class="mi">15</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">10</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s1">'Class 0'</code><code class="p">,</code> <code class="s1">'Class 1'</code><code class="p">,</code> <code class="s1">'Class 2'</code><code class="p">,</code> <code class="s1">'Line class 0'</code><code class="p">,</code> <code class="s1">'Line class 1'</code><code class="p">,</code>
            <code class="s1">'Line class 2'</code><code class="p">],</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="mf">1.01</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">))</code></pre>

<p>You can see that all the points belonging to class 0 in the training
data are above the line corresponding to class 0, which means they are
on the “class 0” side of this binary classifier. The points in class 0
are above the line corresponding to class 2, which means they are
classified as “rest” by the binary classifier for class 2. The points
belonging to class 0 are to the left of the line corresponding to class 1,
which means the binary classifier for class 1 also classifies them
as “rest.” Therefore, any point in this area will be classified as class
0 by the final classifier (the result of the classification confidence formula for classifier 0 is greater than
zero, while it is smaller than zero for the other two classes).</p>

<p>But what about the triangle in the middle of the plot? All three binary
classifiers classify points there as “rest.” Which class would a point
there be assigned to? The answer is the one with the highest value for the classification formula: the class of the closest line.</p>

<figure><div id="three_binary_classifiers" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0219.png" alt="malp 0219" width="1565" height="809" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0219.png">
<h6><span class="label">Figure 2-20. </span>Decision boundaries learned by the three one-vs.-rest classifiers</h6>
</div></figure>

<p>The following example (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#all_regions_of_2d">Figure&nbsp;2-21</a>) shows the predictions for all regions of the
2D space:</p>

<p><code><strong>In[49]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_classification</code><code class="p">(</code><code class="n">linear_svm</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">7</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">line</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">15</code><code class="p">,</code> <code class="mi">15</code><code class="p">)</code>
<code class="k">for</code> <code class="n">coef</code><code class="p">,</code> <code class="n">intercept</code><code class="p">,</code> <code class="n">color</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">linear_svm</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="n">linear_svm</code><code class="o">.</code><code class="n">intercept_</code><code class="p">,</code>
                                  <code class="n">mglearn</code><code class="o">.</code><code class="n">cm3</code><code class="o">.</code><code class="n">colors</code><code class="p">):</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="o">-</code><code class="p">(</code><code class="n">line</code> <code class="o">*</code> <code class="n">coef</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">intercept</code><code class="p">)</code> <code class="o">/</code> <code class="n">coef</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">color</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s1">'Class 0'</code><code class="p">,</code> <code class="s1">'Class 1'</code><code class="p">,</code> <code class="s1">'Class 2'</code><code class="p">,</code> <code class="s1">'Line class 0'</code><code class="p">,</code> <code class="s1">'Line class 1'</code><code class="p">,</code>
            <code class="s1">'Line class 2'</code><code class="p">],</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="mf">1.01</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="all_regions_of_2d" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0220.png" alt="malp 0220" width="1565" height="794" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0220.png">
<h6><span class="label">Figure 2-21. </span>Multiclass decision boundaries derived from the three one-vs.-rest classifiers</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters-1">
<h3>Strengths, weaknesses, and parameters</h3>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="parameters" id="idm45613684193112"></a><a data-type="indexterm" data-primary="L2 regularization" id="idm45613684006008"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearSVC" id="idm45613684005336"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LinearSVC" id="idm45613684004424"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LogisticRegression" id="idm45613684003480"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LogisticRegression" id="idm45613684002520"></a>
The main parameter of linear models is the regularization parameter,
called <code>alpha</code> in the regression models and <code>C</code> in <code>LinearSVC</code> and
<code>LogisticRegression</code>. Large values for <code>alpha</code> or small values for <code>C</code> mean simple models. In
particular for the regression models, tuning these parameters is quite
important. Usually <code>C</code> and <code>alpha</code> are searched for on a logarithmic
scale. The other decision you have to make is whether you want to use L1
regularization or L2 regularization. If you assume that only a few of your
features are actually important, you should use L1. Otherwise, you
should default to L2. L1 can also be useful if interpretability of the
model is important. As L1 will use only a few features, it is easier to
explain which features are important to the model, and what the effects
of these features are.</p>

<p><a data-type="indexterm" data-primary="linear models" data-secondary="strengths and weaknesses" id="idm45613683997368"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="Ridge" id="idm45613683996328"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="Ridge" id="idm45613683995368"></a>
Linear models are very fast to train, and also fast to predict. They
scale to very large datasets and work well with sparse data. If your
data consists of hundreds of thousands or millions of samples, you might
want to investigate using the <code>solver='sag'</code> option in
<code>LogisticRegression</code> and <code>Ridge</code>, which can be faster than the default
on large datasets. Other options are the <code>SGDClassifier</code> class and the
<code>SGDRegressor</code> class, which implement even more scalable versions of the
linear models described here.</p>

<p>Another strength of linear models is that they make it relatively easy
to understand how a prediction is made, using the formulas we saw earlier for regression
and classification. Unfortunately, it is often not
entirely clear why coefficients are the way they are. This is
particularly true if your dataset has highly correlated features; in
these cases, the coefficients might be hard to interpret.</p>

<p>Linear models often perform well when the number of features is large
compared to the number of samples. They are also often used on very
large datasets, simply because it’s not feasible to train other models.
However, in lower-dimensional spaces, other models might yield better
generalization performance. We will look at some examples in which linear
models fail in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#kernelized-support-vector-machines"> Section 2.3.7</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45613683989080">
<h5>Method Chaining</h5>
<p><a data-type="indexterm" data-primary="fit method" id="idm45613683987480"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="fit method" id="idm45613683986776"></a><a data-type="indexterm" data-primary="method chaining" id="idm45613683985832"></a>
The <code>fit</code> method of all <code>scikit-learn</code> models
returns <code>self</code>. This allows you to write code like the following, which
we’ve already used extensively in this chapter:</p>

<p><code><strong>In[50]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># instantiate model and fit it in one line</code>
<code class="n">logreg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="predict method" id="idm45613683978296"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="predict method" id="idm45613683976744"></a>
Here, we used the return value of <code>fit</code> (which is <code>self</code>) to assign the
trained model to the variable <code>logreg</code>. This concatenation of method
calls (here <code>__init__</code> and then <code>fit</code>) is known as <em>method chaining</em>.
Another common application of method chaining in <code>scikit-learn</code> is to
<code>fit</code> and <code>predict</code> in one line:</p>

<p><code><strong>In[51]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">logreg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">logreg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>Finally, you can even do model instantiation, fitting, and predicting in
one line:</p>

<p><code><strong>In[52]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_pred</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>This very short variant is not ideal, though. A lot is happening in a
single line, which might make the code hard to read. Additionally, the
fitted logistic regression model isn’t stored in any variable, so we
can’t inspect it or use it to predict on any other data.<a data-type="indexterm" data-primary="" data-startref="SLalglin2" id="idm45613683916360"></a></p>
</div></aside>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.4 Naive Bayes Classifiers"><div class="sect2" id="naive-bayes-classifiers">
<h2>2.3.4 Naive Bayes Classifiers</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="naive Bayes classifiers" id="idm45613683913784"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="naive Bayes" id="Asupclasnaive2"></a><a data-type="indexterm" data-primary="naive Bayes classifiers" data-secondary="kinds in scikit-learn" id="idm45613683910856"></a><a data-type="indexterm" data-primary="classification problems" data-secondary="naive Bayes classifiers" id="idm45613683909912"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="naive Bayes" id="CnaiveB2"></a>
Naive Bayes classifiers are a family of classifiers that are quite
similar to the linear models discussed in the previous section. However, they tend to be
even faster in training. The price paid for this efficiency is that
naive Bayes models often provide generalization performance that is
slightly worse than that of linear classifiers like <code>LogisticRegression</code> and
<code>LinearSVC</code>.<a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearSVC" id="idm45613683846104"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="LinearSVC" id="idm45613683845128"></a></p>

<p><a data-type="indexterm" data-primary="GaussianNB" id="idm45613683844056"></a><a data-type="indexterm" data-primary="BernoulliNB" id="idm45613683842952"></a><a data-type="indexterm" data-primary="MultinomialNB" id="idm45613683842280"></a>
The reason that naive Bayes models are so efficient is that they learn
parameters by looking at each feature individually and collect simple
per-class statistics from each feature. There are three kinds of naive
Bayes classifiers implemented in <code>scikit-learn</code>: <code>GaussianNB</code>,
<code>BernoulliNB</code>, and <code>MultinomialNB</code>. <code>GaussianNB</code> can be applied to any
continuous data, while <code>BernoulliNB</code> assumes binary data and
<code>MultinomialNB</code> assumes count data (that is, that each feature represents an
integer count of something, like how often a word appears in a
sentence). <code>BernoulliNB</code> and <code>MultinomialNB</code> are mostly used in text
data classification.</p>

<p>The <code>BernoulliNB</code> classifier counts how often every feature of each
class is not zero. This is most easily understood with an example:</p>

<p><code><strong>In[53]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
              <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]])</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code></pre>

<p>Here, we have four data points, with four binary features each. There
are two classes, 0 and 1. For class 0 (the first and third data points),
the first feature is zero two times and nonzero zero times, the second
feature is zero one time and nonzero one time, and so on. These same
counts are then calculated for the data points in the second class.
Counting the nonzero entries per class in essence looks like this:</p>

<p><code><strong>In[54]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">counts</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">y</code><code class="p">):</code>
    <code class="c1"># iterate over each class</code>
    <code class="c1"># count (sum) entries of 1 per feature</code>
    <code class="n">counts</code><code class="p">[</code><code class="n">label</code><code class="p">]</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">y</code> <code class="o">==</code> <code class="n">label</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Feature counts:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">counts</code><code class="p">)</code></pre>

<p><code><strong>Out[54]:</strong></code></p>

<pre data-type="programlisting">Feature counts:
{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}</pre>

<p>The other two naive Bayes models, <code>MultinomialNB</code> and <code>GaussianNB</code>, are
slightly different in what kinds of statistics they compute.
<code>MultinomialNB</code> takes into account the average value of each feature for
each class, while <code>GaussianNB</code> stores the average value as well as the
standard deviation of each feature for each class.</p>

<p>To make a prediction, a data point is compared to the statistics for
each of the classes, and the best matching class is predicted.
Interestingly, for both <code>MultinomialNB</code> and <code>BernoulliNB</code>, this leads to a
prediction formula that is of the same form as in the linear models
(see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear-models-for-classification">“Linear models for classification”</a>). Unfortunately, <code>coef_</code> for the naive Bayes models has a
somewhat different meaning than in the linear models, in that <code>coef_</code> is
not the same as <em>w</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters-2">
<h3>Strengths, weaknesses, and parameters</h3>

<p><code>MultinomialNB</code> <a data-type="indexterm" data-primary="naive Bayes classifiers" data-secondary="parameters" id="idm45613683669000"></a>
and <code>BernoulliNB</code> have a single parameter, <code>alpha</code>,
which controls model complexity. The way <code>alpha</code> works is that the
algorithm adds  to the data <code>alpha</code> many virtual data points that have
positive values for all the features. This results in a “smoothing” of
the statistics. A large <code>alpha</code> means more smoothing, resulting in less
complex models. The algorithm’s performance is relatively robust to the
setting of <code>alpha</code>, meaning that setting <code>alpha</code> is not critical for good
performance. However, tuning it usually improves accuracy somewhat.</p>

<p><code>GaussianNB</code> is mostly used on very high-dimensional data, while the
other two variants of naive Bayes are widely used for sparse count data
such as text. <code>MultinomialNB</code> usually performs better than <code>BernoulliNB</code>, particularly on datasets with a relatively large number of nonzero
features (i.e., large documents).</p>

<p><a data-type="indexterm" data-primary="naive Bayes classifiers" data-secondary="strengths and weaknesses" id="idm45613683662376"></a>
The naive Bayes models share many of the strengths and weaknesses of the
linear models. They are very fast to train and to predict, and the
training procedure is easy to understand. The models work very well with
high-dimensional sparse data and are relatively robust to the
parameters. Naive Bayes models are great baseline models and are often
used on very large datasets, where training even a linear model might
take too long.<a data-type="indexterm" data-primary="" data-startref="CnaiveB2" id="idm45613683660760"></a><a data-type="indexterm" data-primary="" data-startref="Asupclasnaive2" id="idm45613683659816"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.5 Decision Trees"><div class="sect2" id="decision-trees">
<h2>2.3.5 Decision Trees</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="decision trees" id="SLalgdectree2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="decision trees" id="ALdectree2"></a><a data-type="indexterm" data-primary="decision trees" data-secondary="if/else structure of" id="idm45613683654056"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="decision trees" id="ALregtree2"></a>
Decision trees are widely used models for classification and
regression tasks. Essentially, they learn a hierarchy of if/else
questions, leading to a decision.</p>

<p>These questions are similar to the questions you might ask in a game of
20 Questions. Imagine you want to distinguish between the following
four animals: bears, hawks, penguins, and dolphins. Your goal is to get
to the right answer by asking as few if/else questions as possible. You
might start off by asking whether the animal has feathers, a question
that narrows down your possible animals to just two. If the
answer is “yes,” you can ask another question that could help you
distinguish between hawks and penguins. For example, you could ask
whether the animal can fly. If the animal doesn’t have feathers,
your possible animal choices are dolphins and bears, and you will need
to ask a question to distinguish between these two animals—for example,
asking whether the animal has fins.</p>

<p>This series of questions can be expressed as a decision tree, as shown
in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#animal_tree">Figure&nbsp;2-22</a>.</p>

<p><code><strong>In[55]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_animal_tree</code><code class="p">()</code></pre>

<figure><div id="animal_tree" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0221.png" alt="malp 0221" width="880" height="668" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0221.png">
<h6><span class="label">Figure 2-22. </span>A decision tree to distinguish among several animals</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="terminal nodes" id="idm45613683644056"></a><a data-type="indexterm" data-primary="leafs" id="idm45613683643352"></a>
In this illustration, each node in the tree
either represents a question or a terminal node (also called a <em>leaf</em>)
that contains the answer. The edges connect the answers to a question
with the next question you would ask.</p>

<p>In machine learning parlance, we built a model to distinguish between
four classes of animals (hawks, penguins, dolphins, and bears) using the
three features “has feathers,” “can fly,” and “has fins.” Instead of
building these models by hand, we can learn them from data using
supervised learning.</p>










<section data-type="sect3" data-pdf-bookmark="Building decision trees"><div class="sect3" id="building-decision-trees">
<h3>Building decision trees</h3>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="building" id="idm45613683638056"></a>
Let’s go through the process of building a decision tree for the 2D
classification dataset shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildinga">Figure&nbsp;2-23</a>. The
dataset consists of two half-moon shapes, with each class consisting of
75 data points. We will refer to this dataset as <code>two_moons</code>.</p>

<p>Learning a decision tree means learning the sequence of if/else questions
that gets us to the true answer most quickly. In the machine learning
setting, these questions are called <em>tests</em> (not to be confused with the
test set, which is the data we use to test to see how generalizable our
model is). Usually data does not come in the form of binary yes/no
features as in the animal example, but is instead represented as
continuous features such as in the 2D dataset shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildinga">Figure&nbsp;2-23</a>. The
tests that are used on continuous data are of the form “Is feature <em>i</em>
larger than value <em>a</em>?”</p>

<figure><div id="tree_buildinga" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0222a.png" alt="malp 0222a" width="1440" height="963" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0222a.png">
<h6><span class="label">Figure 2-23. </span>Two-moons dataset on which the decision tree will be built</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="top nodes" id="idm45613683600312"></a><a data-type="indexterm" data-primary="roots" id="idm45613683599384"></a>
To build a tree, the algorithm searches over all possible tests and
finds the one that is most informative about the target variable. <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingb">Figure&nbsp;2-24</a> shows the first
test that is picked. Splitting the dataset horizontally at <code>x[1]=0.0596</code>
yields the most information; it best separates the points in class 0
from the points in class 1. The top node, also called the <em>root</em>,
represents the whole dataset, consisting of 50 points belonging to class
0 and 50 points belonging to class 1. The split is done by testing whether
<code>x[1] &lt;= 0.0596</code>, indicated by a black line. If the test is true, a
point is assigned to the left node, which contains 2 points belonging to
class 0 and 32 points belonging to class 1. Otherwise the point is
assigned to the right node, which contains 48 points belonging to class
0 and 18 points belonging to class 1. These two nodes correspond to the
top and bottom regions shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingb">Figure&nbsp;2-24</a>. Even though the first
split did a good job of separating the two classes, the bottom region still
 contains points belonging to class 0, and the top region still contains
points belonging to class 1.
We can build a more accurate model by repeating the process of looking
for the best test in both regions. <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingc">Figure&nbsp;2-25</a> shows that the most informative next split for the left
and the right region is based on <code>x[0]</code>.</p>

<figure><div id="tree_buildingb" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0222b.png" alt="malp 0222b" width="1565" height="564" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0222b.png">
<h6><span class="label">Figure 2-24. </span>Decision boundary of tree with depth 1 (left) and corresponding tree (right)</h6>
</div></figure>

<figure><div id="tree_buildingc" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0222c.png" alt="malp 0222c" width="1565" height="564" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0222c.png">
<h6><span class="label">Figure 2-25. </span>Decision boundary of tree with depth 2 (left) and corresponding decision tree (right)</h6>
</div></figure>

<p>This recursive process yields a binary tree of decisions, with each node
containing a test. Alternatively, you can think of each test as
splitting the part of the data that is currently being considered along one
axis. This yields a view of the algorithm as building a hierarchical
partition. As each test concerns only a single feature, the regions in
the resulting partition always have axis-parallel boundaries.</p>

<p><a data-type="indexterm" data-primary="pure leafs" id="idm45613683570712"></a>
The recursive partitioning of the data is repeated until each region in
the partition (each leaf in the decision tree) only contains a single
target value (a single class or a single regression value). A leaf of
the tree that contains data points that all share the same target value
is called <em>pure</em>. The final partitioning for this dataset is shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingd">Figure&nbsp;2-26</a>.</p>

<figure><div id="tree_buildingd" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0222d.png" alt="malp 0222d" width="1565" height="564" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0222d.png">
<h6><span class="label">Figure 2-26. </span>Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize</h6>
</div></figure>

<p>A prediction on a new data point is made by checking which region of the
partition of the feature space the point lies in, and then predicting
the majority target (or the single target in the case of pure leaves) in
that region. The region can be found by traversing the tree from the
root and going left or right, depending on whether the test is fulfilled
or not.</p>

<p>It is also possible to use trees for regression tasks, using exactly the
same technique. To make a prediction, we traverse the tree based on the
tests in each node and find the leaf the new data point falls into. The
output for this data point is the mean target of the training points in
this leaf.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Controlling complexity of decision trees"><div class="sect3" id="controlling-complexity-of-decision-trees">
<h3>Controlling complexity of decision trees</h3>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="controlling complexity of" id="idm45613683563096"></a>
Typically, building a tree as described here and continuing until all
leaves are pure leads to models that are very complex and highly overfit
to the training data. The presence of pure leaves mean that a tree is
100% accurate on the training set; each data point in the training set
is in a leaf that has the correct majority class. The overfitting can be
seen on the left of <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingd">Figure&nbsp;2-26</a>. You can see the regions determined to belong to class 1
in the middle of all the points belonging to class 0. On the other hand,
there is a small strip predicted as class 0 around the point belonging
to class 0 to the very right. This is not how one would imagine the
decision boundary to look, and the decision boundary focuses a lot on
single outlier points that are far away from the other points in that
class.</p>

<p><a data-type="indexterm" data-primary="pre- and post-pruning" id="idm45613683560136"></a><a data-type="indexterm" data-primary="pruning for decision trees" id="idm45613683559432"></a>
There are two common strategies to prevent overfitting: stopping the
creation of the tree early (also called <em>pre-pruning</em>), or building the
tree but then removing or collapsing nodes that contain little
information (also called <em>post-pruning</em> or just <em>pruning</em>). Possible
criteria for pre-pruning include limiting the maximum depth of the tree,
limiting the maximum number of leaves, or requiring a minimum number of
points in a node to keep splitting it.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="DecisionTreeRegressor" id="idm45613683556664"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="DecisionTreeClassifier" id="idm45613683555496"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="DecisionTreeClassifier" id="idm45613683554584"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="DecisionTreeRegressor" id="idm45613683553640"></a>
Decision trees in <code>scikit-learn</code> are implemented in the
<code>DecisionTreeRegressor</code> and <code>DecisionTreeClassifier</code> classes.
<code>scikit-learn</code> only implements pre-pruning, not post-pruning.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_breast_cancer" id="idm45613683550616"></a>
Let’s look at the effect of pre-pruning in more detail on the Breast
Cancer dataset. As always, we import the dataset and split it into a
training and a test part. Then we build a model using the default setting
of fully developing the tree (growing the tree until all leaves are
pure). We fix the <code>random_state</code> in the tree, which is used for
tie-breaking internally:</p>

<p><code><strong>In[56]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">cancer</code> <code class="o">=</code> <code class="n">load_breast_cancer</code><code class="p">()</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">tree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">tree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[56]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 1.000
Accuracy on test set: 0.937</pre>

<p>As expected, the accuracy on the training set is 100%—because the leaves are
pure, the tree was grown deep enough that it could perfectly
memorize all the labels on the training data. The test set accuracy is
slightly worse than for the linear models we looked at previously, which had around 95%
accuracy.</p>

<p>If we don’t restrict the depth of a decision tree, the tree
can become arbitrarily deep and complex. Unpruned trees are therefore
prone to overfitting and not generalizing well to new data. Now let’s
apply pre-pruning to the tree, which will stop developing the tree
before we perfectly fit to the training data. One option is to
stop building the tree after a certain depth has been reached. Here we
set <code>max_depth=4</code>, meaning only four consecutive questions can be asked
(cf. Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingb">2-24</a> and
<a data-type="xref" data-xrefstyle="select:labelnumber" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#tree_buildingd">2-26</a>). Limiting the depth of the tree decreases overfitting. This leads to a
lower accuracy on the training set, but an improvement on the test set:</p>

<p><code><strong>In[57]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">tree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p class="pagebreak-before"><code><strong>Out[57]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.988
Accuracy on test set: 0.951</pre>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Analyzing decision trees"><div class="sect3" id="analyzing-decision-trees">
<h3>Analyzing decision trees</h3>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="analyzing" id="idm45613683344600"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="export_graphviz" id="idm45613683343432"></a><a data-type="indexterm" data-primary="tree module" id="idm45613683342520"></a><a data-type="indexterm" data-primary="graphviz module" id="idm45613683341848"></a>
We can visualize the tree using the <code>export_graphviz</code> function from the
<code>tree</code> module. This writes a file in the <em>.dot</em> file format, which is a
text file format for storing graphs. We set an option to color the nodes
to reflect the majority class in each node and pass the class and
features names so the tree can be properly labeled:</p>

<p><code><strong>In[58]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">export_graphviz</code>
<code class="n">export_graphviz</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">out_file</code><code class="o">=</code><code class="s2">"tree.dot"</code><code class="p">,</code> <code class="n">class_names</code><code class="o">=</code><code class="p">[</code><code class="s2">"malignant"</code><code class="p">,</code> <code class="s2">"benign"</code><code class="p">],</code>
                <code class="n">feature_names</code><code class="o">=</code><code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">,</code> <code class="n">impurity</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">filled</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<p>We can read this file and visualize it, as seen in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#fig02in01">Figure&nbsp;2-27</a>, using the <code>graphviz</code> module (or you
can use any program that can read <em>.dot</em> files):</p>

<p><code><strong>In[59]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">graphviz</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"tree.dot"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
    <code class="n">dot_graph</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="n">display</code><code class="p">(</code><code class="n">graphviz</code><code class="o">.</code><code class="n">Source</code><code class="p">(</code><code class="n">dot_graph</code><code class="p">))</code></pre>

<figure><div id="fig02in01" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_02in01.png" alt="svg" width="1559" height="906" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_02in01.png">
<h6><span class="label">Figure 2-27. </span>Visualization of the decision tree built on the Breast Cancer dataset</h6>
</div></figure>

<p>The visualization of the tree provides a great in-depth view of how the
algorithm makes predictions, and is a good example of a machine learning
algorithm that is easily explained to nonexperts. However, even with a
tree of depth four, as seen here, the tree can become a bit
overwhelming. Deeper trees (a depth of 10 is not uncommon) are even harder
to grasp. One method of inspecting the tree that may be helpful is to
find out which path most of the data actually takes. The <code>samples</code>
shown in each node in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#fig02in01">Figure&nbsp;2-27</a> gives the number of samples in that
node, while <code>value</code> provides the number of samples per class. Following
the branches to the right, we see that <code>worst radius &gt; 16.795</code> creates
a node that contains only 8 benign but 134 malignant samples. The rest
of this side of the tree then uses some finer distinctions to split off
these 8 remaining benign samples. Of the 142 samples that went to the
right in the initial split, nearly all of them (132) end up in the leaf
to the very right.</p>

<p>Taking a left at the root, for <code>worst radius &lt;= 16.795</code> we end up with
25 malignant and 259 benign samples. Nearly all of the benign samples
end up in the second leaf from the left, with most of the other leaves containing very few samples.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Feature importance in trees"><div class="sect3" id="feature-importance-in-trees">
<h3>Feature importance in trees</h3>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="feature importance in" id="idm45613683226312"></a><a data-type="indexterm" data-primary="feature importance" id="idm45613683225336"></a>
Instead of looking at the whole tree, which can be taxing, there are
some useful properties that we can derive to summarize the workings of
the tree. The most commonly used summary is <em>feature importance</em>, which
rates how important each feature is for the decision a tree makes. It is
a number between 0 and 1 for each feature, where 0 means “not used at
all” and 1 means “perfectly predicts the target.” The feature
importances always sum to 1:</p>

<p><code><strong>In[60]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Feature importances:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">tree</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">)</code></pre>

<p><code><strong>Out[60]:</strong></code></p>

<pre data-type="programlisting" class="less_space2">Feature importances:
[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.01  0.048
 0.    0.    0.002 0.    0.    0.    0.    0.    0.727 0.046 0.    0.
 0.014 0.    0.018 0.122 0.012 0.   ]</pre>

<p>We can visualize the feature importances in a way that is similar to the
way we visualize the coefficients in the linear model (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#visualize_feature_importances">Figure&nbsp;2-28</a>):</p>

<p><code><strong>In[61]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python" class="less_space"><code class="k">def</code> <code class="nf">plot_feature_importances_cancer</code><code class="p">(</code><code class="n">model</code><code class="p">):</code>
    <code class="n">n_features</code> <code class="o">=</code> <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">n_features</code><code class="p">),</code> <code class="n">model</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code> <code class="n">align</code><code class="o">=</code><code class="s1">'center'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">yticks</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">n_features</code><code class="p">),</code> <code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature importance"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="n">n_features</code><code class="p">)</code>

<code class="n">plot_feature_importances_cancer</code><code class="p">(</code><code class="n">tree</code><code class="p">)</code></pre>

<figure><div id="visualize_feature_importances" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0223.png" alt="malp 0223" width="1565" height="844" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0223.png">
<h6><span class="label">Figure 2-28. </span>Feature importances computed from a decision tree learned on the Breast Cancer dataset</h6>
</div></figure>

<p>Here we see that the feature used in the top split (“worst radius”) is
by far the most important feature. This confirms our observation in
analyzing the tree that the first level already separates the two
classes fairly well.</p>

<p>However, if a feature has a low value in <code>feature_importance_</code>, it doesn’t mean
that this feature is uninformative. It only means that the feature was
not picked by the tree, likely because another feature encodes the same
information.</p>

<p>In contrast to the coefficients in linear models, feature importances
are always positive, and don’t encode which class a feature is
indicative of. The feature importances tell us that “worst radius” is
important, but not whether a high radius is indicative
of a sample being benign or malignant. In fact, there might not be
such a simple relationship between features and class, as you can see in the following example (Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#another_relationship_features_class">2-29</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#relationship_features_class">2-30</a>):</p>

<p><code><strong>In[62]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">tree</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_tree_not_monotone</code><code class="p">()</code>
<code class="n">display</code><code class="p">(</code><code class="n">tree</code><code class="p">)</code></pre>

<p><code><strong>Out[62]:</strong></code></p>

<pre data-type="programlisting">Feature importances: [ 0.  1.]</pre>

<figure><div id="another_relationship_features_class" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0225.png" alt="malp 0225" width="1520" height="1015" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0225.png">
<h6><span class="label">Figure 2-29. </span>A two-dimensional dataset in which the feature on the y-axis has a nonmonotonous relationship with the class label, and the decision boundaries found by a decision tree</h6>
</div></figure>

<figure><div id="relationship_features_class" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0224.png" alt="malp 0224" width="844" height="786" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0224.png">
<h6><span class="label">Figure 2-30. </span>Decision tree learned on the data shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#another_relationship_features_class">Figure&nbsp;2-29</a></h6>
</div></figure>

<p>The plot shows a dataset with two features and two classes. Here, all
the information is contained in <code>X[1]</code>, and <code>X[0]</code> is not used at all.
But the relation between <code>X[1]</code> and the output class is not monotonous,
meaning we cannot say “a high value of <code>X[1]</code> means class 0, and a low
value means class 1” (or vice versa).</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="DecisionTreeRegressor" id="idm45613683016664"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="DecisionTreeRegressor" id="idm45613683015720"></a>
While we focused our discussion here on decision trees for
classification, all that was said is similarly true for decision trees
for regression, as implemented in <code>DecisionTreeRegressor</code>. The usage and analysis of regression trees is very similar to that of classification
trees. There is one particular property of using tree-based models for
regression that we want to point out, though. The
<code>DecisionTreeRegressor</code> (and all other tree-based regression models) is
not able to <em>extrapolate</em>, or make predictions outside of the
range of the training data.</p>

<p>Let’s look into this in more detail, using a dataset of historical computer
memory (RAM) prices. <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#ram_prices">Figure&nbsp;2-31</a> shows the dataset, with the date
on the x-axis and the price of one megabyte of RAM in that year on the
y-axis:</p>

<p><code><strong>In[63]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">os</code>
<code class="n">ram_prices</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">mglearn</code><code class="o">.</code><code class="n">datasets</code><code class="o">.</code><code class="n">DATA_PATH</code><code class="p">,</code>
                                      <code class="s2">"ram_price.csv"</code><code class="p">))</code>

<code class="n">plt</code><code class="o">.</code><code class="n">semilogy</code><code class="p">(</code><code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code><code class="p">,</code> <code class="n">ram_prices</code><code class="o">.</code><code class="n">price</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Year"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Price in $/Mbyte"</code><code class="p">)</code></pre>

<figure><div id="ram_prices" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0226.png" alt="malp 0226" width="1565" height="1062" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0226.png">
<h6><span class="label">Figure 2-31. </span>Historical development of the price of RAM, plotted on a log scale</h6>
</div></figure>

<p>Note the logarithmic scale of the y-axis. When plotting
logarithmically, the relation seems to be quite linear and so should be
relatively easy to predict, apart from some bumps.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="LinearRegression" id="idm45613682954408"></a><a data-type="indexterm" data-primary="regression" data-secondary="LinearRegression" id="idm45613682953304"></a>
We will make a forecast for the years after 2000 using the historical
data up to that point, with the date as our only feature. We will
compare two simple models: a <span class="keep-together"><code>DecisionTreeRegressor</code></span> and
<code>LinearRegression</code>. We rescale the prices using a logarithm, so that the
relationship is relatively linear. This doesn’t make a difference for
the <span class="keep-together"><code>DecisionTreeRegressor</code></span>, but it makes a big difference for
<code>LinearRegression</code> (we will discuss this in more depth in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch04.html#representing-data-and-engineering-features">Chapter&nbsp;4</a>).
After training the models and making predictions, we apply the
exponential map to undo the logarithm transform. We make predictions on
the whole dataset for visualization purposes here, but for a quantitative
evaluation we would only consider the test dataset:</p>

<p><code><strong>In[64]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>
<code class="c1"># use historical data to forecast prices after the year 2000</code>
<code class="n">data_train</code> <code class="o">=</code> <code class="n">ram_prices</code><code class="p">[</code><code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code> <code class="o">&lt;</code> <code class="mi">2000</code><code class="p">]</code>
<code class="n">data_test</code> <code class="o">=</code> <code class="n">ram_prices</code><code class="p">[</code><code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code> <code class="o">&gt;=</code> <code class="mi">2000</code><code class="p">]</code>

<code class="c1"># predict prices based on date</code>
<code class="n">X_train</code> <code class="o">=</code> <code class="n">data_train</code><code class="o">.</code><code class="n">date</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>
<code class="c1"># we use a log-transform to get a simpler relationship of data to target</code>
<code class="n">y_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">data_train</code><code class="o">.</code><code class="n">price</code><code class="p">)</code>

<code class="n">tree</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">linear_reg</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="c1"># predict on all data</code>
<code class="n">X_all</code> <code class="o">=</code> <code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>

<code class="n">pred_tree</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_all</code><code class="p">)</code>
<code class="n">pred_lr</code> <code class="o">=</code> <code class="n">linear_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_all</code><code class="p">)</code>

<code class="c1"># undo log-transform</code>
<code class="n">price_tree</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">pred_tree</code><code class="p">)</code>
<code class="n">price_lr</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">pred_lr</code><code class="p">)</code></pre>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#ram_prices_predictions">Figure&nbsp;2-32</a>, created here, compares the predictions of the decision
tree and the linear regression model with the ground truth:</p>

<p><code><strong>In[65]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">semilogy</code><code class="p">(</code><code class="n">data_train</code><code class="o">.</code><code class="n">date</code><code class="p">,</code> <code class="n">data_train</code><code class="o">.</code><code class="n">price</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Training data"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">semilogy</code><code class="p">(</code><code class="n">data_test</code><code class="o">.</code><code class="n">date</code><code class="p">,</code> <code class="n">data_test</code><code class="o">.</code><code class="n">price</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Test data"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">semilogy</code><code class="p">(</code><code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code><code class="p">,</code> <code class="n">price_tree</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Tree prediction"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">semilogy</code><code class="p">(</code><code class="n">ram_prices</code><code class="o">.</code><code class="n">date</code><code class="p">,</code> <code class="n">price_lr</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Linear prediction"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code></pre>

<figure><div id="ram_prices_predictions" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0227.png" alt="malp 0227" width="384" height="252" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0227.png">
<h6><span class="label">Figure 2-32. </span>Comparison of predictions made by a linear model and predictions made by a regression tree on the RAM price data</h6>
</div></figure>

<p>The
difference between the models is quite striking. The linear model approximates the data with a line, as we knew it would.
This line provides quite a good forecast for the test data (the years
after 2000), while glossing over some of the finer variations in both
the training and the test data. The tree model, on the other hand, makes
perfect predictions on the training data; we did not restrict the complexity
of the tree, so it learned the whole dataset by heart. However, once we
leave the data range for which the model has data, the model simply
keeps predicting the last known point. The tree has no ability to
generate “new” responses, outside of what was seen in the training data.
This shortcoming applies to all models based on trees.<sup><a data-type="noteref" id="idm45613682724856-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613682724856" class="totri-footnote">9</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters-3">
<h3>Strengths, weaknesses, and parameters</h3>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="parameters" id="idm45613682722824"></a>
As discussed earlier, the parameters that control model complexity in
decision trees are the pre-pruning parameters that stop the building of
the tree before it is fully developed. Usually, picking one of the
pre-pruning strategies—setting either <code>max_depth</code>, <code>max_leaf_nodes</code>, or
<code>min_samples_leaf</code>—is sufficient to prevent overfitting.</p>

<p><a data-type="indexterm" data-primary="decision trees" data-secondary="strengths and weaknesses" id="idm45613682663672"></a>
Decision trees have two advantages over many of the algorithms we’ve
discussed so far: the resulting model can easily be visualized and
understood by nonexperts (at least for smaller trees), and the
algorithms are completely invariant to scaling of the data. As each
feature is processed separately, and the possible splits of the data
don’t depend on scaling, no preprocessing like normalization or
standardization of features is needed for decision tree algorithms. In
particular, decision trees work well when you have features that are on
completely different scales, or a mix of binary and continuous features.</p>

<p>The main downside of decision trees is that even with the use of
pre-pruning, they tend to overfit and provide poor
generalization performance. Therefore, in most applications, the
ensemble methods we discuss next are usually used in place of a single
decision tree.<a data-type="indexterm" data-primary="" data-startref="ALdectree2" id="idm45613682661288"></a><a data-type="indexterm" data-primary="" data-startref="SLalgdectree2" id="idm45613682660312"></a><a data-type="indexterm" data-primary="" data-startref="ALregtree2" id="idm45613682659368"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.6 Ensembles of Decision Trees"><div class="sect2" id="ensembles-of-decision-trees">
<h2>2.3.6 Ensembles of Decision Trees</h2>

<p><em>Ensembles</em> <a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="ensembles of decision trees" id="SLalgensem2"></a><a data-type="indexterm" data-primary="ensembles" data-secondary="defined" id="idm45613682654840"></a>
are methods that combine multiple machine learning models to
create more powerful models. There are many models in the machine
learning literature that belong to this category, but there are two
ensemble models that have proven to be effective on a wide range of
datasets for classification and regression, both of which use decision
trees as their building blocks: random forests and gradient boosted
decision trees.</p>










<section data-type="sect3" data-pdf-bookmark="Random forests"><div class="sect3" id="random-forests">
<h3>Random forests</h3>

<p><a data-type="indexterm" data-primary="ensembles" data-secondary="random forests" id="ENrandom2"></a><a data-type="indexterm" data-primary="decision trees" data-secondary="vs. random forests" data-secondary-sortas="random forests" id="idm45613682650280"></a><a data-type="indexterm" data-primary="random forests" data-secondary="vs. decision trees" data-secondary-sortas="decision trees" id="idm45613682649064"></a>
As we just observed, a main drawback of decision trees is that they tend
to overfit the training data. Random forests are one way to address this
problem. A random forest is essentially a collection of decision trees,
where each tree is slightly different from the others. The idea behind
random forests is that each tree might do a relatively good job of
predicting, but will likely overfit on part of the data. If we build
many trees, all of which work well and overfit in different ways, we can
reduce the amount of overfitting by averaging their results. This
reduction in overfitting, while retaining the predictive power of the
trees, can be shown using rigorous mathematics.</p>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="randomization in" id="idm45613682646760"></a>
To implement this strategy, we need to build many decision trees. Each
tree should do an acceptable job of predicting the target, and should
also be different from the other trees. Random forests get their name
from injecting randomness into the tree building to ensure each tree is
different. There are two ways in which the trees in a random forest are
randomized: by selecting the data points used to build a tree and by
selecting the features in each split test. Let’s go into this process in
more detail.</p>












<section data-type="sect4" data-pdf-bookmark="Building random forests"><div class="sect4" id="building-random-forests">
<h4>Building random forests</h4>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="building" id="idm45613682643320"></a><a data-type="indexterm" data-primary="bootstrap samples" id="idm45613682642184"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="RandomForestRegressor" id="idm45613682641512"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="RandomForestClassifier" id="SLCFranforclas2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="random forests" id="Asuclranf2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="random forests" id="Asupregrforest2"></a>
To build a random forest model, you need to decide on the number of
trees to build (the <code>n_estimators</code> parameter of <code>RandomForestRegressor</code>
or <code>RandomForestClassifier</code>). Let’s say we want to build 10 trees. These
trees will be built completely independently from each other, and the
algorithm will make different random choices for each tree to make sure
the trees are distinct. To build a tree, we first take what is called a
<em>bootstrap sample</em> of our data. That is, from our
<code>n_samples</code> data points, we repeatedly draw an example randomly with
replacement (meaning the same sample can be picked multiple times),
<code>n_samples</code> times. This will create a dataset that is as big as the
original dataset, but some data points will be missing from it
(approximately one third), and some will be repeated.</p>

<p>To illustrate, let’s say we want to create a bootstrap sample of the list
<code>['a', 'b', 'c', 'd']</code>. A possible bootstrap sample would be
<code>['b', 'd', 'd', 'c']</code>. Another possible sample would be
<code>['d', 'a', 'd', 'a']</code>.</p>

<p>Next, a decision tree is built based on this newly created dataset.
However, the algorithm we described for the decision tree is slightly
modified. Instead of looking for the best test for each node, in each
node the algorithm randomly selects a subset of the features, and it looks
for the best possible test involving one of these features. The number
of features that are selected is controlled by the <code>max_features</code>
parameter. This selection of a subset of features is repeated separately
in each node, so that each node in a tree can make a decision using a
different subset of the features.</p>

<p>The bootstrap sampling leads to each decision tree in the random forest
being built on a slightly different dataset. Because of the selection of
features in each node, each split in each tree operates on a different
subset of features. Together, these two mechanisms ensure that all the
trees in the random forest are different.</p>

<p><a data-type="indexterm" data-primary="max_features parameter" id="idm45613682628888"></a>
A critical parameter in this process is <code>max_features</code>. If we set
<code>max_features</code> to <code>n_features</code>, that means that each split can look at
all features in the dataset, and no randomness will be injected in the
feature selection (the randomness due to the bootstrapping remains,
though). If we set <code>max_features</code> to <code>1</code>, that means that the splits
have no choice at all on which feature to test, and can only search over
different thresholds for the feature that was selected randomly.
Therefore, a high <code>max_features</code> means that the trees in the random
forest will be quite similar, and they will be able to fit the data
easily, using the most distinctive features. A low <code>max_features</code> means
that the trees in the random forest will be quite different, and that
each tree might need to be very deep in order to fit the data well.</p>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="predictions with" id="idm45613682623944"></a><a data-type="indexterm" data-primary="soft voting strategy" id="idm45613682622968"></a>
To make a prediction using the random forest, the algorithm first makes
a prediction for every tree in the forest. For regression, we can
average these results to get our final prediction. For classification, a
“soft voting” strategy is used. This means each algorithm makes a “soft”
prediction, providing a probability for each possible output label. The
probabilities predicted by all the trees are averaged, and the class
with the highest probability is predicted.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Analyzing random forests"><div class="sect4" id="analyzing-random-forests">
<h4>Analyzing random forests</h4>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="analyzing" id="idm45613682619496"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="make_moons" id="idm45613682618520"></a>
Let’s apply a random forest consisting of five trees to the <code>two_moons</code>
dataset we studied earlier:</p>

<p><code><strong>In[66]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">y</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">forest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">forest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p><a data-type="indexterm" data-primary="estimator_ attribute of RFECV" id="idm45613682614936"></a>
The trees that are built as part of the random forest are stored in the
<code>estimator_</code> attribute. Let’s visualize the decision boundaries learned
by each tree, together with their aggregate prediction as made by the
forest (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#estimator_attribute">Figure&nbsp;2-33</a>):</p>

<p><code><strong>In[67]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="p">(</code><code class="n">ax</code><code class="p">,</code> <code class="n">tree</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">forest</code><code class="o">.</code><code class="n">estimators_</code><code class="p">)):</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Tree {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">i</code><code class="p">))</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_tree_partition</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">tree</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>

<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">forest</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">],</code>
                                <code class="n">alpha</code><code class="o">=.</code><code class="mi">4</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Random Forest"</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p>You can clearly see that the decision boundaries learned by the five trees are
quite different. Each of them makes some mistakes, as some of the
training points that are plotted here were not actually included in the
training sets of the trees, due to the bootstrap sampling.</p>

<p>The random forest overfits less than any of the trees individually, and
provides a much more intuitive decision boundary. In any real
application, we would use many more trees (often hundreds or thousands),
leading to even smoother boundaries.</p>

<figure><div id="estimator_attribute" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0228.png" alt="malp 0228" width="1565" height="802" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0228.png">
<h6><span class="label">Figure 2-33. </span>Decision boundaries found by five randomized decision trees and the decision boundary obtained by averaging their predicted probabilities</h6>
</div></figure>

<p>As another example, let’s apply a random forest consisting of 100 trees on the Breast Cancer
dataset:</p>

<p><code><strong>In[68]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">forest</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">forest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">forest</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">forest</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[68]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 1.000
Accuracy on test set: 0.972</pre>

<p>The random forest gives us an accuracy of 97%, better than the linear
models or a single decision tree, without tuning any parameters. We
could adjust the <code>max_features</code> setting, or apply pre-pruning as we did
for the single decision tree. However, often the default parameters of
the random forest already work quite well.</p>

<p>Similarly to the decision tree, the random forest provides feature
importances, which are computed by aggregating the feature importances
over the trees in the forest. Typically, the feature importances provided
by the random forest are more reliable than the ones provided by a
single tree. Take a look at <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#random_forest">Figure&nbsp;2-34</a>.<a data-type="indexterm" data-primary="" data-startref="SLCFranforclas2" id="idm45613682363032"></a></p>

<p class="pagebreak-before"><code><strong>In[69]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_feature_importances_cancer</code><code class="p">(</code><code class="n">forest</code><code class="p">)</code></pre>

<figure><div id="random_forest" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0229.png" alt="malp 0229" width="1565" height="838" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0229.png">
<h6><span class="label">Figure 2-34. </span>Feature importances computed from a random forest that was fit to the Breast Cancer dataset</h6>
</div></figure>

<p>As you can see, the random forest gives nonzero importance to many more
features than the single tree. Similarly to the single decision tree, the
random forest also gives a lot of importance to the “worst radius” feature, but
it actually chooses “worst perimeter” to be the most informative feature
overall. The randomness in building the random forest forces the
algorithm to consider many possible explanations, the result being that the random forest captures a much broader picture of the data
than a single tree.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect4" id="strengths-weaknesses-and-parameters-4">
<h4>Strengths, weaknesses, and parameters</h4>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="strengths and weaknesses" id="idm45613682225528"></a>
Random forests for regression and classification are currently among the
most widely used machine learning methods. They are very powerful, often
work well without heavy tuning of the parameters, and don’t require
scaling of the data.</p>

<p>Essentially, random forests share all of the benefits of decision trees,
while making up for some of their deficiencies. One reason to still use
decision trees is if you need a compact representation of the decision-making process. It is basically impossible to interpret tens or hundreds
of trees in detail, and trees in random forests tend to be deeper than
decision trees (because of the use of feature subsets). Therefore, if
you need to summarize the prediction making in a visual way to
nonexperts, a single decision tree might be a better choice. While
building random forests on large datasets might be somewhat
time consuming, it can be parallelized across multiple CPU cores within a
computer easily. If you are using a multi-core processor (as nearly all
modern computers do), you can use the <code>n_jobs</code> parameter to adjust the
number of cores to use. Using more CPU cores will result in linear
speed-ups (using two cores, the training of the random forest will be
twice as fast), but specifying <code>n_jobs</code> larger than the number of cores
will not help. You can set <code>n_jobs=-1</code> to use all the cores in your
computer.</p>

<p>You should keep in mind that random forests, by their nature, are
random, and setting different random states (or not setting the
<code>random_state</code> at all) can drastically change the model that is built.
The more trees there are in the forest, the more robust it will be
against the choice of random state. If you want to have reproducible
results, it is important to fix the <code>random_state</code>.</p>

<p>Random forests don’t tend to perform well on very high dimensional,
sparse data, such as text data. For this kind of data, linear models
might be more appropriate. Random forests usually work well even on very
large datasets, and training can easily be parallelized over many CPU
cores within a powerful computer. However, random forests require more
memory and are slower to train and to predict than linear models. If
time and memory are important in an application, it might make sense to
use a linear model instead.</p>

<p><a data-type="indexterm" data-primary="random forests" data-secondary="parameters" id="idm45613682171528"></a>
The important parameters to adjust are <code>n_estimators</code>, <code>max_features</code>,
and possibly pre-pruning options like <code>max_depth</code>. For <code>n_estimators</code>,
larger is always better. Averaging more trees will yield a more robust
ensemble by reducing overfitting. However, there are diminishing
returns, and more trees need more memory and more time to train. A
common rule of thumb is to build “as many as you have time/memory
for.”</p>

<p>As described earlier, <code>max_features</code> determines how random each tree is,
and a smaller <code>max_features</code> reduces overfitting. In general, it’s a good rule of thumb to use the default values: <code>max_features=sqrt(n_features)</code> for classification and <code>max_features=n_features</code> for regression. Adding <code>max_features</code> or <code>max_leaf_nodes</code> might sometimes improve performance. It can also drastically reduce space and time requirements for training and prediction.<a data-type="indexterm" data-primary="" data-startref="ENrandom2" id="idm45613682164824"></a><a data-type="indexterm" data-primary="" data-startref="Asupregrforest2" id="idm45613682186504"></a><a data-type="indexterm" data-primary="" data-startref="Asuclranf2" id="idm45613682185560"></a></p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Gradient boosted regression trees (gradient boosting machines)"><div class="sect3" id="gradient-boosted-regression-trees-gradient-boosting-machines">
<h3>Gradient boosted regression trees (gradient boosting machines)</h3>

<p><a data-type="indexterm" data-primary="ensembles" data-secondary="gradient boosted regression trees" id="ENgrad2"></a><a data-type="indexterm" data-primary="gradient boosted regression trees" data-secondary="vs. random forests" data-secondary-sortas="random forests" id="idm45613682181352"></a><a data-type="indexterm" data-primary="random forests" data-secondary="vs. gradient boosted regression trees" data-secondary-sortas="gradient boosted regression trees" id="idm45613682180168"></a><a data-type="indexterm" data-primary="weak learners" id="idm45613682178920"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="gradient boosting" id="Asrgrad2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="gradient boosting" id="Ascgrad2"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="GradientBoostingClassifier" id="SLCFgradboostclas2"></a>
The gradient boosted regression tree is another ensemble method that
combines multiple decision trees to create a more powerful model. Despite the
“regression” in the name, these models can be used for regression and
classification. In contrast to the random forest approach, gradient boosting works
by building trees in a serial manner, where each tree tries to correct
the mistakes of the previous one. By default, there is no randomization
in gradient boosted regression trees; instead, strong pre-pruning is
used. Gradient boosted trees often use very shallow trees, of depth one
to five, which makes the model smaller in terms of memory and makes
predictions faster. The main idea behind gradient boosting is to combine
many simple models (in this context known as <em>weak learners</em>), like
shallow trees. Each tree can only provide good predictions on part of
the data, and so more and more trees are added to iteratively improve
performance.</p>

<p>Gradient boosted trees are frequently the winning entries in machine
learning competitions, and are widely used in industry. They are
generally a bit more sensitive to parameter settings than random
forests, but can provide better accuracy if the parameters are set
correctly.</p>

<p><a data-type="indexterm" data-primary="learning_rate parameter" id="idm45613682214856"></a><a data-type="indexterm" data-primary="gradient boosted regression trees" data-secondary="learning_rate parameter" id="idm45613682214152"></a>
Apart from the pre-pruning and the number of trees in the ensemble,
another important parameter of gradient boosting is the <code>learning_rate</code>,
which controls how strongly each tree tries to correct the mistakes of
the previous trees. A higher learning rate means each tree can make
stronger corrections, allowing for more complex models. Adding more trees to the ensemble, which can be accomplished by increasing
<code>n_estimators</code>, also increases the model complexity, as the model has
more chances to correct mistakes on the training set.</p>

<p>Here is an example of using <code>GradientBoostingClassifier</code> on the Breast
Cancer dataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:</p>

<p><code><strong>In[70]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">GradientBoostingClassifier</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[70]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 1.000
Accuracy on test set: 0.958</pre>

<p>As the training set accuracy is 100%, we are likely to be overfitting.
To reduce overfitting, we could either apply stronger pre-pruning by
limiting the maximum depth or lower the learning rate:</p>

<p class="pagebreak-before"><code><strong>In[71]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[71]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.991
Accuracy on test set: 0.972</pre>

<p><code><strong>In[72]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[72]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.988
Accuracy on test set: 0.965</pre>

<p><a data-type="indexterm" data-primary="gradient boosted regression trees" data-secondary="training set accuracy" id="idm45613681911032"></a>
Both methods of decreasing the model complexity reduced the training
set accuracy, as expected. In this case, lowering the maximum depth of
the trees provided a significant improvement of the model, while
lowering the learning rate only increased the generalization performance
slightly.</p>

<p>As for the other decision tree–based models, we can again visualize the
feature importances to get more insight into our model (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#inspect_some">Figure&nbsp;2-35</a>). As we used 100
trees, it is impractical to inspect them all, even if they are all of
depth 1:</p>

<p><code><strong>In[73]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">plot_feature_importances_cancer</code><code class="p">(</code><code class="n">gbrt</code><code class="p">)</code></pre>

<figure><div id="inspect_some" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0230.png" alt="malp 0230" width="1565" height="838" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0230.png">
<h6><span class="label">Figure 2-35. </span>Feature importances computed from a gradient boosting classifier that was fit to the Breast Cancer dataset</h6>
</div></figure>

<p>We can see that the feature importances of the gradient boosted trees
are somewhat similar to the feature importances of the random forests,
though the gradient boosting completely ignored some of the features.</p>

<p>As both gradient boosting and random forests perform well on similar kinds of
data, a common approach is to first try random forests, which work quite
robustly. If random forests work well but prediction time is at a
premium, or it is important to squeeze out the last percentage of
accuracy from the machine learning model, moving to gradient boosting
often helps.</p>

<p><a data-type="indexterm" data-primary="xgboost package" id="idm45613681827080"></a>
If you want to apply gradient boosting to a large-scale problem, it
might be worth looking into the <code>xgboost</code> package and its Python
interface, which at the time of writing is faster (and sometimes easier
to tune) than the <code>scikit-learn</code> implementation of gradient boosting on
many datasets.<a data-type="indexterm" data-primary="" data-startref="SLCFgradboostclas2" id="idm45613681825144"></a><a data-type="indexterm" data-primary="" data-startref="Ascgrad2" id="idm45613681824168"></a><a data-type="indexterm" data-primary="" data-startref="Asrgrad2" id="idm45613681823224"></a></p>












<section data-type="sect4" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect4" id="strengths-weaknesses-and-parameters-5">
<h4>Strengths, weaknesses, and parameters</h4>

<p><a data-type="indexterm" data-primary="gradient boosted regression trees" data-secondary="strengths and weaknesses" id="idm45613681883768"></a>
Gradient boosted decision trees are among the most powerful and widely
used models for supervised learning. Their main drawback is that they
require careful tuning of the parameters and may take a long time to
train. Similarly to other tree-based models, the algorithm works well
without scaling and on a mixture of binary and continuous features. As
with other tree-based models, it also often does not work well on
high-dimensional sparse data.</p>

<p><a data-type="indexterm" data-primary="gradient boosted regression trees" data-secondary="parameters" id="idm45613681881864"></a>
The main parameters of gradient boosted tree models are the number
of trees, <code>n_estimators</code>, and the <code>learning_rate</code>, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees.
These two parameters are highly interconnected, as a lower
<code>learning_rate</code> means that more trees are needed to build a model of
similar complexity. In contrast to random forests, where a higher
<code>n_estimators</code> value is always better, increasing <code>n_estimators</code> in gradient
boosting leads to a more complex model, which may lead to overfitting. A
common practice is to fit <code>n_estimators</code> depending on the time and
memory budget, and then search over different <code>learning_rate</code>s.</p>

<p>Another important parameter is <code>max_depth</code> (or alternatively
<code>max_leaf_nodes</code>), to reduce the complexity of each tree. Usually
<code>max_depth</code> is set very low for gradient boosted models, often not
deeper than five splits.<a data-type="indexterm" data-primary="" data-startref="SLalgensem2" id="idm45613681875640"></a><a data-type="indexterm" data-primary="" data-startref="ENgrad2" id="idm45613681874664"></a></p>
</div></section>

</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.7 Kernelized Support Vector Machines"><div class="sect2" id="kernelized-support-vector-machines">
<h2>2.3.7 Kernelized Support Vector Machines</h2>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="kernelized support vector machines" id="SLalgkern2"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="kernelized support vector machines" id="Akernel2"></a><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="vs. linear support vector machines" data-secondary-sortas="linear support vector machines" id="idm45613681869112"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="SVR" id="idm45613681867768"></a>
The next type of supervised model we will discuss is kernelized support
vector machines. We explored the use of linear support vector machines
for classification in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear-models-for-classification">“Linear models for classification”</a>. Kernelized support
vector machines (often just referred to as SVMs) are an extension that
allows for more complex models that are not defined simply by
hyperplanes in the input space. While there are support vector machines
for classification and regression, we will restrict ourselves to the
classification case, as implemented in <code>SVC</code>. Similar concepts apply to
support vector regression, as implemented in <code>SVR</code>.</p>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="mathematics of" id="idm45613681801080"></a>
The math behind kernelized support vector machines is a bit involved,
and is beyond the scope of this book. You can find the details in
Chapter 12 of Hastie, Tibshirani, and Friedman’s <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/"><em>The Elements of Statistical Learning</em></a>. However, we will try to
give you some sense of the idea behind the method.</p>










<section data-type="sect3" data-pdf-bookmark="Linear models and nonlinear features"><div class="sect3" id="linear-models-and-non-linear-features">
<h3>Linear models and nonlinear features</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="linear models and nonlinear features" id="idm45613681796712"></a>
As you saw in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#logisticregression_and_linearsvc">Figure&nbsp;2-15</a>,
linear models can be quite limiting in low-dimensional spaces, as lines
and hyperplanes have limited flexibility. One way to make a linear model
more flexible is by adding more features—for example, by adding
interactions or polynomials of the input features.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="make_blobs" id="idm45613681794136"></a>
Let’s look at the synthetic dataset we used in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#feature-importance-in-trees">“Feature importance in trees”</a> (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#another_relationship_features_class">Figure&nbsp;2-29</a>):</p>

<p><code><strong>In[74]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">centers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">y</code> <code class="o">%</code> <code class="mi">2</code>

<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="synthetic_dataset_again" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0231.png" alt="malp 0231" width="1565" height="1092" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0231.png">
<h6><span class="label">Figure 2-36. </span>Two-class classification dataset in which classes are not linearly separable</h6>
</div></figure>

<p>A linear model for classification can only separate points using a line,
and will not be able to do a very good job on this dataset (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear_model_insufficient">Figure&nbsp;2-37</a>):</p>

<p><code><strong>In[75]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>
<code class="n">linear_svm</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">linear_svm</code><code class="p">,</code> <code class="n">X</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<p>Now let’s expand the set of input features, say by also adding
<code>feature1 ** 2</code>, the square of the second feature, as a new feature.
Instead of representing each data point as a two-dimensional point,
<code>(feature0, feature1)</code>, we now represent it as a three-dimensional point, <code>(feature0, feature1, feature1 ** 2)</code>.<sup><a data-type="noteref" id="idm45613681662568-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613681662568">10</a></sup> This new representation is illustrated in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#three_d_scatter_plot">Figure&nbsp;2-38</a> in
a three-dimensional scatter plot:</p>

<figure><div id="linear_model_insufficient" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0232.png" alt="malp 0232" width="1517" height="1052" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0232.png">
<h6><span class="label">Figure 2-37. </span>Decision boundary found by a linear SVM</h6>
</div></figure>

<p><code><strong>In[76]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># add the squared second feature</code>
<code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">hstack</code><code class="p">([</code><code class="n">X</code><code class="p">,</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">:]</code> <code class="o">**</code> <code class="mi">2</code><code class="p">])</code>

<code class="kn">from</code> <code class="nn">mpl_toolkits.mplot3d</code> <code class="kn">import</code> <code class="n">Axes3D</code><code class="p">,</code> <code class="n">axes3d</code>
<code class="n">figure</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="c1"># visualize in 3D</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">Axes3D</code><code class="p">(</code><code class="n">figure</code><code class="p">,</code> <code class="n">elev</code><code class="o">=-</code><code class="mi">152</code><code class="p">,</code> <code class="n">azim</code><code class="o">=-</code><code class="mi">26</code><code class="p">)</code>
<code class="c1"># plot first all the points with y == 0, then all with y == 1</code>
<code class="n">mask</code> <code class="o">=</code> <code class="n">y</code> <code class="o">==</code> <code class="mi">0</code>
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="s1">'b'</code><code class="p">,</code>
           <code class="n">cmap</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="s1">'r'</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'^'</code><code class="p">,</code>
           <code class="n">cmap</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"feature0"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"feature1"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_zlabel</code><code class="p">(</code><code class="s2">"feature1 ** 2"</code><code class="p">)</code></pre>

<figure><div id="three_d_scatter_plot" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0233.png" alt="malp 0233" width="1565" height="1060" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0233.png">
<h6><span class="label">Figure 2-38. </span>Expansion of the dataset shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear_model_insufficient">Figure&nbsp;2-37</a>, created by adding a third feature derived from feature1</h6>
</div></figure>

<p>In the new representation of the data, it is now
indeed possible to separate the two classes using a linear model, a
plane in three dimensions. We can confirm this by fitting a linear model
to the augmented data (see <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear_model_augmented_data">Figure&nbsp;2-39</a>):</p>

<p><code><strong>In[77]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python" class="less_space"><code class="n">linear_svm_3d</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">coef</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">=</code> <code class="n">linear_svm_3d</code><code class="o">.</code><code class="n">coef_</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">linear_svm_3d</code><code class="o">.</code><code class="n">intercept_</code>

<code class="c1"># show linear decision boundary</code>
<code class="n">figure</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">Axes3D</code><code class="p">(</code><code class="n">figure</code><code class="p">,</code> <code class="n">elev</code><code class="o">=-</code><code class="mi">152</code><code class="p">,</code> <code class="n">azim</code><code class="o">=-</code><code class="mi">26</code><code class="p">)</code>
<code class="n">xx</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">X_new</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">min</code><code class="p">()</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="n">X_new</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">+</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">50</code><code class="p">)</code>
<code class="n">yy</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">X_new</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">min</code><code class="p">()</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="n">X_new</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">max</code><code class="p">()</code> <code class="o">+</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">50</code><code class="p">)</code>

<code class="n">XX</code><code class="p">,</code> <code class="n">YY</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">meshgrid</code><code class="p">(</code><code class="n">xx</code><code class="p">,</code> <code class="n">yy</code><code class="p">)</code>
<code class="n">ZZ</code> <code class="o">=</code> <code class="p">(</code><code class="n">coef</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="n">XX</code> <code class="o">+</code> <code class="n">coef</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="n">YY</code> <code class="o">+</code> <code class="n">intercept</code><code class="p">)</code> <code class="o">/</code> <code class="o">-</code><code class="n">coef</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot_surface</code><code class="p">(</code><code class="n">XX</code><code class="p">,</code> <code class="n">YY</code><code class="p">,</code> <code class="n">ZZ</code><code class="p">,</code> <code class="n">rstride</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">cstride</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="n">mask</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="s1">'b'</code><code class="p">,</code>
           <code class="n">cmap</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">X_new</code><code class="p">[</code><code class="o">~</code><code class="n">mask</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="s1">'r'</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'^'</code><code class="p">,</code>
           <code class="n">cmap</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">60</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'k'</code><code class="p">)</code>

<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"feature0"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"feature1"</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_zlabel</code><code class="p">(</code><code class="s2">"feature1 ** 2"</code><code class="p">)</code></pre>

<figure><div id="linear_model_augmented_data" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0234.png" alt="malp 0234" width="1565" height="1060" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0234.png">
<h6><span class="label">Figure 2-39. </span>Decision boundary found by a linear SVM on the expanded three-dimensional dataset</h6>
</div></figure>

<p>As a function of the original features, the linear SVM model is not
actually linear anymore. It is not a line, but more of an ellipse, as you can see from the plot created here (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#svm_model_ellipse">Figure&nbsp;2-40</a>):</p>

<p><code><strong>In[78]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ZZ</code> <code class="o">=</code> <code class="n">YY</code> <code class="o">**</code> <code class="mi">2</code>
<code class="n">dec</code> <code class="o">=</code> <code class="n">linear_svm_3d</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">XX</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">YY</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">ZZ</code><code class="o">.</code><code class="n">ravel</code><code class="p">()])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">contourf</code><code class="p">(</code><code class="n">XX</code><code class="p">,</code> <code class="n">YY</code><code class="p">,</code> <code class="n">dec</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">XX</code><code class="o">.</code><code class="n">shape</code><code class="p">),</code> <code class="n">levels</code><code class="o">=</code><code class="p">[</code><code class="n">dec</code><code class="o">.</code><code class="n">min</code><code class="p">(),</code> <code class="mi">0</code><code class="p">,</code> <code class="n">dec</code><code class="o">.</code><code class="n">max</code><code class="p">()],</code>
             <code class="n">cmap</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="svm_model_ellipse" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0235.png" alt="malp 0235" width="1565" height="1051" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0235.png">
<h6><span class="label">Figure 2-40. </span>The decision boundary from <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#linear_model_augmented_data">Figure&nbsp;2-39</a> as a function of the original two features</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The kernel trick"><div class="sect3" id="the-kernel-trick">
<h3>The kernel trick</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="kernel trick" id="idm45613681051288"></a>
The lesson here is that adding nonlinear features to the representation
of our data can make linear models much more powerful. However, often we
don’t know which features to add, and adding many features (like all
possible interactions in a 100-dimensional feature space) might make
computation very expensive. Luckily, there is a clever mathematical
trick that allows us to learn a classifier in a higher-dimensional space
without actually computing the new, possibly very large representation.
This is known as the <em>kernel trick</em>, and it works by directly
computing the distance (more precisely, the scalar products) of the data
points for the expanded feature representation, without ever actually
computing the expansion.</p>

<p><a data-type="indexterm" data-primary="polynomial kernels" id="idm45613681048664"></a><a data-type="indexterm" data-primary="radial basis function (RBF) kernel" id="idm45613681047960"></a><a data-type="indexterm" data-primary="Gaussian kernels of SVC" id="idm45613681047320"></a>
There are two ways to map your data into a higher-dimensional space that
are commonly used with support vector machines: the polynomial kernel,
which computes all possible polynomials up to a certain degree of the
original features (like <code>feature1 ** 2 * feature2 ** 5</code>); and the radial
basis function (RBF) kernel, also known as the Gaussian kernel. The Gaussian
kernel is a bit harder to explain, as it corresponds to an infinite-dimensional feature space. One way to explain the Gaussian kernel is
that it considers all possible polynomials of all degrees, but the
importance of the features decreases for higher degrees.<sup><a data-type="noteref" id="idm45613681045512-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613681045512">11</a></sup></p>

<p>In practice, the mathematical details behind the kernel SVM are not that
important, though, and how an SVM with an RBF kernel makes a decision can
be summarized quite easily—we’ll do so in the next section.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Understanding SVMs"><div class="sect3" id="understanding-svms">
<h3>Understanding SVMs</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="understanding" id="idm45613681042488"></a><a data-type="indexterm" data-primary="support vectors" id="idm45613681041544"></a>
During training, the SVM learns how important each of the training data
points is to represent the decision boundary between the two classes.
Typically only a subset of the training points matter for defining the
decision boundary: the ones that lie on the border between the classes.
These are called <em>support vectors</em> and give the support vector machine
its name.</p>

<p><a data-type="indexterm" data-primary="dual_coef_ attribute" id="idm45613681039752"></a><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="predictions with" id="idm45613681039048"></a>
To make a prediction for a new point, the distance to each of the support
vectors is measured. A classification decision is made based on the
distances to the support vector, and the importance of the support
vectors that was learned during training (stored in the <code>dual_coef_</code>
attribute of <code>SVC</code>).</p>

<p>The distance between data points is measured by the Gaussian kernel:</p>
<ul class="simplelist">
  <li><em>k</em><sub>rbf</sub>(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>) = exp (–<em>ɣ</em>ǁ<em>x</em><sub>1</sub> - <em>x</em><sub>2</sub>ǁ<sup>2</sup>)</li>
</ul>

<p>Here, <em>x</em><sub>1</sub> and <em>x</em><sub>2</sub> are data points,
ǁ <em>x</em><sub>1</sub> - <em>x</em><sub>2</sub> ǁ denotes Euclidean distance, and
ɣ (gamma) is a parameter that controls the width of the
Gaussian kernel.</p>

<p><a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#smooth_and_nonlinear">Figure&nbsp;2-41</a> shows the result of training a support vector machine on a
two-dimensional two-class dataset. The decision boundary is shown in
black, and the support vectors are larger points with the wide outline. The following code creates this plot by training an SVM on the <code>forge</code> dataset:</p>

<p><code><strong>In[79]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python" class="less_space"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">make_handcrafted_dataset</code><code class="p">()</code>
<code class="n">svm</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'rbf'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">svm</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">eps</code><code class="o">=.</code><code class="mi">5</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
<code class="c1"># plot support vectors</code>
<code class="n">sv</code> <code class="o">=</code> <code class="n">svm</code><code class="o">.</code><code class="n">support_vectors_</code>
<code class="c1"># class labels of support vectors are given by the sign of the dual coefficients</code>
<code class="n">sv_labels</code> <code class="o">=</code> <code class="n">svm</code><code class="o">.</code><code class="n">dual_coef_</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code> <code class="o">&gt;</code> <code class="mi">0</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">sv</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">sv</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">sv_labels</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">15</code><code class="p">,</code> <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="smooth_and_nonlinear" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0236.png" alt="malp 0236" width="1517" height="1052" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0236.png">
<h6><span class="label">Figure 2-41. </span>Decision boundary and support vectors found by an SVM with RBF kernel</h6>
</div></figure>

<p>In this case, the SVM yields a very smooth and nonlinear (not a
straight line) boundary. We adjusted two parameters here: the
<code>C</code> parameter and the <code>gamma</code> parameter, which we will now discuss in
detail.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tuning SVM parameters"><div class="sect3" id="tuning-svm-parameters">
<h3>Tuning SVM parameters</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="tuning SVM parameters" id="idm45613680766520"></a><a data-type="indexterm" data-primary="C parameter in SVC" id="idm45613680765576"></a>
The <code>gamma</code> parameter is the one shown in the formula given
in the previous section, which corresponds to the inverse of the width of
the Gaussian kernel.
Intuitively, the gamma parameter determines how far the influence of a single
training example reaches, with low values meaning corresponding to a far reach,
and high values to a limited reach. In other words, the wider the radius of the
Gaussian kernel, the further the influence of each training example.
The <code>C</code> parameter is a regularization parameter, similar to that used in the
linear models. It limits the importance of each point (or more precisely, their
<code>dual_coef_</code>).</p>

<p>Let’s have a look at what happens when we vary these parameters (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#parameters_varied">Figure&nbsp;2-42</a>):</p>

<p><code><strong>In[80]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">15</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>

<code class="k">for</code> <code class="n">ax</code><code class="p">,</code> <code class="n">C</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">axes</code><code class="p">,</code> <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">]):</code>
    <code class="k">for</code> <code class="n">a</code><code class="p">,</code> <code class="n">gamma</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">ax</code><code class="p">,</code> <code class="nb">range</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)):</code>
        <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_svm</code><code class="p">(</code><code class="n">log_C</code><code class="o">=</code><code class="n">C</code><code class="p">,</code> <code class="n">log_gamma</code><code class="o">=</code><code class="n">gamma</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">a</code><code class="p">)</code>

<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"class 0"</code><code class="p">,</code> <code class="s2">"class 1"</code><code class="p">,</code> <code class="s2">"sv class 0"</code><code class="p">,</code> <code class="s2">"sv class 1"</code><code class="p">],</code>
                  <code class="n">ncol</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="o">.</code><code class="mi">9</code><code class="p">,</code> <code class="mf">1.2</code><code class="p">))</code></pre>

<figure><div id="parameters_varied" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0237.png" alt="malp 0237" width="1565" height="1151" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0237.png">
<h6><span class="label">Figure 2-42. </span>Decision boundaries and support vectors for different settings of the parameters C and gamma</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="gamma parameter" id="idm45613680721112"></a><a data-type="indexterm" data-primary="Gaussian kernels of SVC" id="idm45613680720248"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="SVC" id="idm45613680719576"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="SVC" id="idm45613680718664"></a>
Going from left to right, we increase the value of the parameter <code>gamma</code> from <code>0.1</code> to
<code>10</code>. A small <code>gamma</code> means a large radius for the Gaussian kernel, which
means that many points are considered close by. This is reflected in
very smooth decision boundaries on the left, and boundaries that focus
more on single points further to the right. A low value of <code>gamma</code> means
that the decision boundary will vary slowly, which yields a model of low
complexity, while a high value of <code>gamma</code> yields a more complex model.</p>

<p>Going from top to bottom, we increase the <code>C</code> parameter from <code>0.1</code> to
<code>1000</code>. As with the linear models, a small <code>C</code> means a very restricted
model, where each data point can only have very limited influence. You
can see that at the top left the decision boundary looks nearly linear,
with the misclassified points barely having any influence on the line. Increasing <code>C</code>, as shown on the bottom left, allows these
points to have a stronger influence on the model and makes the decision
boundary bend to correctly classify them.</p>

<p class="pagebreak-before">Let’s apply the RBF kernel SVM to the Breast Cancer dataset. By default,
<code>C=1</code> and <code>gamma=1/n_features</code>:</p>

<p><code><strong>In[81]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">()</code>
<code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[81]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 1.00
Accuracy on test set: 0.63</pre>

<p>The model overfits quite substantially, with a perfect score on the
training set and only 63% accuracy on the test set. While SVMs often
perform quite well, they are very sensitive to the settings of the
parameters and to the scaling of the data. In particular, they require
all the features to vary on a similar scale. Let’s look at the minimum
and maximum values for each feature, plotted in log-space (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#min_max_logspace">Figure&nbsp;2-43</a>):</p>

<p><code><strong>In[82]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">manage_xticks</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">yscale</code><code class="p">(</code><code class="s2">"symlog"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature index"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature magnitude"</code><code class="p">)</code></pre>

<p>From this plot we can determine that features in the Breast Cancer
dataset are of completely different orders of magnitude. This can be
somewhat of a problem for other models (like linear models), but it has
devastating effects for the kernel SVM. Let’s examine some ways to deal
with this issue.</p>

<figure><div id="min_max_logspace" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0238.png" alt="malp 0238" width="1574" height="1067" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0238.png">
<h6><span class="label">Figure 2-43. </span>Feature ranges for the Breast Cancer dataset (note that the y axis has a logarithmic scale)</h6>
</div></figure>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Preprocessing data for SVMs"><div class="sect3" id="preprocessing-data-for-svms">
<h3>Preprocessing data for SVMs</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="preprocessing data for" id="idm45613680457640"></a><a data-type="indexterm" data-primary="rescaling" data-secondary="kernel SVMs" id="idm45613680456472"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="MinMaxScaler" id="idm45613680455528"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="scaling" data-tertiary="MinMaxScaler" id="idm45613680454616"></a>
One way to resolve this problem is by rescaling each feature so that
they are all approximately on the same scale. A common rescaling method for
kernel SVMs is to scale the data such that all features have 0 mean
and unit variance, or that all features are between 0 and 1. We
will see how to do this using the <code>StandardScaler</code> or <code>MinMaxScaler</code>
preprocessing method in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html#unsupervised-learning-and-preprocessing">Chapter&nbsp;3</a>, where we’ll
give more details. The best choice of preprocessing method depends on
the properties of the dataset. For now, let’s do this “by hand”:</p>

<p><code><strong>In[83]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># compute the minimum value per feature on the training set</code>
<code class="n">min_on_training</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="c1"># compute the range of each feature (max - min) on the training set</code>
<code class="n">range_on_training</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_train</code> <code class="o">-</code> <code class="n">min_on_training</code><code class="p">)</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># subtract the min, and divide by range</code>
<code class="c1"># afterward, min=0 and max=1 for each feature</code>
<code class="n">X_train_scaled</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_train</code> <code class="o">-</code> <code class="n">min_on_training</code><code class="p">)</code> <code class="o">/</code> <code class="n">range_on_training</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Minimum for each feature</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">X_train_scaled</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Maximum for each feature</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">X_train_scaled</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">))</code></pre>

<p class="pagebreak-before"><code><strong>Out[83]:</strong></code></p>

<pre data-type="programlisting">Minimum for each feature
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
Maximum for each feature
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]</pre>

<p><code><strong>In[84]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># use THE SAME transformation on the test set,</code>
<code class="c1"># using min and range of the training set (see Chapter 3 for details)</code>
<code class="n">X_test_scaled</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_test</code> <code class="o">-</code> <code class="n">min_on_training</code><code class="p">)</code> <code class="o">/</code> <code class="n">range_on_training</code></pre>

<p><code><strong>In[85]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">()</code>
<code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
        <code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[85]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.948
Accuracy on test set: 0.951</pre>

<p>Scaling the data made a huge difference! Now we are actually in an
underfitting regime, where training and test set performance are quite
similar but less close to 100% accuracy. From here, we can try
increasing either <code>C</code> or <code>gamma</code> to fit a more complex model. For example:</p>

<p><code><strong>In[86]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">svc</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">svc</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
    <code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">svc</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[86]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.988
Accuracy on test set: 0.972</pre>

<p>Here, increasing <code>C</code> allows us to improve the model significantly,
resulting in 97.2% accuracy.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters-6">
<h3>Strengths, weaknesses, and parameters</h3>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="strengths and weaknesses" id="idm45613680124856"></a>
Kernelized support vector machines are powerful models and perform well
on a variety of datasets. SVMs allow for complex decision boundaries,
even if the data has only a few features. They work well on
low-dimensional and high-dimensional data (i.e., few and many features),
but don’t scale very well with the number of samples. Running an SVM on data
with up to 10,000 samples might work well, but working with datasets of
size 100,000 or more can become challenging in terms of runtime and
memory usage.</p>

<p>Another downside of SVMs is that they require careful preprocessing of
the data and tuning of the parameters. This is why, these days, most
people instead use tree-based models such as random forests or gradient
boosting (which require little or no preprocessing) in many applications.
Furthermore, SVM models are hard to inspect; it can be difficult to
understand why a particular prediction was made, and it might be tricky
to explain the model to a nonexpert.</p>

<p>Still, it might be worth trying SVMs, particularly if all of your
features represent measurements in similar units (e.g., all are pixel
intensities) and they are on similar scales.</p>

<p><a data-type="indexterm" data-primary="kernelized support vector machines (SVMs)" data-secondary="parameters" id="idm45613680121480"></a>
The important parameters in kernel SVMs are the regularization parameter
<code>C</code>, the choice of the kernel, and the kernel-specific parameters. Although we primarily focused on the RBF kernel, other choices are
available in <code>scikit-learn</code>. The RBF kernel has only one parameter,
<code>gamma</code>, which is the inverse of the width of the Gaussian kernel.
<code>gamma</code> and <code>C</code> both control the complexity of the model, with large
values in either resulting in a more complex model. Therefore, good
settings for the two parameters are usually strongly correlated, and <code>C</code>
and <code>gamma</code> should be adjusted together.<a data-type="indexterm" data-primary="" data-startref="Akernel2" id="idm45613680116808"></a><a data-type="indexterm" data-primary="" data-startref="SLalgkern2" id="idm45613680115832"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="2.3.8 Neural Networks (Deep Learning)"><div class="sect2" id="neural-networks-deep-learning">
<h2>2.3.8 Neural Networks (Deep Learning)</h2>

<p><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="neural networks" id="Aneural2"></a><a data-type="indexterm" data-primary="supervised learning" data-secondary="algorithms for" data-tertiary="neural networks (deep learning)" id="SLalneural2"></a><a data-type="indexterm" data-primary="deep learning" data-see="neural networks" id="idm45613680110184"></a><a data-type="indexterm" data-primary="multilayer perceptrons (MLPs)" id="idm45613680109240"></a><a data-type="indexterm" data-primary="feed-forward neural networks" id="idm45613680108552"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="neural networks" id="Aregneural2"></a>
A family of algorithms known as neural networks has recently seen a
revival under the name “deep learning.” While deep learning shows great
promise in many machine learning applications, deep learning
algorithms are often tailored very carefully to a specific use case. Here, we
will only discuss some relatively simple methods, namely <em>multilayer
perceptrons</em> for classification and regression, that can serve as a
starting point for more involved deep learning methods. Multilayer
perceptrons (MLPs) are also known as (vanilla) feed-forward neural
networks, or sometimes just neural <span class="keep-together">networks</span>.</p>










<section data-type="sect3" data-pdf-bookmark="The neural network model"><div class="sect3" id="the-neural-network-model">
<h3>The neural network model</h3>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="predictions with" id="idm45613680102472"></a>
MLPs can be viewed as generalizations of linear models that perform
multiple stages of processing to come to a decision.</p>

<p>Remember that the prediction by a linear regressor is given as:</p>
<ul class="simplelist">
  <li><em>ŷ</em> = <em>w</em>[0] * <em>x</em>[0] + <em>w</em>[1] * <em>x</em>[1] + ... + <em>w</em>[<em>p</em>] * <em>x</em>[<em>p</em>] + <em>b</em></li>
</ul>

<p>In plain English, <em>ŷ</em> is a weighted sum of the input features <em>x</em>[0] to <em>x</em>[<em>p</em>],
weighted by the learned coefficients <em>w</em>[0] to <em>w</em>[<em>p</em>]. We could
visualize this graphically as shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#fig02in02">Figure&nbsp;2-44</a>:</p>

<p><code><strong>In[87]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">display</code><code class="p">(</code><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_logistic_regression_graph</code><code class="p">())</code></pre>

<figure><div id="fig02in02" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_02in02.png" alt="svg" width="443" height="537" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_02in02.png">
<h6><span class="label">Figure 2-44. </span>Visualization of logistic regression, where input features and predictions are shown as nodes, and the coefficients are connections between the nodes</h6>
</div></figure>

<p>Here, each node on the left represents an input feature, the connecting
lines represent the learned coefficients, and the node on the right
represents the output, which is a weighted sum of the inputs.</p>

<p><a data-type="indexterm" data-primary="hidden units" id="idm45613680055176"></a>
In an MLP this process of computing weighted sums is repeated multiple
times, first computing <em>hidden units</em> that represent an intermediate
processing step, which are again combined using weighted sums to yield
the final result (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_hidden_layer">Figure&nbsp;2-45</a>):</p>

<p><code><strong>In[88]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">display</code><code class="p">(</code><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_single_hidden_layer_graph</code><code class="p">())</code></pre>

<figure><div id="single_hidden_layer" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0239.png" alt="svg" width="669" height="538" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0239.png">
<h6><span class="label">Figure 2-45. </span>Illustration of a multilayer perceptron with a single hidden layer</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="weights" id="idm45613680042248"></a><a data-type="indexterm" data-primary="hidden layers" id="idm45613680041544"></a>
This model has a lot more coefficients (also called weights) to learn:
there is one between every input and every hidden unit (which make up
the <em>hidden layer</em>), and one between every unit in the hidden layer and
the output.</p>

<p><a data-type="indexterm" data-primary="rectifying nonlinearity" id="idm45613680039912"></a><a data-type="indexterm" data-primary="rectified linear unit (relu)" id="idm45613680039048"></a><a data-type="indexterm" data-primary="tangens hyperbolicus (tanh)" id="idm45613680038408"></a>
Computing a series of weighted sums is mathematically the same as
computing just one weighted sum, so to make this model truly more
powerful than a linear model, we need one extra trick. After
computing a weighted sum for each hidden unit, a nonlinear function is
applied to the result—usually the <em>rectifying nonlinearity</em> (also known
as rectified linear unit or relu) or the <em>tangens hyperbolicus</em> (tanh).
The result of this function is then used in the weighted sum that
computes the output, <em>ŷ</em>. The two functions are visualized in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_hidden_layer2">Figure&nbsp;2-46</a>. The relu cuts off values below zero, while tanh
saturates to –1 for low input values and +1 for high input values.
Either nonlinear function allows the neural network to learn much more
complicated functions than a linear model could:</p>

<p><code><strong>In[89]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">line</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">tanh</code><code class="p">(</code><code class="n">line</code><code class="p">),</code> <code class="n">label</code><code class="o">=</code><code class="s2">"tanh"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">maximum</code><code class="p">(</code><code class="n">line</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code> <code class="n">label</code><code class="o">=</code><code class="s2">"relu"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s2">"best"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"x"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"relu(x), tanh(x)"</code><code class="p">)</code></pre>

<figure><div id="single_hidden_layer2" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0240.png" alt="malp 0240" width="1565" height="1111" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0240.png">
<h6><span class="label">Figure 2-46. </span>The  hyperbolic tangent activation function and the rectified linear activation function</h6>
</div></figure>

<p>For the small neural network pictured in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_hidden_layer">Figure&nbsp;2-45</a>, the full formula for computing <em>ŷ</em> in the case of regression would
be (when using a tanh nonlinearity):</p>
<ul class="simplelist">
  <li><em>h</em>[0] = tanh(<em>w</em>[0, 0] * <em>x</em>[0] + <em>w</em>[1, 0] * <em>x</em>[1] + <em>w</em>[2, 0] * <em>x</em>[2] + <em>w</em>[3, 0] * <em>x</em>[3] + <em>b</em>[0])</li>
  <li><em>h</em>[1] = tanh(<em>w</em>[0, 1] * <em>x</em>[0] + <em>w</em>[1, 1] * <em>x</em>[1] + <em>w</em>[2, 1] * <em>x</em>[2] + <em>w</em>[3, 1] * <em>x</em>[3] + <em>b</em>[1])</li>
  <li><em>h</em>[2] = tanh(<em>w</em>[0, 2] * <em>x</em>[0] + <em>w</em>[1, 2] * <em>x</em>[1] + <em>w</em>[2, 2] * <em>x</em>[2] + <em>w</em>[3, 2] * <em>x</em>[3] + <em>b</em>[2])</li>
  <li><em>ŷ</em> = <em>v</em>[0] * <em>h</em>[0] + <em>v</em>[1] * <em>h</em>[1] + <em>v</em>[2] * <em>h</em>[2] + <em>b</em></li>
</ul>

<p>Here, <em>w</em> are the weights between the input <em>x</em> and the hidden layer
<em>h</em>, and <em>v</em> are the weights between the hidden layer <em>h</em> and the output
<em>ŷ</em>. The weights <em>v</em> and <em>w</em> are learned from data, <em>x</em> are the input
features, <em>ŷ</em> is the computed output, and <em>h</em> are intermediate
computations.
<a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="MLPClassifier" id="SCFCmlp2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="MLPClassifier" id="Cmlpclass2"></a>
An important parameter that needs to be set by the user is the number of
nodes in the hidden layer. This can be as small as 10 for very small or
simple datasets and as big as 10,000 for very complex data. It is
also possible to add additional hidden layers, as shown in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#two_hidden_layers">Figure&nbsp;2-47</a>:</p>

<p class="pagebreak-before"><code><strong>In[90]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_two_hidden_layer_graph</code><code class="p">()</code></pre>

<figure><div id="two_hidden_layers" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0241.png" alt="svg" width="944" height="537" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0241.png">
<h6><span class="label">Figure 2-47. </span>A multilayer perceptron with two hidden layers</h6>
</div></figure>

<p>Having large neural networks made up of many of
these layers of computation is what inspired the term “deep learning.”</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tuning neural networks"><div class="sect3" id="tuning-neural-networks">
<h3>Tuning neural networks</h3>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="tuning" id="idm45613679860872"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="make_moons" id="idm45613679858536"></a>
Let’s look into the workings of the MLP by applying the <code>MLPClassifier</code>
to the <code>two_moons</code> dataset we used earlier in this chapter. The results are shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#workings_of_the_mlp">Figure&nbsp;2-48</a>:</p>

<p><code><strong>In[91]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neural_network</code> <code class="kn">import</code> <code class="n">MLPClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">y</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="workings_of_the_mlp" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0242.png" alt="malp 0242" width="1458" height="986" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0242.png">
<h6><span class="label">Figure 2-48. </span>Decision boundary learned by a neural network with 100 hidden units on the two_moons dataset</h6>
</div></figure>

<p>As you can see, the neural network learned a very nonlinear but
relatively smooth decision boundary. We used <code>solver='lbfgs'</code>, which
we will discuss later.</p>

<p>By default, the MLP uses 100 hidden nodes, which is quite a lot for this
small dataset. We can reduce the number (which reduces the complexity of
the model) and still get a good result (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#fewer_hidden_nodes">Figure&nbsp;2-49</a>):</p>

<p><code><strong>In[92]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">10</code><code class="p">])</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="fewer_hidden_nodes" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0243.png" alt="malp 0243" width="1458" height="986" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0243.png">
<h6><span class="label">Figure 2-49. </span>Decision boundary learned by a neural network with 10 hidden units on the two_moons dataset</h6>
</div></figure>

<p>With only 10 hidden units, the decision boundary looks somewhat more
ragged. The default nonlinearity is relu, shown in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#single_hidden_layer2">Figure&nbsp;2-46</a>. With a single hidden layer, this means the decision
function will be made up of 10 straight line segments. If we want a
smoother decision boundary, we could add more hidden units (as in
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#workings_of_the_mlp">Figure&nbsp;2-48</a>), add a second hidden layer (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#activation_function">Figure&nbsp;2-50</a>), or use the tanh
nonlinearity (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#with_tanh_nonlinearity">Figure&nbsp;2-51</a>):</p>

<p><code><strong>In[93]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># using two hidden layers, with 10 units each</code>
<code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                    <code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">])</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<p class="pagebreak-before"><code><strong>In[94]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># using two hidden layers, with 10 units each, now with tanh nonlinearity</code>
<code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">activation</code><code class="o">=</code><code class="s1">'tanh'</code><code class="p">,</code>
                    <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">])</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">)</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code></pre>

<figure><div id="activation_function" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0244.png" alt="malp 0244" width="1458" height="986" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0244.png">
<h6><span class="label">Figure 2-50. </span>Decision boundary learned using 2 hidden layers with 10 hidden units each, with rect activation function</h6>
</div></figure>

<figure><div id="with_tanh_nonlinearity" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0245.png" alt="malp 0245" width="1458" height="986" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0245.png">
<h6><span class="label">Figure 2-51. </span>Decision boundary learned using 2 hidden layers with 10 hidden units each, with tanh activation function</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="Ridge" id="idm45613679344280"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, regression" data-tertiary="Ridge" id="idm45613679343112"></a>
Finally, we can also control the complexity of a neural network by using
an <code>l2</code> penalty to shrink the weights toward zero, as we did in ridge
regression and the linear classifiers. The parameter for this in the
<code>MLPClassifier</code> is <code>alpha</code> (as in the linear regression models), and it’s
set to a very low value (little regularization) by default. <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#using_l2_penalty">Figure&nbsp;2-52</a> shows the
effect of different values of <code>alpha</code> on the <code>two_moons</code> dataset, using
two hidden layers of 10 or 100 units each:</p>

<p><code><strong>In[95]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>
<code class="k">for</code> <code class="n">axx</code><code class="p">,</code> <code class="n">n_hidden_nodes</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">axes</code><code class="p">,</code> <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">100</code><code class="p">]):</code>
    <code class="k">for</code> <code class="n">ax</code><code class="p">,</code> <code class="n">alpha</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">axx</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.0001</code><code class="p">,</code> <code class="mf">0.01</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mi">1</code><code class="p">]):</code>
        <code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                            <code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="n">n_hidden_nodes</code><code class="p">,</code> <code class="n">n_hidden_nodes</code><code class="p">],</code>
                            <code class="n">alpha</code><code class="o">=</code><code class="n">alpha</code><code class="p">)</code>
        <code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
        <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
        <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
        <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"n_hidden=[{}, {}]</code><code class="se">\n</code><code class="s2">alpha={:.4f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
                      <code class="n">n_hidden_nodes</code><code class="p">,</code> <code class="n">n_hidden_nodes</code><code class="p">,</code> <code class="n">alpha</code><code class="p">))</code></pre>

<figure><div id="using_l2_penalty" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0246.png" alt="malp 0246" width="1565" height="665" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0246.png">
<h6><span class="label">Figure 2-52. </span>Decision functions for different numbers of hidden units and different settings of the alpha parameter</h6>
</div></figure>

<p>As you probably have realized by now, there are many ways to control the
complexity of a neural network: the number of hidden layers, the number
of units in each hidden layer, and the regularization (<code>alpha</code>). There are
actually even more, which we won’t go into here.</p>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="randomization in" id="idm45613679171192"></a>
An important property of neural networks is that their weights are set
randomly before learning is started, and this random initialization
affects the model that is learned. That means that even when using
exactly the same parameters, we can obtain very different models when
using different random seeds. If the networks are large, and their
complexity is chosen properly, this should not affect accuracy too much,
but it is worth keeping in mind (particularly for smaller networks).
<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#plots_of_several_models">Figure&nbsp;2-53</a> shows plots of several models, all learned with the same settings of
the parameters:</p>

<p><code><strong>In[96]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">8</code><code class="p">))</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="o">.</code><code class="n">ravel</code><code class="p">()):</code>
    <code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">solver</code><code class="o">=</code><code class="s1">'lbfgs'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="n">i</code><code class="p">,</code>
                        <code class="n">hidden_layer_sizes</code><code class="o">=</code><code class="p">[</code><code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>
    <code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">plots</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">mlp</code><code class="p">,</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">3</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code></pre>

<figure><div id="plots_of_several_models" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0247.png" alt="malp 0247" width="1565" height="627" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0247.png">
<h6><span class="label">Figure 2-53. </span>Decision functions learned with the same parameters but different random initializations</h6>
</div></figure>

<p>To get a better understanding of neural networks on real-world data,
let’s apply the <code>MLPClassifier</code> to the Breast Cancer dataset. We start
with the default parameters:</p>

<p><code><strong>In[97]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Cancer data per-feature maxima:</code><code class="se">\n</code><code class="s2">{}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)))</code></pre>

<p><code><strong>Out[97]:</strong></code></p>

<pre data-type="programlisting">Cancer data per-feature maxima:
[  28.11    39.28   188.5   2501.       0.163    0.345    0.427    0.201
    0.304    0.097    2.873    4.885   21.98   542.2      0.031    0.135
    0.396    0.053    0.079    0.03    36.04    49.54   251.2   4254.
    0.223    1.058    1.252    0.291    0.664    0.207]</pre>

<p><code><strong>In[98]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">cancer</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">cancer</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[98]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.92
Accuracy on test set: 0.90</pre>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="accuracy of" id="idm45613678838872"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="StandardScaler" id="idm45613678837992"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="scaling" data-tertiary="StandardScaler" id="idm45613678837080"></a>
The accuracy of the MLP is quite good, but not as good as the other
models. As in the earlier SVC example, this is likely due to scaling of
the data. Neural networks also expect all input features to vary in a
similar way, and ideally to have a mean of 0, and a variance of
1. We must rescale our data so that it fulfills these requirements.
Again, we will do this by hand here, but we’ll introduce the
<code>StandardScaler</code> to do this automatically in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html#unsupervised-learning-and-preprocessing">Chapter&nbsp;3</a>:</p>

<p><code><strong>In[99]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># compute the mean value per feature on the training set</code>
<code class="n">mean_on_train</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="c1"># compute the standard deviation of each feature on the training set</code>
<code class="n">std_on_train</code> <code class="o">=</code> <code class="n">X_train</code><code class="o">.</code><code class="n">std</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># subtract the mean, and scale by inverse standard deviation</code>
<code class="c1"># afterward, mean=0 and std=1</code>
<code class="n">X_train_scaled</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_train</code> <code class="o">-</code> <code class="n">mean_on_train</code><code class="p">)</code> <code class="o">/</code> <code class="n">std_on_train</code>
<code class="c1"># use THE SAME transformation (using training mean and std) on the test set</code>
<code class="n">X_test_scaled</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_test</code> <code class="o">-</code> <code class="n">mean_on_train</code><code class="p">)</code> <code class="o">/</code> <code class="n">std_on_train</code>

<code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
    <code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[99]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.991
Accuracy on test set: 0.965

ConvergenceWarning:
    Stochastic Optimizer: Maximum iterations reached and the optimization
    hasn't converged yet.</pre>

<p>The results are much better after scaling, and already quite
competitive. We got a warning from the model, though, that tells us that
the maximum number of iterations has been reached. This is part of the
<code>adam</code> algorithm for learning the model, and tells us that we should
increase the number of iterations:</p>

<p><code><strong>In[100]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
    <code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[100]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.995
Accuracy on test set: 0.965</pre>

<p>Increasing the number of iterations only increased the training set
performance, not the generalization performance. Still, the model is
performing quite well. As there is some gap between the training and the
test performance, we might try to decrease the model’s complexity to get
better generalization performance. Here, we choose to increase the <code>alpha</code>
parameter (quite aggressively, from <code>0.0001</code> to <code>1</code>) to add stronger
regularization of the weights:</p>

<p><code><strong>In[101]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mlp</code> <code class="o">=</code> <code class="n">MLPClassifier</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">mlp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on training set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
    <code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_train_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Accuracy on test set: {:.3f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test_scaled</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[101]:</strong></code></p>

<pre data-type="programlisting">Accuracy on training set: 0.988
Accuracy on test set: 0.972</pre>

<p>This leads to a performance on par with the best models so far.<sup><a data-type="noteref" id="idm45613678555032-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613678555032">12</a></sup></p>

<p>While it is possible to analyze what a neural network has learned, this is
usually much trickier than analyzing a linear model or a tree-based
model. One way to inspect what was learned is to look at the weights
in the model. You can see an example of this in the <a href="http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html"><code>scikit-learn</code> example gallery</a>. For the Breast Cancer dataset, this might be a
bit hard to understand. The following plot (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#input_to_first_hidden_layer">Figure&nbsp;2-54</a>) shows the weights that were
learned connecting the input to the first hidden layer. The rows in this
plot correspond to the 30 input features, while the columns correspond
to the 100 hidden units. Light colors represent large positive values,
while dark colors represent negative values:</p>

<p><code><strong>In[102]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">mlp</code><code class="o">.</code><code class="n">coefs_</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">yticks</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">30</code><code class="p">),</code> <code class="n">cancer</code><code class="o">.</code><code class="n">feature_names</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Columns in weight matrix"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Input feature"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">()</code></pre>

<figure><div id="input_to_first_hidden_layer" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0248.png" alt="malp 0248" width="1565" height="437" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0248.png">
<h6><span class="label">Figure 2-54. </span>Heat map of the first layer weights in a neural network learned on the Breast Cancer dataset</h6>
</div></figure>

<p>One possible inference we can make is that features that have very small
weights for all of the hidden units are “less important” to the model.
We can see that “mean smoothness” and “mean compactness,” in addition to
the features found between “smoothness error” and “fractal dimension
error,” have relatively low weights compared to other features. This
could mean that these are less important features or possibly that we
didn’t represent them in a way that the neural network could use.</p>

<p>We could also visualize the weights connecting the hidden layer to the
output layer, but those are even harder to interpret.</p>

<p>While the <code>MLPClassifier</code> and <code>MLPRegressor</code> provide easy-to-use
interfaces for the most common neural network architectures, they only
capture a small subset of what is possible with neural networks. If you
are interested in working with more flexible or larger models, we
encourage you to look beyond <code>scikit-learn</code> into the fantastic deep
learning libraries that are out there. For Python users, the most
well-established are <code>keras</code>, <code>lasagna</code>, and <code>tensor-flow</code>. <code>lasagna</code> builds on
the <code>theano</code> library, while <code>keras</code> can use either <code>tensor-flow</code> or <code>theano</code>.
These libraries provide a much more flexible interface to build neural
networks and track the rapid progress in deep learning research. All of
the popular deep learning libraries also allow the use of
high-performance graphics processing units (GPUs), which <code>scikit-learn</code>
does not support. Using GPUs allows us to accelerate computations by
factors of 10x to 100x, and they are essential for applying deep learning
methods to large-scale datasets.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Strengths, weaknesses, and parameters"><div class="sect3" id="strengths-weaknesses-and-parameters-7">
<h3>Strengths, weaknesses, and parameters</h3>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="strengths and weaknesses" id="idm45613678455240"></a>
Neural networks have reemerged as state-of-the-art models in many
applications of machine learning. One of their main advantages is that
they are able to capture information contained in large amounts of data
and build incredibly complex models. Given enough computation time,
data, and careful tuning of the parameters, neural networks often beat
other machine learning algorithms (for classification and regression
tasks).</p>

<p>This brings us to the downsides. Neural networks—particularly the
large and powerful ones—often take a long time to train. They also
require careful preprocessing of the data, as we saw here. Similarly to
SVMs, they work best with “homogeneous” data, where all the features
have similar meanings. For data that has very different kinds of
features, tree-based models might work better. Tuning neural network
parameters is also an art unto itself. In our experiments, we
barely scratched the surface of possible ways to adjust neural network
models and how to train them.</p>












<section data-type="sect4" data-pdf-bookmark="Estimating complexity in neural networks"><div class="sect4" id="estimating-complexity-in-neural-networks">
<h4>Estimating complexity in neural networks</h4>

<p><a data-type="indexterm" data-primary="neural networks (deep learning)" data-secondary="estimating complexity in" id="idm45613678450840"></a>
The most important parameters are the number of layers and the number of
hidden units per layer. You should start with one or two hidden layers,
and possibly expand from there. The number of nodes per hidden layer is
often similar to the number of input features, but rarely higher than
in the low to mid-thousands.</p>

<p>A helpful measure when thinking about the model
complexity of a neural network is the number of weights or coefficients
that are learned. If you have a binary classification dataset with 100
features, and you have 100 hidden units, then there are
100 * 100 = 10,000 weights between the input and the first hidden
layer. There are also <span class="keep-together">100 * 1 = 100</span> weights between the hidden layer
and the output layer, for a total of around 10,100 weights. If you add
a second hidden layer with 100 hidden units, there will be another
100 * 100 = 10,000 weights from the first hidden layer to the second
hidden layer, resulting in a total of 20,100 weights. If instead you
use one layer with 1,000 hidden units, you are learning
100 * 1,000 = 100,000 weights from the input to the hidden layer and
1,000 * 1 weights from the hidden layer to the output layer, for a total of
101,000. If you add a second hidden layer you add
1,000 * 1,000 = 1,000,000 weights, for a whopping total of 1,101,000—50 times larger than the model with two hidden layers of size 100.</p>

<p>A
common way to adjust parameters in a neural network is to first create a
network that is large enough to overfit, making sure that the task can
actually be learned by the network. Then, once you know the training data can
be learned, either shrink the network or increase <code>alpha</code> to add
regularization, which will improve generalization performance.</p>

<p><a data-type="indexterm" data-primary="algorithm parameter" id="idm45613678445816"></a>
In our experiments, we focused mostly on the definition of the
model: the number of layers and nodes per layer, the regularization, and
the nonlinearity. These define the model we want to learn. There is also
the question of <em>how</em> to learn the model, or the algorithm that is used
for learning the parameters, which is set using the <code>algorithm</code>
parameter. There are two easy-to-use choices for <code>algorithm</code>. The
default is <code>'adam'</code>, which works well in most situations but is quite
sensitive to the scaling of the data (so it is important to always scale
your data to 0 mean and unit variance). The other one is <code>'lbfgs'</code>,
which is quite robust but might take a long time on larger models or
larger datasets. There is also the more advanced <code>'sgd'</code> option, which
is what many deep learning researchers use. The <code>'sgd'</code> option comes
with many additional parameters that need to be tuned for best results.
You can find all of these parameters and their definitions in the
user guide. When starting to work with MLPs, we recommend sticking to
<code>'adam'</code> and <code>'lbfgs'</code>.<a data-type="indexterm" data-primary="" data-startref="SLalneural2" id="idm45613678439880"></a><a data-type="indexterm" data-primary="" data-startref="Aneural2" id="idm45613678438872"></a><a data-type="indexterm" data-primary="" data-startref="Aregneural2" id="idm45613678437928"></a></p>
<div data-type="note" epub:type="note"><h1>fit Resets a Model</h1>
<p><a data-type="indexterm" data-primary="fit method" id="idm45613678436104"></a><a data-type="indexterm" data-primary="scikit-learn" data-secondary="fit method" id="idm45613678435208"></a>
An important property of <code>scikit-learn</code> models
is that calling <code>fit</code> will always reset everything a model previously
learned. So if you build a model on one dataset, and then call <code>fit</code>
again on a different dataset, the model will “forget” everything it
learned from the first dataset. You can call <code>fit</code> as often as you like on
a model, and the outcome will be the same as calling <code>fit</code> on a “new”
model.<a data-type="indexterm" data-primary="" data-startref="Cmlpclass2" id="idm45613678431832"></a><a data-type="indexterm" data-primary="" data-startref="SCFCmlp2" id="idm45613678430824"></a></p>
</div>
</div></section>

</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="2.4 Uncertainty Estimates from Classifiers"><div class="sect1" id="uncertainty-estimates-from-classifiers">
<h1>2.4 Uncertainty Estimates from Classifiers</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="uncertainty estimates" id="SLuncert2"></a><a data-type="indexterm" data-primary="classifiers" data-secondary="uncertainty estimates from" id="Cuncert2"></a><a data-type="indexterm" data-primary="uncertainty estimates" data-secondary="applications for" id="idm45613678425832"></a>
Another useful part of the <code>scikit-learn</code> interface that we haven’t talked
about yet is the ability of classifiers to provide uncertainty estimates
of predictions. Often, you are not only interested in which class a
classifier predicts for a certain test point, but also how certain it is
that this is the right class. In practice, different kinds of mistakes
lead to very different outcomes in real-world applications. Imagine a
medical application testing for cancer. Making a false positive
prediction might lead to a patient undergoing additional tests, while a
false negative prediction might lead to a serious disease not being
treated. We will go into this topic in more detail in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch06.html#algorithm-chains-and-pipelines">Chapter&nbsp;6</a>.</p>

<p><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="GradientBoostingClassifier" id="idm45613678422568"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="gradient boosting" id="idm45613678421448"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="make_blobs" id="idm45613678420216"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="make_circles" id="idm45613678419256"></a>
There are two different functions in <code>scikit-learn</code> that can be used to
obtain uncertainty estimates from classifiers: <code>decision_function</code> and
<code>predict_proba</code>. Most (but not all) classifiers have at least one of
them, and many classifiers have both. Let’s look at what these two
functions do on a synthetic two-dimensional dataset, when building a
<code>GradientBoostingClassifier</code> classifier, which has both a <code>decision_function</code> and a <code>predict_proba</code> method:</p>

<p><code><strong>In[103]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python" class="less_space"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">GradientBoostingClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_circles</code>
<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_circles</code><code class="p">(</code><code class="n">noise</code><code class="o">=</code><code class="mf">0.25</code><code class="p">,</code> <code class="n">factor</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

<code class="c1"># we rename the classes "blue" and "red" for illustration purposes</code>
<code class="n">y_named</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="s2">"blue"</code><code class="p">,</code> <code class="s2">"red"</code><code class="p">])[</code><code class="n">y</code><code class="p">]</code>

<code class="c1"># we can call train_test_split with arbitrarily many arrays;</code>
<code class="c1"># all will be split in a consistent manner</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train_named</code><code class="p">,</code> <code class="n">y_test_named</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> \
    <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y_named</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

<code class="c1"># build the gradient boosting model</code>
<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_named</code><code class="p">)</code></pre>








<section data-type="sect2" data-pdf-bookmark="2.4.1 The Decision Function"><div class="sect2" id="the-decision-function">
<h2>2.4.1 The Decision Function</h2>

<p><a data-type="indexterm" data-primary="uncertainty estimates" data-secondary="decision function" id="idm45613678292696"></a><a data-type="indexterm" data-primary="decision function" id="idm45613678291688"></a>
In the binary classification case, the return value of
<code>decision_function</code> is of shape <code>(n_samples,)</code>, and it returns one
floating-point number for each sample:</p>

<p><code><strong>In[104]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"X_test.shape: {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Decision function shape: {}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
      <code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="o">.</code><code class="n">shape</code><code class="p">))</code></pre>

<p><code><strong>Out[104]:</strong></code></p>

<pre data-type="programlisting">X_test.shape: (25, 2)
Decision function shape: (25,)</pre>

<p>This value encodes how strongly the model believes a data point to
belong to the “positive” class, in this case class 1. Positive values
indicate a preference for the positive class, and negative values indicate
a preference for the “negative” (other) class:</p>

<p><code><strong>In[105]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># show the first few entries of decision_function</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Decision function:"</code><code class="p">,</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[:</code><code class="mi">6</code><code class="p">])</code></pre>

<p><code><strong>Out[105]:</strong></code></p>

<pre data-type="programlisting">Decision function:
[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]</pre>

<p>We can recover the prediction by looking only at the sign of the
decision function:</p>

<p><code><strong>In[106]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Thresholded decision function:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code>
      <code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Predictions:</code><code class="se">\n</code><code class="s2">"</code><code class="p">,</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code></pre>

<p><code><strong>Out[106]:</strong></code></p>

<pre data-type="programlisting">Thresholded decision function:
[ True False False False  True  True False  True  True  True False  True
  True False  True False False False  True  True  True  True  True False
  False]
Predictions:
['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'
 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'
 'red' 'blue' 'blue']</pre>

<p>For binary classification, the “negative” class is always the first
entry of the <code>classes_</code> attribute, and the “positive” class is the
second entry of <code>classes_</code>. So if you want to fully recover the output
of <code>predict</code>, you need to make use of the <code>classes_</code> attribute:</p>

<p class="pagebreak-before"><code><strong>In[107]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># make the boolean True/False into 0 and 1</code>
<code class="n">greater_zero</code> <code class="o">=</code> <code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>
<code class="c1"># use 0 and 1 as indices into classes_</code>
<code class="n">pred</code> <code class="o">=</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">classes_</code><code class="p">[</code><code class="n">greater_zero</code><code class="p">]</code>
<code class="c1"># pred is the same as the output of gbrt.predict</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"pred is equal to predictions:"</code><code class="p">,</code>
      <code class="n">np</code><code class="o">.</code><code class="n">all</code><code class="p">(</code><code class="n">pred</code> <code class="o">==</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)))</code></pre>

<p><code><strong>Out[107]:</strong></code></p>

<pre data-type="programlisting">pred is equal to predictions: True</pre>

<p>The range of <code>decision_function</code> can be arbitrary, and depends on the
data and the model parameters:</p>

<p><code><strong>In[108]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">decision_function</code> <code class="o">=</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Decision function minimum: {:.2f} maximum: {:.2f}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code>
      <code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">decision_function</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">decision_function</code><code class="p">)))</code></pre>

<p><code><strong>Out[108]:</strong></code></p>

<pre data-type="programlisting">Decision function minimum: -7.69 maximum: 4.29</pre>

<p>This arbitrary scaling makes the output of <code>decision_function</code> often
hard to <span class="keep-together">interpret</span>.</p>

<p>In the following example we plot the <code>decision_function</code> for all points in the 2D plane
using a color coding, next to a visualization of the decision boundary,
as we saw earlier. We show training points as circles and test data as
triangles (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#decision_function">Figure&nbsp;2-55</a>):</p>

<p><code><strong>In[109]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">13</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code><code class="n">gbrt</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">4</code><code class="p">,</code>
                                <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">cm</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">)</code>
<code class="n">scores_image</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">plot_2d_scores</code><code class="p">(</code><code class="n">gbrt</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>
                                            <code class="n">alpha</code><code class="o">=.</code><code class="mi">4</code><code class="p">,</code> <code class="n">cm</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">ReBl</code><code class="p">)</code>

<code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">axes</code><code class="p">:</code>
    <code class="c1"># plot training and test points</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_test</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_test</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_test</code><code class="p">,</code>
                             <code class="n">markers</code><code class="o">=</code><code class="s1">'^'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">,</code>
                             <code class="n">markers</code><code class="o">=</code><code class="s1">'o'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code>
<code class="n">cbar</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">(</code><code class="n">scores_image</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Test class 0"</code><code class="p">,</code> <code class="s2">"Test class 1"</code><code class="p">,</code> <code class="s2">"Train class 0"</code><code class="p">,</code>
                <code class="s2">"Train class 1"</code><code class="p">],</code> <code class="n">ncol</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="o">.</code><code class="mi">1</code><code class="p">,</code> <code class="mf">1.1</code><code class="p">))</code></pre>

<figure><div id="decision_function" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0249.png" alt="malp 0249" width="1565" height="813" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0249.png">
<h6><span class="label">Figure 2-55. </span>Decision boundary (left) and decision function (right) for a gradient boosting model on a two-dimensional toy dataset</h6>
</div></figure>

<p>Encoding not only the predicted outcome but also how certain the
classifier is provides additional information. However, in this
visualization, it is hard to make out the boundary between the two
classes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="2.4.2 Predicting Probabilities"><div class="sect2" id="predicting-probabilities">
<h2>2.4.2 Predicting Probabilities</h2>

<p><a data-type="indexterm" data-primary="predict_proba function" id="idm45613677750088"></a><a data-type="indexterm" data-primary="uncertainty estimates" data-secondary="predicting probabilities" id="idm45613677749384"></a>
The output of <code>predict_proba</code> is a probability for each class, and is
often more easily understood than the output of <code>decision_function</code>. It is always of
shape <code>(n_samples, 2)</code> for binary classification:</p>

<p><code><strong>In[110]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Shape of probabilities:"</code><code class="p">,</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

<p><code><strong>Out[110]:</strong></code></p>

<pre data-type="programlisting">Shape of probabilities: (25, 2)</pre>

<p>The first entry in each row is the estimated probability of the first
class, and the second entry is the estimated probability of the second
class. Because it is a probability, the output of <code>predict_proba</code> is
always between 0 and 1, and the sum of the entries for both classes
is always 1:</p>

<p><code><strong>In[111]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># show the first few entries of predict_proba</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Predicted probabilities:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">[:</code><code class="mi">6</code><code class="p">]))</code></pre>

<p><code><strong>Out[111]:</strong></code></p>

<pre data-type="programlisting">Predicted probabilities:
[[0.016 0.984]
 [0.846 0.154]
 [0.981 0.019]
 [0.974 0.026]
 [0.014 0.986]
 [0.025 0.975]]</pre>

<p>Because the probabilities for the two classes sum to 1, exactly one of
the classes will be above 50% certainty. That class is the one that is
predicted.<sup><a data-type="noteref" id="idm45613677677144-marker" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613677677144">13</a></sup></p>

<p>You can see in the previous output that the classifier is relatively
certain for most points. How well the uncertainty actually reflects
uncertainty in the data depends on the model and the parameters. A model
that is more overfitted tends to make more certain predictions, even if
they might be wrong. A model with less complexity usually has more
uncertainty in its predictions. A model is called <em>calibrated</em> if the
reported uncertainty actually matches how correct it is—in a
calibrated model, a prediction made with 70% certainty would be correct
70% of the time.</p>

<p>In the following example (<a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#decision_boundary_on_dataset">Figure&nbsp;2-56</a>) we again show the decision boundary on the dataset, next to the
class probabilities for the class 1:</p>

<p><code><strong>In[112]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">13</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>

<code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">plot_2d_separator</code><code class="p">(</code>
    <code class="n">gbrt</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">4</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">cm</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">cm2</code><code class="p">)</code>
<code class="n">scores_image</code> <code class="o">=</code> <code class="n">mglearn</code><code class="o">.</code><code class="n">tools</code><code class="o">.</code><code class="n">plot_2d_scores</code><code class="p">(</code>
    <code class="n">gbrt</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">alpha</code><code class="o">=.</code><code class="mi">5</code><code class="p">,</code> <code class="n">cm</code><code class="o">=</code><code class="n">mglearn</code><code class="o">.</code><code class="n">ReBl</code><code class="p">,</code> <code class="n">function</code><code class="o">=</code><code class="s1">'predict_proba'</code><code class="p">)</code>

<code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">axes</code><code class="p">:</code>
    <code class="c1"># plot training and test points</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_test</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_test</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_test</code><code class="p">,</code>
                             <code class="n">markers</code><code class="o">=</code><code class="s1">'^'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">mglearn</code><code class="o">.</code><code class="n">discrete_scatter</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X_train</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">y_train</code><code class="p">,</code>
                             <code class="n">markers</code><code class="o">=</code><code class="s1">'o'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Feature 0"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Feature 1"</code><code class="p">)</code>
<code class="n">cbar</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">(</code><code class="n">scores_image</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s2">"Test class 0"</code><code class="p">,</code> <code class="s2">"Test class 1"</code><code class="p">,</code> <code class="s2">"Train class 0"</code><code class="p">,</code>
                <code class="s2">"Train class 1"</code><code class="p">],</code> <code class="n">ncol</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">loc</code><code class="o">=</code><code class="p">(</code><code class="o">.</code><code class="mi">1</code><code class="p">,</code> <code class="mf">1.1</code><code class="p">))</code></pre>

<figure><div id="decision_boundary_on_dataset" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_0250.png" alt="malp 0250" width="1565" height="822" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_0250.png">
<h6><span class="label">Figure 2-56. </span>Decision boundary (left) and predicted probabilities for the gradient boosting model shown in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#decision_function">Figure&nbsp;2-55</a></h6>
</div></figure>

<p>The boundaries in this plot are much more well-defined, and the small
areas of uncertainty are clearly visible.</p>

<p>The <a href="http://bit.ly/2cqCYx6"><code>scikit-learn</code> website</a>
has a great comparison of many models and what their uncertainty
estimates look like. We’ve reproduced this in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#fig02in03">Figure&nbsp;2-57</a>, and we encourage you
to go through the example there.</p>

<figure><div id="fig02in03" class="figure">
<img src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/malp_02in03.png" alt="classifier_comparison" width="1565" height="518" data-mfp-src="/library/view/introduction-to-machine/9781449369880/assets/malp_02in03.png">
<h6><span class="label">Figure 2-57. </span>Comparison of several classifiers in scikit-learn on synthetic datasets (image courtesy <a href="http://scikit-learn.org%29/"><em class="hyperlink">http://scikit-learn.org)</em></a></h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="2.4.3 Uncertainty in Multiclass Classification"><div class="sect2" id="uncertainty-in-multi-class-classification">
<h2>2.4.3 Uncertainty in Multiclass Classification</h2>

<p><a data-type="indexterm" data-primary="uncertainty estimates" data-secondary="multiclass classification" id="idm45613677396376"></a><a data-type="indexterm" data-primary="multiclass classification" data-secondary="uncertainty estimates" id="idm45613677395336"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="GradientBoostingClassifier" id="idm45613677394376"></a><a data-type="indexterm" data-primary="algorithms" data-secondary="supervised, classification" data-tertiary="gradient boosting" id="idm45613677393400"></a><a data-type="indexterm" data-primary="scikit-learn classes and functions" data-secondary="load_iris" id="idm45613677392168"></a>
So far, we’ve only talked about uncertainty estimates in binary
classification. But the <code>decision_function</code> and <code>predict_proba</code> methods
also work in the multiclass setting. Let’s apply them on the Iris dataset, which is a three-class classification dataset:</p>

<p><code><strong>In[113]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code>
    <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>

<code class="n">gbrt</code> <code class="o">=</code> <code class="n">GradientBoostingClassifier</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">gbrt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

<p><code><strong>In[114]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Decision function shape:"</code><code class="p">,</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="c1"># plot the first few entries of the decision function</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Decision function:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[:</code><code class="mi">6</code><code class="p">,</code> <code class="p">:])</code></pre>

<p><code><strong>Out[114]:</strong></code></p>

<pre data-type="programlisting">Decision function shape: (38, 3)
Decision function:
[[-0.529  1.466 -0.504]
 [ 1.512 -0.496 -0.503]
 [-0.524 -0.468  1.52 ]
 [-0.529  1.466 -0.504]
 [-0.531  1.282  0.215]
 [ 1.512 -0.496 -0.503]]</pre>

<p>In the multiclass case, the <code>decision_function</code> has the shape
<code>(n_samples, n_classes)</code> and each column provides a “certainty score”
for each class, where a large score means that a class is more likely
and a small score means the class is less likely. You can recover the
predictions from these scores by finding the maximum entry for each data
point:</p>

<p><code><strong>In[115]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Argmax of decision function:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Predictions:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code></pre>

<p><code><strong>Out[115]:</strong></code></p>

<pre data-type="programlisting">Argmax of decision function:
[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]
Predictions:
[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]</pre>

<p>The output of <code>predict_proba</code> has the same shape,
<code>(n_samples, n_classes)</code>. Again, the probabilities for the possible
classes for each data point sum to 1:</p>

<p class="pagebreak-before"><code><strong>In[116]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># show the first few entries of predict_proba</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Predicted probabilities:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[:</code><code class="mi">6</code><code class="p">])</code>
<code class="c1"># show that sums across rows are one</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Sums:"</code><code class="p">,</code> <code class="n">gbrt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[:</code><code class="mi">6</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code></pre>

<p><code><strong>Out[116]:</strong></code></p>

<pre data-type="programlisting">Predicted probabilities:
[[0.107 0.784 0.109]
 [0.789 0.106 0.105]
 [0.102 0.108 0.789]
 [0.107 0.784 0.109]
 [0.108 0.663 0.228]
 [0.789 0.106 0.105]]
Sums: [1. 1. 1. 1. 1. 1.]</pre>

<p>We can again recover the predictions by computing the <code>argmax</code> of
<code>predict_proba</code>:</p>

<p><code><strong>In[117]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"Argmax of predicted probabilities:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_test</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Predictions:"</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gbrt</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">))</code></pre>

<p><code><strong>Out[117]:</strong></code></p>

<pre data-type="programlisting">Argmax of predicted probabilities:
[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]
Predictions:
[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]</pre>

<p>To summarize, <code>predict_proba</code> and <code>decision_function</code> always have shape
<code>(n_samples, n_classes)</code>—apart from <code>decision_function</code> in the special binary case. In the binary case,
<code>decision_function</code> only has one column, corresponding to the “positive”
class <code>classes_[1]</code>. This is mostly for historical reasons.</p>

<p>You can recover the prediction when there are <code>n_classes</code> many columns
by computing the <code>argmax</code> across columns. Be careful, though, if your
classes are strings, or you use integers but they are not consecutive
and starting from 0. If you want to compare results obtained with
<code>predict</code> to results obtained via <code>decision_function</code> or <code>predict_proba</code>,
make sure to use the <code>classes_</code> attribute of the classifier to get the
actual class names:</p>

<p class="pagebreak-before"><code><strong>In[118]:</strong></code></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">logreg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>

<code class="c1"># represent each target by its class name in the iris dataset</code>
<code class="n">named_target</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target_names</code><code class="p">[</code><code class="n">y_train</code><code class="p">]</code>
<code class="n">logreg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">named_target</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"unique classes in training data:"</code><code class="p">,</code> <code class="n">logreg</code><code class="o">.</code><code class="n">classes_</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"predictions:"</code><code class="p">,</code> <code class="n">logreg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)[:</code><code class="mi">10</code><code class="p">])</code>
<code class="n">argmax_dec_func</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">logreg</code><code class="o">.</code><code class="n">decision_function</code><code class="p">(</code><code class="n">X_test</code><code class="p">),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"argmax of decision function:"</code><code class="p">,</code> <code class="n">argmax_dec_func</code><code class="p">[:</code><code class="mi">10</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"argmax combined with classes_:"</code><code class="p">,</code>
      <code class="n">logreg</code><code class="o">.</code><code class="n">classes_</code><code class="p">[</code><code class="n">argmax_dec_func</code><code class="p">][:</code><code class="mi">10</code><code class="p">])</code></pre>

<p><code><strong>Out[118]:</strong></code></p>

<pre data-type="programlisting">unique classes in training data: ['setosa' 'versicolor' 'virginica']
predictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'
 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']
argmax of decision function: [1 0 2 1 1 0 1 2 1 1]
argmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'
 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']</pre>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="2.5 Summary and Outlook"><div class="sect1" id="supervised-learning-summary-and-outlook">
<h1>2.5 Summary and Outlook</h1>

<p><a data-type="indexterm" data-primary="supervised learning" data-secondary="overview of" id="idm45613676934216"></a><a data-type="indexterm" data-primary="" data-startref="SLuncert2" id="idm45613676933240"></a><a data-type="indexterm" data-primary="" data-startref="Cuncert2" id="idm45613676932296"></a>
We started this chapter with a discussion of model complexity, then
discussed <em>generalization</em>, or learning a model that is able to perform
well on new, previously unseen data. This led us to the concepts of underfitting,
which describes a model that cannot capture the variations present in
the training data, and overfitting, which describes a model that focuses
too much on the training data and is not able to generalize to new data
very well.</p>

<p>We then discussed a wide array of machine learning models for
classification and regression, what their advantages and disadvantages
are, and how to control model complexity for each of them. We saw that
for many of the algorithms, setting the right parameters is important
for good performance. Some of the algorithms are also sensitive to how
we represent the input data, and in particular to how the features are
scaled. Therefore, blindly applying an algorithm to a dataset without
understanding the assumptions the model makes and the meanings of the
parameter settings will rarely lead to an accurate model.</p>

<p>This chapter contains a lot of information about the algorithms, and it
is not necessary for you to remember all of these details for the
following chapters. However, some knowledge of the models described here—and which to use in a specific situation—is important for
successfully applying machine learning in practice. Here is a quick
summary of when to use each model:</p>
<dl class="pagebreak-before">
<dt>Nearest neighbors</dt>
<dd>
<p>For small datasets, good as a baseline, easy to
explain.</p>
</dd>
<dt>Linear models</dt>
<dd>
<p>Go-to as a first algorithm to try, good for very large
datasets, good for very high-dimensional data.</p>
</dd>
<dt>Naive Bayes</dt>
<dd>
<p>Only for classification. Even faster than linear models,
good for very large datasets and high-dimensional data. Often less accurate than
linear models.</p>
</dd>
<dt>Decision trees</dt>
<dd>
<p>Very fast, don’t need scaling of the data, can be
visualized and easily explained.</p>
</dd>
<dt>Random forests</dt>
<dd>
<p>Nearly always perform better than a single decision
tree, very robust and powerful. Don’t need scaling of data. Not good for
very high-dimensional sparse data.</p>
</dd>
<dt>Gradient boosted decision trees</dt>
<dd>
<p>Often slightly more accurate than
random forests. Slower to train but faster to predict than random forests,
and smaller in memory. Need more parameter tuning than random forests.</p>
</dd>
<dt>Support vector machines</dt>
<dd>
<p>Powerful for medium-sized datasets of
features with similar meaning. Require scaling of data, sensitive to
parameters.</p>
</dd>
<dt>Neural networks</dt>
<dd>
<p>Can build very complex models, particularly for
large datasets. Sensitive to scaling of the data and to the choice of
parameters. Large models need a long time to train.</p>
</dd>
</dl>

<p>When working with a new dataset, it is in general a good idea to start
with a simple model, such as a linear model or a naive Bayes or nearest
neighbors classifier, and see how far you can get. After understanding more about
the data, you can consider moving to an algorithm that can build more
complex models, such as random forests, gradient boosted decision trees, SVMs, or neural networks.</p>

<p>You should now be in a position where you have some idea of how to apply,
tune, and analyze the models we discussed here. In this chapter, we
focused on the binary classification case, as this is usually easiest to
understand. Most of the algorithms presented have classification
and regression variants, however, and all of the classification
algorithms support both binary and multiclass classification. Try
applying any of these algorithms to the built-in datasets in
<code>scikit-learn</code>, like the <code>boston_housing</code> or <code>diabetes</code> datasets for
regression, or the <code>digits</code> dataset for multiclass classification.
Playing around with the algorithms on different datasets will give you a
better feel for how long they need to train, how easy it is to analyze
the models, and how sensitive they are to the representation of the data.</p>

<p>While we analyzed the consequences of different parameter settings for
the algorithms we investigated, building a model that actually
generalizes well to new data in production is a bit trickier than that.
We will see how to properly adjust parameters and how to find good
parameters automatically in <a data-type="xref" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch06.html#algorithm-chains-and-pipelines">Chapter&nbsp;6</a>.</p>

<p>First, though, we will dive in more detail into unsupervised learning and preprocessing in the next chapter.<a data-type="indexterm" data-primary="" data-startref="super2" id="idm45613676910520"></a><a data-type="indexterm" data-primary="" data-startref="MLsuper2" id="idm45613676909544"></a></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45613687045144"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613687045144-marker" class="totri-footnote">1</a></sup> We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.</p><p data-type="footnote" id="idm45613687036216"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613687036216-marker" class="totri-footnote">2</a></sup> In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a boat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy one in the future.</p><p data-type="footnote" id="idm45613686938360"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613686938360-marker" class="totri-footnote">3</a></sup> And also provably, with the right math.</p><p data-type="footnote" id="idm45613686921400"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613686921400-marker" class="totri-footnote">4</a></sup> Discussing all of them is beyond the scope of the book, and we refer you to the <a href="http://scikit-learn.org/stable/documentation"><code>scikit-learn</code> documentation</a> for more details.</p><p data-type="footnote" id="idm45613690758712"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613690758712-marker" class="totri-footnote">5</a></sup> This is 13 interactions for the first feature, plus 12 for the second not involving the first, plus 11 for the third and so on (13 + 12 + 11 + … + 1 = 91).</p><p data-type="footnote" id="idm45613689524920"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613689524920-marker" class="totri-footnote">6</a></sup> This is easy to see if you know some linear algebra.</p><p data-type="footnote" id="idm45613689153080"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613689153080-marker" class="totri-footnote">7</a></sup> Mathematically, <code>Ridge</code> penalizes the squared L2 norm of the coefficients, or the Euclidean length of <em>w</em>.</p><p data-type="footnote" id="idm45613688738392"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613688738392-marker" class="totri-footnote">8</a></sup> The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of the coefficients.</p><p data-type="footnote" id="idm45613682724856"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613682724856-marker" class="totri-footnote">9</a></sup> It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict whether a price will go up or down). The point of this example was not to show that trees are a bad model for time series, but to illustrate a particular property of how trees make predictions.</p><p data-type="footnote" id="idm45613681662568"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613681662568-marker">10</a></sup> We picked this particular feature to add for illustration purposes. The choice is not particularly important.</p><p data-type="footnote" id="idm45613681045512"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613681045512-marker">11</a></sup> This follows from the Taylor expansion of the exponential map.</p><p data-type="footnote" id="idm45613678555032"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613678555032-marker">12</a></sup> You might have noticed at this point that many of the well-performing models achieved exactly the same accuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four. If you compare the actual predictions, you can even see that they make exactly the same mistakes! This might be a consequence of the dataset being very small, or it may be because these points are really different from the rest.</p><p data-type="footnote" id="idm45613677677144"><sup><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#idm45613677677144-marker">13</a></sup> Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. However, if that happens, the prediction is made at random.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none; top: 57617.6px; left: 300px; width: 260px;">
	<ul class="adders expandUp">
		
		<li class="copy"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#">
			Add Note
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch01.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">1. Introduction</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">3. Unsupervised Learning and Preprocessing</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>

<script>
  function decodeHtml(html) {
    var txt = document.createElement("textarea");
    txt.innerHTML = html;
    return txt.value;
  }

  window.dataLayer = window.dataLayer || [];
  context = {
    product: {
      title: 'Introduction to Machine Learning with Python',
      type: 'book',
      identifier: '9781449369880',
      topic: 'Machine Learning'
    },
    content: {
      title: 'Introduction to Machine Learning with Python',
      formatType: 'book',
      identifier: '9781449369880',
      author: 'Sarah Guido, Andreas C. Müller',
      publisher: decodeHtml('O&#39;Reilly Media, Inc.'),
      releaseDate: '2016-10-11',
      parentTopic: 'none',
      free: 'no',
      subdirectory: 'none'
    }
  }
  dataLayer.push(context);
</script>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://learning.oreilly.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      
        


<footer class="pagefoot t-pagefoot" style="padding-bottom: 69.0104px;">
  <a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" class="icon-up" style="display: block;"><div class="visuallyhidden">Back to top</div></a>
  <ul class="js-footer-nav">
  
    
    <li><a href="https://learning.oreilly.com/u/preferences/">Settings</a></li>
    
    <li><a href="https://learning.oreilly.com/public/support/">Support</a></li>
    
    <li><a href="https://learning.oreilly.com/accounts/logout/">Sign Out</a></li>
    
  
  
  </ul>
  <span class="copyright">© 2019 <a href="https://learning.oreilly.com/" target="_blank">O'Reilly Media, Inc</a>.</span>
  
    
    <a href="https://www.oreilly.com/terms/">Terms of Service</a> 
     / 
    
    <a href="https://learning.oreilly.com/privacy">Privacy Policy</a> 
    
    
  
</footer>
<script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"licenseKey":"510f1a6865","transactionName":"YgdaZ0NSW0cEB0RdWltNfkZfUEFdCgofXFBHDVYdR1pQQxZeRl1QQj1aWkU=","errorBeacon":"bam.nr-data.net","applicationID":"172641827,79672898,93931619","queueTime":2,"applicationTime":415,"beacon":"bam.nr-data.net","agent":""}</script>

      
    
    <script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/saved_resource" charset="utf-8"></script>
    <script src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/saved_resource(1)" charset="utf-8"></script>
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 200.986px; left: 1428.81px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#">Reset</a>
</div>
</div><script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","1732687426968531");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=1732687426968531&amp;ev=PageView&amp;noscript=1"></noscript><script type="text/javascript" id="">(function(){window.medalliaUserIdentifier=document.documentElement.dataset.userUuid;window.medalliaUserName=document.documentElement.dataset.username})();</script>
<script type="text/javascript" id="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/embed.js"></script>
<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("set","agent","tmgoogletagmanager","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>

<script type="text/javascript" id="">!function(b,e,f,g,a,c,d){b.fbq||(a=b.fbq=function(){a.callMethod?a.callMethod.apply(a,arguments):a.queue.push(arguments)},b._fbq||(b._fbq=a),a.push=a,a.loaded=!0,a.version="2.0",a.queue=[],c=e.createElement(f),c.async=!0,c.src=g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d))}(window,document,"script","https://connect.facebook.net/en_US/fbevents.js");fbq("init","443792972845831");fbq("track","PageView");</script>
<noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=443792972845831&amp;ev=PageView&amp;noscript=1"></noscript>
<script type="text/javascript" id="">window._pp=window._pp||[];if("\/library\/view\/introduction-to-machine\/9781449369880\/ch02.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/introduction-to-machine\/9781449369880\/"=="https://learning.oreilly.com/register/")_pp.targetUrl="/confirm/trial";else if("\/library\/view\/introduction-to-machine\/9781449369880\/ch02.html"=="/confirmation/nv/"&&"https:\/\/learning.oreilly.com\/library\/view\/introduction-to-machine\/9781449369880\/"=="https://learning.oreilly.com/subscribe/")_pp.targetUrl="/confirm/paid";else if("\/library\/view\/introduction-to-machine\/9781449369880\/ch02.html"=="/confirmation/nnv/"&&"https:\/\/learning.oreilly.com\/library\/view\/introduction-to-machine\/9781449369880\/"=="https://learning.oreilly.com/signup/")_pp.targetUrl="/confirm/paid";_pp.siteId="2508";
_pp.siteUId="f0b2a002-7968-4745-a334-a519b5a26e85";_pp.orderValue="undefined";_pp.orderId="undefined";(function(){var ppjs=document.createElement("script");ppjs.type="text/javascript";ppjs.async=true;ppjs.src=("https:"==document.location.protocol?"https:":"http:")+"//cdn.pbbl.co/r/"+_pp.siteId+".js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(ppjs,s)})();</script><div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.46417938891608235"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.9475414835226708" width="0" height="0" alt="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/0"></div><script type="text/javascript" async="" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/generic1575498018713.js" charset="UTF-8"></script><div style="display: none; visibility: hidden;"><script>(function(){if(null!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')&&void 0!==document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]')){var a=!1;window.addEventListener("blur",function(){a&&dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"facebook",eventVal:0,nonInteraction:0})});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseover",function(){window.focus();
a=!0});document.querySelector('iframe[title\x3d"fb:share_button Facebook Social Plugin"]').addEventListener("mouseout",function(){a=!1})}try{window.twttr=function(b,a,d){var c,e=b.getElementsByTagName(a)[0];if(!b.getElementById(d))return b=b.createElement(a),b.id=d,b.src="//platform.twitter.com/widgets.js",e.parentNode.insertBefore(b,e),window.twttr||(c={_e:[],ready:function(a){c._e.push(a)}})}(document,"script","twitter-wjs"),twttr.ready(function(a){a.events.bind("tweet",trackTwitter)})}catch(b){}})();
null!==document.querySelector(".IN-widget")&&void 0!==document.querySelector(".IN-widget")&&document.querySelector(".IN-widget").addEventListener("click",function(){dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"linkedin",eventVal:0,nonInteraction:0})});
function trackTwitter(a){a&&(a.target&&"IFRAME"==a.target.nodeName&&(opt_target=extractParamFromUri(a.target.src,"url")),dataLayer.push({event:"eventTracker",eventCat:"social",eventAct:"share",eventLbl:"twitter",eventVal:0,nonInteraction:0}))}function extractParamFromUri(a,b){if(a){var c=new RegExp("[\\?\x26#]"+b+"\x3d([^\x26#]*)");c=c.exec(a);if(null!=c)return unescape(c[1])}};</script></div><script type="text/javascript" id="">function forceInputUppercase(a){var b=a.target.selectionStart,c=a.target.selectionEnd;a.target.value=a.target.value.toUpperCase();a.target.setSelectionRange(b,c)}void 0!=document.getElementById("id_promotion")&&null!=document.getElementById("id_promotion")&&document.getElementById("id_promotion").addEventListener("keyup",forceInputUppercase,!1);
void 0!=document.getElementsByName("promotionCode")[0]&&null!=document.getElementsByName("promotionCode")[0]&&document.getElementsByName("promotionCode")[0].addEventListener("keyup",forceInputUppercase,!1);</script><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Decision boundary of tree with depth 9 (left) and part of the corresponding tree (right); the full tree is quite large and hard to visualize_files/widget_iframe.6a44a9d26983bbb5b04ae399f9e496fe.html" title="Twitter settings iframe" style="display: none;"></iframe></body></html>